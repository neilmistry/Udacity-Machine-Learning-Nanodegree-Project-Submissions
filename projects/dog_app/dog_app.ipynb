{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n56NJygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294c5108780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 Human faces detected in human face images\n",
      "11 Human faces detected in dog face images\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "\n",
    "counthh = 0\n",
    "countdh = 0\n",
    "\n",
    "for i in human_files_short:\n",
    "    if face_detector(i) == True:\n",
    "        counthh= counthh + 1\n",
    "for i in dog_files_short:\n",
    "    if face_detector(i) == True:\n",
    "        countdh= countdh + 1\n",
    "\n",
    "print('%d Human faces detected in human face images' % (counthh))\n",
    "print('%d Human faces detected in dog face images' % (countdh))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__ I think it is a reasonable expectation that the user presents a clear face.  If the user does not clearly present his/her face, we could reject their submission and ask them to resub a clear picture of there face.  We could also try some augmentation, although I am unsure on the results of using any augmentation algorithm in this case without trying it. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Human faces detected in human face images\n",
      "20 Human faces detected in dog face images\n"
     ]
    }
   ],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# I am not sure how to do this without using OpenCV at this time, so I will attempt to try another OpenCV algorithm\n",
    "\n",
    "face_cascade2 = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "def face_detector2(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade2.detectMultiScale(gray)\n",
    "    return len(faces) > 0\n",
    "\n",
    "counthh = 0\n",
    "countdh = 0\n",
    "\n",
    "for i in human_files_short:\n",
    "    if face_detector2(i) == True:\n",
    "        counthh= counthh + 1\n",
    "for i in dog_files_short:\n",
    "    if face_detector2(i) == True:\n",
    "        countdh= countdh + 1\n",
    "\n",
    "print('%d Human faces detected in human face images' % (counthh))\n",
    "print('%d Human faces detected in dog face images' % (countdh))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Dog faces detected in human face images\n",
      "100 Dog faces detected in dog face images\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "\n",
    "countdh = 0\n",
    "countdd = 0\n",
    "\n",
    "for i in human_files_short:\n",
    "    if dog_detector(i) == True:\n",
    "        countdh= countdh + 1\n",
    "for i in dog_files_short:\n",
    "    if dog_detector(i) == True:\n",
    "        countdd= countdd + 1\n",
    "\n",
    "print('%d Dog faces detected in human face images' % (countdh))\n",
    "print('%d Dog faces detected in dog face images' % (countdd))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6680/6680 [00:54<00:00, 121.80it/s]\n",
      "100%|| 835/835 [00:06<00:00, 119.87it/s]\n",
      "100%|| 836/836 [00:06<00:00, 130.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 133)               8645      \n",
      "=================================================================\n",
      "Total params: 19,189.0\n",
      "Trainable params: 19,189.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=2, strides=1, padding='same', activation='relu', input_shape=(224, 224, 3))) #input layer and use filters to find patterns\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same')) # to reduce image space\n",
    "\n",
    "model.add(Dropout(0.25)) # to prevent overfitting and to speed up computation\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, strides=1, padding='same', activation='relu')) # to add number of filters to find patterns\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same')) # to reduce image space\n",
    "\n",
    "model.add(Dropout(0.25)) # to prevent overfitting and to speed up computation\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, strides=1, padding='same', activation='relu')) # to add number of filters to find patterns\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same')) # to reduce image space\n",
    "\n",
    "#model.add(Flatten()) # to flatten the arrays for fully connected dense layers\n",
    "\n",
    "#model.add(Dense(500, activation='relu')) # to train based on fully connected layers\n",
    "\n",
    "#model.add(Dropout(0.25)) # to prevent overfitting and to speed up computation\n",
    "\n",
    "model.add(GlobalAveragePooling2D()) # used this to flatten and to connect to a dense layer\n",
    "\n",
    "model.add(Dense(133, activation='softmax')) #in order to return probabilities, and since we have 133 possible dog breeds\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4240/6680 [==================>...........] - ETA: 101s - loss: 4.8862 - acc: 0.0000e+0 - ETA: 99s - loss: 4.8649 - acc: 0.0000e+0 - ETA: 99s - loss: 4.8676 - acc: 0.0000e+ - ETA: 99s - loss: 4.8775 - acc: 0.0000e+ - ETA: 98s - loss: 4.8632 - acc: 0.0000e+ - ETA: 98s - loss: 4.8602 - acc: 0.0000e+ - ETA: 98s - loss: 4.8525 - acc: 0.0000e+ - ETA: 98s - loss: 4.8565 - acc: 0.0000e+ - ETA: 97s - loss: 4.8566 - acc: 0.0056   - ETA: 97s - loss: 4.8588 - acc: 0.00 - ETA: 96s - loss: 4.8657 - acc: 0.00 - ETA: 96s - loss: 4.8669 - acc: 0.00 - ETA: 96s - loss: 4.8632 - acc: 0.00 - ETA: 96s - loss: 4.8626 - acc: 0.00 - ETA: 95s - loss: 4.8619 - acc: 0.00 - ETA: 95s - loss: 4.8629 - acc: 0.00 - ETA: 95s - loss: 4.8637 - acc: 0.00 - ETA: 94s - loss: 4.8621 - acc: 0.00 - ETA: 94s - loss: 4.8607 - acc: 0.00 - ETA: 94s - loss: 4.8579 - acc: 0.00 - ETA: 94s - loss: 4.8631 - acc: 0.00 - ETA: 93s - loss: 4.8659 - acc: 0.00 - ETA: 93s - loss: 4.8695 - acc: 0.00 - ETA: 93s - loss: 4.8717 - acc: 0.00 - ETA: 93s - loss: 4.8725 - acc: 0.00 - ETA: 92s - loss: 4.8710 - acc: 0.00 - ETA: 92s - loss: 4.8702 - acc: 0.00 - ETA: 92s - loss: 4.8702 - acc: 0.00 - ETA: 91s - loss: 4.8691 - acc: 0.00 - ETA: 91s - loss: 4.8700 - acc: 0.00 - ETA: 91s - loss: 4.8701 - acc: 0.00 - ETA: 91s - loss: 4.8701 - acc: 0.00 - ETA: 90s - loss: 4.8714 - acc: 0.00 - ETA: 90s - loss: 4.8710 - acc: 0.00 - ETA: 90s - loss: 4.8698 - acc: 0.00 - ETA: 89s - loss: 4.8729 - acc: 0.00 - ETA: 89s - loss: 4.8736 - acc: 0.00 - ETA: 89s - loss: 4.8731 - acc: 0.00 - ETA: 88s - loss: 4.8720 - acc: 0.00 - ETA: 88s - loss: 4.8708 - acc: 0.00 - ETA: 88s - loss: 4.8703 - acc: 0.00 - ETA: 88s - loss: 4.8691 - acc: 0.00 - ETA: 87s - loss: 4.8687 - acc: 0.00 - ETA: 87s - loss: 4.8681 - acc: 0.00 - ETA: 87s - loss: 4.8674 - acc: 0.00 - ETA: 87s - loss: 4.8676 - acc: 0.00 - ETA: 86s - loss: 4.8689 - acc: 0.00 - ETA: 86s - loss: 4.8684 - acc: 0.00 - ETA: 86s - loss: 4.8695 - acc: 0.00 - ETA: 85s - loss: 4.8696 - acc: 0.00 - ETA: 85s - loss: 4.8695 - acc: 0.00 - ETA: 85s - loss: 4.8705 - acc: 0.00 - ETA: 84s - loss: 4.8719 - acc: 0.00 - ETA: 84s - loss: 4.8709 - acc: 0.01 - ETA: 84s - loss: 4.8705 - acc: 0.01 - ETA: 83s - loss: 4.8720 - acc: 0.00 - ETA: 83s - loss: 4.8717 - acc: 0.00 - ETA: 83s - loss: 4.8713 - acc: 0.00 - ETA: 83s - loss: 4.8703 - acc: 0.00 - ETA: 82s - loss: 4.8708 - acc: 0.00 - ETA: 82s - loss: 4.8708 - acc: 0.00 - ETA: 82s - loss: 4.8706 - acc: 0.00 - ETA: 81s - loss: 4.8702 - acc: 0.00 - ETA: 81s - loss: 4.8695 - acc: 0.00 - ETA: 81s - loss: 4.8698 - acc: 0.00 - ETA: 80s - loss: 4.8704 - acc: 0.00 - ETA: 80s - loss: 4.8709 - acc: 0.00 - ETA: 80s - loss: 4.8709 - acc: 0.00 - ETA: 79s - loss: 4.8707 - acc: 0.00 - ETA: 79s - loss: 4.8703 - acc: 0.00 - ETA: 79s - loss: 4.8698 - acc: 0.00 - ETA: 79s - loss: 4.8690 - acc: 0.00 - ETA: 78s - loss: 4.8690 - acc: 0.00 - ETA: 78s - loss: 4.8691 - acc: 0.00 - ETA: 78s - loss: 4.8683 - acc: 0.00 - ETA: 77s - loss: 4.8694 - acc: 0.00 - ETA: 77s - loss: 4.8696 - acc: 0.00 - ETA: 77s - loss: 4.8698 - acc: 0.00 - ETA: 76s - loss: 4.8697 - acc: 0.00 - ETA: 76s - loss: 4.8689 - acc: 0.00 - ETA: 76s - loss: 4.8702 - acc: 0.00 - ETA: 76s - loss: 4.8704 - acc: 0.00 - ETA: 75s - loss: 4.8699 - acc: 0.00 - ETA: 75s - loss: 4.8700 - acc: 0.00 - ETA: 75s - loss: 4.8699 - acc: 0.00 - ETA: 74s - loss: 4.8688 - acc: 0.00 - ETA: 74s - loss: 4.8688 - acc: 0.00 - ETA: 74s - loss: 4.8680 - acc: 0.00 - ETA: 73s - loss: 4.8680 - acc: 0.00 - ETA: 73s - loss: 4.8680 - acc: 0.00 - ETA: 73s - loss: 4.8673 - acc: 0.00 - ETA: 73s - loss: 4.8672 - acc: 0.00 - ETA: 72s - loss: 4.8686 - acc: 0.00 - ETA: 72s - loss: 4.8674 - acc: 0.00 - ETA: 72s - loss: 4.8686 - acc: 0.00 - ETA: 71s - loss: 4.8693 - acc: 0.00 - ETA: 71s - loss: 4.8691 - acc: 0.00 - ETA: 71s - loss: 4.8694 - acc: 0.00 - ETA: 70s - loss: 4.8691 - acc: 0.00 - ETA: 70s - loss: 4.8692 - acc: 0.00 - ETA: 70s - loss: 4.8688 - acc: 0.00 - ETA: 70s - loss: 4.8686 - acc: 0.00 - ETA: 69s - loss: 4.8683 - acc: 0.00 - ETA: 69s - loss: 4.8683 - acc: 0.00 - ETA: 69s - loss: 4.8683 - acc: 0.00 - ETA: 68s - loss: 4.8683 - acc: 0.00 - ETA: 68s - loss: 4.8682 - acc: 0.00 - ETA: 68s - loss: 4.8687 - acc: 0.00 - ETA: 67s - loss: 4.8684 - acc: 0.00 - ETA: 67s - loss: 4.8684 - acc: 0.00 - ETA: 67s - loss: 4.8675 - acc: 0.00 - ETA: 66s - loss: 4.8672 - acc: 0.00 - ETA: 66s - loss: 4.8675 - acc: 0.00 - ETA: 66s - loss: 4.8680 - acc: 0.00 - ETA: 66s - loss: 4.8680 - acc: 0.00 - ETA: 65s - loss: 4.8680 - acc: 0.00 - ETA: 65s - loss: 4.8677 - acc: 0.00 - ETA: 65s - loss: 4.8680 - acc: 0.00 - ETA: 64s - loss: 4.8678 - acc: 0.00 - ETA: 64s - loss: 4.8681 - acc: 0.00 - ETA: 64s - loss: 4.8685 - acc: 0.00 - ETA: 63s - loss: 4.8682 - acc: 0.00 - ETA: 63s - loss: 4.8688 - acc: 0.00 - ETA: 63s - loss: 4.8686 - acc: 0.00 - ETA: 63s - loss: 4.8690 - acc: 0.00 - ETA: 62s - loss: 4.8690 - acc: 0.00 - ETA: 62s - loss: 4.8688 - acc: 0.00 - ETA: 62s - loss: 4.8688 - acc: 0.00 - ETA: 61s - loss: 4.8683 - acc: 0.00 - ETA: 61s - loss: 4.8687 - acc: 0.00 - ETA: 61s - loss: 4.8684 - acc: 0.00 - ETA: 60s - loss: 4.8684 - acc: 0.00 - ETA: 60s - loss: 4.8687 - acc: 0.00 - ETA: 60s - loss: 4.8688 - acc: 0.00 - ETA: 60s - loss: 4.8692 - acc: 0.00 - ETA: 59s - loss: 4.8692 - acc: 0.00 - ETA: 59s - loss: 4.8688 - acc: 0.00 - ETA: 59s - loss: 4.8690 - acc: 0.00 - ETA: 58s - loss: 4.8690 - acc: 0.00 - ETA: 58s - loss: 4.8683 - acc: 0.00 - ETA: 58s - loss: 4.8685 - acc: 0.00 - ETA: 57s - loss: 4.8681 - acc: 0.00 - ETA: 57s - loss: 4.8684 - acc: 0.00 - ETA: 57s - loss: 4.8683 - acc: 0.00 - ETA: 57s - loss: 4.8681 - acc: 0.00 - ETA: 56s - loss: 4.8673 - acc: 0.00 - ETA: 56s - loss: 4.8679 - acc: 0.00 - ETA: 56s - loss: 4.8682 - acc: 0.00 - ETA: 55s - loss: 4.8685 - acc: 0.00 - ETA: 55s - loss: 4.8686 - acc: 0.00 - ETA: 55s - loss: 4.8689 - acc: 0.00 - ETA: 54s - loss: 4.8692 - acc: 0.00 - ETA: 54s - loss: 4.8688 - acc: 0.00 - ETA: 54s - loss: 4.8688 - acc: 0.00 - ETA: 53s - loss: 4.8685 - acc: 0.00 - ETA: 53s - loss: 4.8685 - acc: 0.00 - ETA: 53s - loss: 4.8685 - acc: 0.00 - ETA: 53s - loss: 4.8684 - acc: 0.00 - ETA: 52s - loss: 4.8684 - acc: 0.00 - ETA: 52s - loss: 4.8681 - acc: 0.00 - ETA: 52s - loss: 4.8683 - acc: 0.00 - ETA: 51s - loss: 4.8682 - acc: 0.00 - ETA: 51s - loss: 4.8682 - acc: 0.00 - ETA: 51s - loss: 4.8685 - acc: 0.00 - ETA: 50s - loss: 4.8684 - acc: 0.00 - ETA: 50s - loss: 4.8683 - acc: 0.00 - ETA: 50s - loss: 4.8680 - acc: 0.00 - ETA: 50s - loss: 4.8684 - acc: 0.00 - ETA: 49s - loss: 4.8686 - acc: 0.00 - ETA: 49s - loss: 4.8683 - acc: 0.00 - ETA: 49s - loss: 4.8684 - acc: 0.00 - ETA: 48s - loss: 4.8686 - acc: 0.00 - ETA: 48s - loss: 4.8683 - acc: 0.01 - ETA: 48s - loss: 4.8678 - acc: 0.01 - ETA: 47s - loss: 4.8682 - acc: 0.01 - ETA: 47s - loss: 4.8687 - acc: 0.01 - ETA: 47s - loss: 4.8690 - acc: 0.01 - ETA: 47s - loss: 4.8693 - acc: 0.01 - ETA: 46s - loss: 4.8687 - acc: 0.01 - ETA: 46s - loss: 4.8685 - acc: 0.01 - ETA: 46s - loss: 4.8684 - acc: 0.01 - ETA: 45s - loss: 4.8682 - acc: 0.01 - ETA: 45s - loss: 4.8682 - acc: 0.01 - ETA: 45s - loss: 4.8683 - acc: 0.01 - ETA: 44s - loss: 4.8682 - acc: 0.01 - ETA: 44s - loss: 4.8680 - acc: 0.01 - ETA: 44s - loss: 4.8679 - acc: 0.01 - ETA: 44s - loss: 4.8678 - acc: 0.01 - ETA: 43s - loss: 4.8675 - acc: 0.01 - ETA: 43s - loss: 4.8674 - acc: 0.01 - ETA: 43s - loss: 4.8673 - acc: 0.01 - ETA: 42s - loss: 4.8669 - acc: 0.01 - ETA: 42s - loss: 4.8669 - acc: 0.01 - ETA: 42s - loss: 4.8666 - acc: 0.01 - ETA: 41s - loss: 4.8668 - acc: 0.01 - ETA: 41s - loss: 4.8672 - acc: 0.01 - ETA: 41s - loss: 4.8669 - acc: 0.01 - ETA: 41s - loss: 4.8671 - acc: 0.01 - ETA: 40s - loss: 4.8674 - acc: 0.01 - ETA: 40s - loss: 4.8674 - acc: 0.01 - ETA: 40s - loss: 4.8672 - acc: 0.01 - ETA: 39s - loss: 4.8671 - acc: 0.01 - ETA: 39s - loss: 4.8673 - acc: 0.01 - ETA: 39s - loss: 4.8679 - acc: 0.01 - ETA: 38s - loss: 4.8676 - acc: 0.01 - ETA: 38s - loss: 4.8675 - acc: 0.01 - ETA: 38s - loss: 4.8678 - acc: 0.01 - ETA: 38s - loss: 4.8674 - acc: 0.01 - ETA: 37s - loss: 4.8674 - acc: 0.01 - ETA: 37s - loss: 4.8675 - acc: 0.01 - ETA: 37s - loss: 4.8675 - acc: 0.01 - ETA: 36s - loss: 4.8675 - acc: 0.016660/6680 [============================>.] - ETA: 36s - loss: 4.8671 - acc: 0.01 - ETA: 36s - loss: 4.8667 - acc: 0.01 - ETA: 35s - loss: 4.8664 - acc: 0.01 - ETA: 35s - loss: 4.8663 - acc: 0.01 - ETA: 35s - loss: 4.8664 - acc: 0.01 - ETA: 34s - loss: 4.8662 - acc: 0.01 - ETA: 34s - loss: 4.8660 - acc: 0.01 - ETA: 34s - loss: 4.8659 - acc: 0.01 - ETA: 34s - loss: 4.8654 - acc: 0.01 - ETA: 33s - loss: 4.8655 - acc: 0.01 - ETA: 33s - loss: 4.8649 - acc: 0.01 - ETA: 33s - loss: 4.8651 - acc: 0.01 - ETA: 32s - loss: 4.8648 - acc: 0.01 - ETA: 32s - loss: 4.8642 - acc: 0.01 - ETA: 32s - loss: 4.8643 - acc: 0.01 - ETA: 31s - loss: 4.8644 - acc: 0.01 - ETA: 31s - loss: 4.8643 - acc: 0.01 - ETA: 31s - loss: 4.8642 - acc: 0.01 - ETA: 31s - loss: 4.8643 - acc: 0.01 - ETA: 30s - loss: 4.8645 - acc: 0.01 - ETA: 30s - loss: 4.8645 - acc: 0.01 - ETA: 30s - loss: 4.8644 - acc: 0.01 - ETA: 29s - loss: 4.8640 - acc: 0.01 - ETA: 29s - loss: 4.8638 - acc: 0.01 - ETA: 29s - loss: 4.8634 - acc: 0.01 - ETA: 28s - loss: 4.8637 - acc: 0.01 - ETA: 28s - loss: 4.8636 - acc: 0.01 - ETA: 28s - loss: 4.8637 - acc: 0.01 - ETA: 28s - loss: 4.8635 - acc: 0.01 - ETA: 27s - loss: 4.8633 - acc: 0.01 - ETA: 27s - loss: 4.8636 - acc: 0.01 - ETA: 27s - loss: 4.8634 - acc: 0.01 - ETA: 26s - loss: 4.8635 - acc: 0.01 - ETA: 26s - loss: 4.8634 - acc: 0.01 - ETA: 26s - loss: 4.8631 - acc: 0.01 - ETA: 25s - loss: 4.8634 - acc: 0.01 - ETA: 25s - loss: 4.8630 - acc: 0.01 - ETA: 25s - loss: 4.8631 - acc: 0.01 - ETA: 25s - loss: 4.8632 - acc: 0.01 - ETA: 24s - loss: 4.8630 - acc: 0.01 - ETA: 24s - loss: 4.8631 - acc: 0.01 - ETA: 24s - loss: 4.8631 - acc: 0.01 - ETA: 23s - loss: 4.8631 - acc: 0.01 - ETA: 23s - loss: 4.8632 - acc: 0.01 - ETA: 23s - loss: 4.8631 - acc: 0.01 - ETA: 22s - loss: 4.8629 - acc: 0.01 - ETA: 22s - loss: 4.8629 - acc: 0.01 - ETA: 22s - loss: 4.8630 - acc: 0.01 - ETA: 21s - loss: 4.8629 - acc: 0.01 - ETA: 21s - loss: 4.8629 - acc: 0.01 - ETA: 21s - loss: 4.8630 - acc: 0.01 - ETA: 21s - loss: 4.8625 - acc: 0.01 - ETA: 20s - loss: 4.8628 - acc: 0.01 - ETA: 20s - loss: 4.8626 - acc: 0.01 - ETA: 20s - loss: 4.8626 - acc: 0.01 - ETA: 19s - loss: 4.8626 - acc: 0.01 - ETA: 19s - loss: 4.8624 - acc: 0.01 - ETA: 19s - loss: 4.8623 - acc: 0.01 - ETA: 18s - loss: 4.8624 - acc: 0.01 - ETA: 18s - loss: 4.8619 - acc: 0.01 - ETA: 18s - loss: 4.8618 - acc: 0.01 - ETA: 18s - loss: 4.8623 - acc: 0.01 - ETA: 17s - loss: 4.8623 - acc: 0.01 - ETA: 17s - loss: 4.8625 - acc: 0.01 - ETA: 17s - loss: 4.8623 - acc: 0.01 - ETA: 16s - loss: 4.8623 - acc: 0.01 - ETA: 16s - loss: 4.8617 - acc: 0.01 - ETA: 16s - loss: 4.8616 - acc: 0.01 - ETA: 15s - loss: 4.8611 - acc: 0.01 - ETA: 15s - loss: 4.8612 - acc: 0.01 - ETA: 15s - loss: 4.8609 - acc: 0.01 - ETA: 15s - loss: 4.8605 - acc: 0.01 - ETA: 14s - loss: 4.8600 - acc: 0.01 - ETA: 14s - loss: 4.8599 - acc: 0.01 - ETA: 14s - loss: 4.8602 - acc: 0.01 - ETA: 13s - loss: 4.8600 - acc: 0.01 - ETA: 13s - loss: 4.8595 - acc: 0.01 - ETA: 13s - loss: 4.8589 - acc: 0.01 - ETA: 12s - loss: 4.8589 - acc: 0.01 - ETA: 12s - loss: 4.8586 - acc: 0.01 - ETA: 12s - loss: 4.8582 - acc: 0.01 - ETA: 12s - loss: 4.8585 - acc: 0.01 - ETA: 11s - loss: 4.8582 - acc: 0.01 - ETA: 11s - loss: 4.8586 - acc: 0.01 - ETA: 11s - loss: 4.8584 - acc: 0.01 - ETA: 10s - loss: 4.8579 - acc: 0.01 - ETA: 10s - loss: 4.8583 - acc: 0.01 - ETA: 10s - loss: 4.8579 - acc: 0.01 - ETA: 9s - loss: 4.8581 - acc: 0.0151 - ETA: 9s - loss: 4.8583 - acc: 0.015 - ETA: 9s - loss: 4.8579 - acc: 0.015 - ETA: 9s - loss: 4.8579 - acc: 0.015 - ETA: 8s - loss: 4.8578 - acc: 0.015 - ETA: 8s - loss: 4.8575 - acc: 0.015 - ETA: 8s - loss: 4.8575 - acc: 0.015 - ETA: 7s - loss: 4.8576 - acc: 0.015 - ETA: 7s - loss: 4.8577 - acc: 0.015 - ETA: 7s - loss: 4.8575 - acc: 0.015 - ETA: 6s - loss: 4.8575 - acc: 0.015 - ETA: 6s - loss: 4.8573 - acc: 0.015 - ETA: 6s - loss: 4.8576 - acc: 0.015 - ETA: 6s - loss: 4.8574 - acc: 0.015 - ETA: 5s - loss: 4.8574 - acc: 0.015 - ETA: 5s - loss: 4.8573 - acc: 0.015 - ETA: 5s - loss: 4.8571 - acc: 0.015 - ETA: 4s - loss: 4.8572 - acc: 0.015 - ETA: 4s - loss: 4.8570 - acc: 0.015 - ETA: 4s - loss: 4.8569 - acc: 0.015 - ETA: 3s - loss: 4.8566 - acc: 0.015 - ETA: 3s - loss: 4.8564 - acc: 0.015 - ETA: 3s - loss: 4.8562 - acc: 0.015 - ETA: 3s - loss: 4.8560 - acc: 0.015 - ETA: 2s - loss: 4.8560 - acc: 0.015 - ETA: 2s - loss: 4.8554 - acc: 0.015 - ETA: 2s - loss: 4.8556 - acc: 0.015 - ETA: 1s - loss: 4.8554 - acc: 0.015 - ETA: 1s - loss: 4.8554 - acc: 0.015 - ETA: 1s - loss: 4.8551 - acc: 0.015 - ETA: 0s - loss: 4.8555 - acc: 0.015 - ETA: 0s - loss: 4.8556 - acc: 0.015 - ETA: 0s - loss: 4.8556 - acc: 0.0150Epoch 00000: val_loss improved from inf to 4.84389, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 104s - loss: 4.8553 - acc: 0.0150 - val_loss: 4.8439 - val_acc: 0.0168\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 96s - loss: 4.7987 - acc: 0.10 - ETA: 96s - loss: 4.7642 - acc: 0.05 - ETA: 96s - loss: 4.7962 - acc: 0.03 - ETA: 97s - loss: 4.7761 - acc: 0.07 - ETA: 96s - loss: 4.7747 - acc: 0.06 - ETA: 96s - loss: 4.7748 - acc: 0.05 - ETA: 96s - loss: 4.7805 - acc: 0.05 - ETA: 95s - loss: 4.7760 - acc: 0.04 - ETA: 96s - loss: 4.7714 - acc: 0.04 - ETA: 95s - loss: 4.7774 - acc: 0.04 - ETA: 96s - loss: 4.7786 - acc: 0.04 - ETA: 95s - loss: 4.7804 - acc: 0.03 - ETA: 95s - loss: 4.7889 - acc: 0.03 - ETA: 94s - loss: 4.7916 - acc: 0.03 - ETA: 94s - loss: 4.7986 - acc: 0.03 - ETA: 94s - loss: 4.8033 - acc: 0.03 - ETA: 94s - loss: 4.8101 - acc: 0.02 - ETA: 93s - loss: 4.8124 - acc: 0.02 - ETA: 93s - loss: 4.8179 - acc: 0.02 - ETA: 93s - loss: 4.8186 - acc: 0.02 - ETA: 92s - loss: 4.8144 - acc: 0.02 - ETA: 92s - loss: 4.8163 - acc: 0.02 - ETA: 92s - loss: 4.8139 - acc: 0.02 - ETA: 91s - loss: 4.8147 - acc: 0.02 - ETA: 91s - loss: 4.8201 - acc: 0.02 - ETA: 91s - loss: 4.8215 - acc: 0.02 - ETA: 90s - loss: 4.8217 - acc: 0.02 - ETA: 90s - loss: 4.8242 - acc: 0.02 - ETA: 90s - loss: 4.8235 - acc: 0.02 - ETA: 90s - loss: 4.8234 - acc: 0.03 - ETA: 89s - loss: 4.8228 - acc: 0.03 - ETA: 89s - loss: 4.8222 - acc: 0.02 - ETA: 89s - loss: 4.8142 - acc: 0.03 - ETA: 89s - loss: 4.8096 - acc: 0.03 - ETA: 88s - loss: 4.8094 - acc: 0.03 - ETA: 88s - loss: 4.8082 - acc: 0.02 - ETA: 88s - loss: 4.8117 - acc: 0.02 - ETA: 87s - loss: 4.8122 - acc: 0.02 - ETA: 87s - loss: 4.8163 - acc: 0.02 - ETA: 87s - loss: 4.8157 - acc: 0.03 - ETA: 86s - loss: 4.8177 - acc: 0.02 - ETA: 86s - loss: 4.8166 - acc: 0.02 - ETA: 86s - loss: 4.8190 - acc: 0.02 - ETA: 85s - loss: 4.8171 - acc: 0.02 - ETA: 85s - loss: 4.8173 - acc: 0.02 - ETA: 85s - loss: 4.8170 - acc: 0.02 - ETA: 85s - loss: 4.8192 - acc: 0.02 - ETA: 84s - loss: 4.8193 - acc: 0.02 - ETA: 84s - loss: 4.8225 - acc: 0.02 - ETA: 84s - loss: 4.8228 - acc: 0.02 - ETA: 83s - loss: 4.8216 - acc: 0.02 - ETA: 83s - loss: 4.8222 - acc: 0.02 - ETA: 83s - loss: 4.8219 - acc: 0.02 - ETA: 82s - loss: 4.8214 - acc: 0.02 - ETA: 82s - loss: 4.8208 - acc: 0.02 - ETA: 82s - loss: 4.8204 - acc: 0.02 - ETA: 82s - loss: 4.8241 - acc: 0.02 - ETA: 81s - loss: 4.8246 - acc: 0.02 - ETA: 81s - loss: 4.8250 - acc: 0.02 - ETA: 81s - loss: 4.8253 - acc: 0.02 - ETA: 80s - loss: 4.8249 - acc: 0.02 - ETA: 80s - loss: 4.8281 - acc: 0.02 - ETA: 80s - loss: 4.8274 - acc: 0.02 - ETA: 79s - loss: 4.8273 - acc: 0.02 - ETA: 79s - loss: 4.8266 - acc: 0.02 - ETA: 79s - loss: 4.8268 - acc: 0.02 - ETA: 79s - loss: 4.8258 - acc: 0.02 - ETA: 78s - loss: 4.8260 - acc: 0.02 - ETA: 78s - loss: 4.8271 - acc: 0.02 - ETA: 78s - loss: 4.8266 - acc: 0.02 - ETA: 77s - loss: 4.8271 - acc: 0.02 - ETA: 77s - loss: 4.8262 - acc: 0.02 - ETA: 77s - loss: 4.8283 - acc: 0.02 - ETA: 76s - loss: 4.8282 - acc: 0.02 - ETA: 76s - loss: 4.8262 - acc: 0.02 - ETA: 76s - loss: 4.8283 - acc: 0.02 - ETA: 76s - loss: 4.8281 - acc: 0.02 - ETA: 75s - loss: 4.8281 - acc: 0.02 - ETA: 75s - loss: 4.8275 - acc: 0.02 - ETA: 75s - loss: 4.8280 - acc: 0.02 - ETA: 74s - loss: 4.8280 - acc: 0.01 - ETA: 74s - loss: 4.8309 - acc: 0.01 - ETA: 74s - loss: 4.8319 - acc: 0.01 - ETA: 73s - loss: 4.8318 - acc: 0.01 - ETA: 73s - loss: 4.8320 - acc: 0.01 - ETA: 73s - loss: 4.8319 - acc: 0.01 - ETA: 73s - loss: 4.8324 - acc: 0.01 - ETA: 72s - loss: 4.8321 - acc: 0.01 - ETA: 72s - loss: 4.8316 - acc: 0.01 - ETA: 72s - loss: 4.8311 - acc: 0.01 - ETA: 71s - loss: 4.8321 - acc: 0.01 - ETA: 71s - loss: 4.8330 - acc: 0.01 - ETA: 71s - loss: 4.8324 - acc: 0.01 - ETA: 70s - loss: 4.8322 - acc: 0.01 - ETA: 70s - loss: 4.8330 - acc: 0.01 - ETA: 70s - loss: 4.8324 - acc: 0.01 - ETA: 70s - loss: 4.8330 - acc: 0.01 - ETA: 69s - loss: 4.8326 - acc: 0.01 - ETA: 69s - loss: 4.8321 - acc: 0.01 - ETA: 69s - loss: 4.8331 - acc: 0.01 - ETA: 68s - loss: 4.8326 - acc: 0.01 - ETA: 68s - loss: 4.8320 - acc: 0.01 - ETA: 68s - loss: 4.8324 - acc: 0.01 - ETA: 68s - loss: 4.8318 - acc: 0.01 - ETA: 67s - loss: 4.8319 - acc: 0.01 - ETA: 67s - loss: 4.8316 - acc: 0.01 - ETA: 67s - loss: 4.8305 - acc: 0.01 - ETA: 66s - loss: 4.8303 - acc: 0.01 - ETA: 66s - loss: 4.8296 - acc: 0.01 - ETA: 66s - loss: 4.8298 - acc: 0.01 - ETA: 65s - loss: 4.8295 - acc: 0.01 - ETA: 65s - loss: 4.8284 - acc: 0.01 - ETA: 65s - loss: 4.8291 - acc: 0.01 - ETA: 65s - loss: 4.8287 - acc: 0.01 - ETA: 64s - loss: 4.8284 - acc: 0.01 - ETA: 64s - loss: 4.8275 - acc: 0.01 - ETA: 64s - loss: 4.8270 - acc: 0.01 - ETA: 63s - loss: 4.8258 - acc: 0.01 - ETA: 63s - loss: 4.8256 - acc: 0.01 - ETA: 63s - loss: 4.8253 - acc: 0.01 - ETA: 62s - loss: 4.8233 - acc: 0.01 - ETA: 62s - loss: 4.8233 - acc: 0.01 - ETA: 62s - loss: 4.8229 - acc: 0.01 - ETA: 62s - loss: 4.8253 - acc: 0.01 - ETA: 61s - loss: 4.8258 - acc: 0.01 - ETA: 61s - loss: 4.8256 - acc: 0.01 - ETA: 61s - loss: 4.8248 - acc: 0.01 - ETA: 60s - loss: 4.8233 - acc: 0.01 - ETA: 60s - loss: 4.8240 - acc: 0.01 - ETA: 60s - loss: 4.8247 - acc: 0.01 - ETA: 60s - loss: 4.8257 - acc: 0.01 - ETA: 59s - loss: 4.8250 - acc: 0.01 - ETA: 59s - loss: 4.8238 - acc: 0.01 - ETA: 59s - loss: 4.8234 - acc: 0.01 - ETA: 58s - loss: 4.8236 - acc: 0.01 - ETA: 58s - loss: 4.8233 - acc: 0.01 - ETA: 58s - loss: 4.8244 - acc: 0.01 - ETA: 58s - loss: 4.8240 - acc: 0.01 - ETA: 57s - loss: 4.8242 - acc: 0.01 - ETA: 57s - loss: 4.8236 - acc: 0.01 - ETA: 57s - loss: 4.8236 - acc: 0.01 - ETA: 56s - loss: 4.8234 - acc: 0.01 - ETA: 56s - loss: 4.8238 - acc: 0.01 - ETA: 56s - loss: 4.8238 - acc: 0.01 - ETA: 55s - loss: 4.8244 - acc: 0.01 - ETA: 55s - loss: 4.8240 - acc: 0.01 - ETA: 55s - loss: 4.8238 - acc: 0.01 - ETA: 55s - loss: 4.8236 - acc: 0.01 - ETA: 54s - loss: 4.8239 - acc: 0.01 - ETA: 54s - loss: 4.8239 - acc: 0.01 - ETA: 54s - loss: 4.8235 - acc: 0.01 - ETA: 53s - loss: 4.8243 - acc: 0.01 - ETA: 53s - loss: 4.8243 - acc: 0.01 - ETA: 53s - loss: 4.8234 - acc: 0.01 - ETA: 52s - loss: 4.8234 - acc: 0.01 - ETA: 52s - loss: 4.8235 - acc: 0.01 - ETA: 52s - loss: 4.8231 - acc: 0.01 - ETA: 52s - loss: 4.8231 - acc: 0.01 - ETA: 51s - loss: 4.8234 - acc: 0.01 - ETA: 51s - loss: 4.8234 - acc: 0.01 - ETA: 51s - loss: 4.8235 - acc: 0.01 - ETA: 50s - loss: 4.8232 - acc: 0.01 - ETA: 50s - loss: 4.8226 - acc: 0.01 - ETA: 50s - loss: 4.8221 - acc: 0.01 - ETA: 50s - loss: 4.8221 - acc: 0.01 - ETA: 49s - loss: 4.8216 - acc: 0.01 - ETA: 49s - loss: 4.8212 - acc: 0.01 - ETA: 49s - loss: 4.8209 - acc: 0.01 - ETA: 48s - loss: 4.8204 - acc: 0.01 - ETA: 48s - loss: 4.8201 - acc: 0.01 - ETA: 48s - loss: 4.8208 - acc: 0.01 - ETA: 47s - loss: 4.8208 - acc: 0.01 - ETA: 47s - loss: 4.8198 - acc: 0.01 - ETA: 47s - loss: 4.8203 - acc: 0.01 - ETA: 47s - loss: 4.8204 - acc: 0.01 - ETA: 46s - loss: 4.8195 - acc: 0.01 - ETA: 46s - loss: 4.8196 - acc: 0.01 - ETA: 46s - loss: 4.8201 - acc: 0.01 - ETA: 45s - loss: 4.8210 - acc: 0.01 - ETA: 45s - loss: 4.8211 - acc: 0.01 - ETA: 45s - loss: 4.8210 - acc: 0.01 - ETA: 44s - loss: 4.8209 - acc: 0.01 - ETA: 44s - loss: 4.8198 - acc: 0.01 - ETA: 44s - loss: 4.8192 - acc: 0.01 - ETA: 44s - loss: 4.8192 - acc: 0.01 - ETA: 43s - loss: 4.8185 - acc: 0.01 - ETA: 43s - loss: 4.8179 - acc: 0.01 - ETA: 43s - loss: 4.8188 - acc: 0.01 - ETA: 42s - loss: 4.8186 - acc: 0.01 - ETA: 42s - loss: 4.8192 - acc: 0.01 - ETA: 42s - loss: 4.8193 - acc: 0.01 - ETA: 42s - loss: 4.8199 - acc: 0.01 - ETA: 41s - loss: 4.8192 - acc: 0.01 - ETA: 41s - loss: 4.8192 - acc: 0.01 - ETA: 41s - loss: 4.8199 - acc: 0.01 - ETA: 40s - loss: 4.8198 - acc: 0.01 - ETA: 40s - loss: 4.8197 - acc: 0.01 - ETA: 40s - loss: 4.8193 - acc: 0.01 - ETA: 39s - loss: 4.8195 - acc: 0.01 - ETA: 39s - loss: 4.8185 - acc: 0.01 - ETA: 39s - loss: 4.8185 - acc: 0.01 - ETA: 39s - loss: 4.8181 - acc: 0.01 - ETA: 38s - loss: 4.8183 - acc: 0.01 - ETA: 38s - loss: 4.8180 - acc: 0.01 - ETA: 38s - loss: 4.8182 - acc: 0.01 - ETA: 37s - loss: 4.8174 - acc: 0.01 - ETA: 37s - loss: 4.8173 - acc: 0.01 - ETA: 37s - loss: 4.8167 - acc: 0.01 - ETA: 36s - loss: 4.8164 - acc: 0.01 - ETA: 36s - loss: 4.8165 - acc: 0.01 - ETA: 36s - loss: 4.8166 - acc: 0.01 - ETA: 36s - loss: 4.8165 - acc: 0.01 - ETA: 35s - loss: 4.8175 - acc: 0.01 - ETA: 35s - loss: 4.8175 - acc: 0.01 - ETA: 35s - loss: 4.8173 - acc: 0.0172"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 34s - loss: 4.8173 - acc: 0.01 - ETA: 34s - loss: 4.8172 - acc: 0.01 - ETA: 34s - loss: 4.8168 - acc: 0.01 - ETA: 34s - loss: 4.8164 - acc: 0.01 - ETA: 33s - loss: 4.8164 - acc: 0.01 - ETA: 33s - loss: 4.8159 - acc: 0.01 - ETA: 33s - loss: 4.8162 - acc: 0.01 - ETA: 32s - loss: 4.8162 - acc: 0.01 - ETA: 32s - loss: 4.8164 - acc: 0.01 - ETA: 32s - loss: 4.8165 - acc: 0.01 - ETA: 31s - loss: 4.8166 - acc: 0.01 - ETA: 31s - loss: 4.8162 - acc: 0.01 - ETA: 31s - loss: 4.8160 - acc: 0.01 - ETA: 31s - loss: 4.8162 - acc: 0.01 - ETA: 30s - loss: 4.8162 - acc: 0.01 - ETA: 30s - loss: 4.8165 - acc: 0.01 - ETA: 30s - loss: 4.8160 - acc: 0.01 - ETA: 29s - loss: 4.8155 - acc: 0.01 - ETA: 29s - loss: 4.8149 - acc: 0.01 - ETA: 29s - loss: 4.8153 - acc: 0.01 - ETA: 28s - loss: 4.8147 - acc: 0.01 - ETA: 28s - loss: 4.8149 - acc: 0.01 - ETA: 28s - loss: 4.8155 - acc: 0.01 - ETA: 28s - loss: 4.8153 - acc: 0.01 - ETA: 27s - loss: 4.8154 - acc: 0.01 - ETA: 27s - loss: 4.8148 - acc: 0.01 - ETA: 27s - loss: 4.8145 - acc: 0.01 - ETA: 26s - loss: 4.8149 - acc: 0.01 - ETA: 26s - loss: 4.8159 - acc: 0.01 - ETA: 26s - loss: 4.8162 - acc: 0.01 - ETA: 26s - loss: 4.8161 - acc: 0.01 - ETA: 25s - loss: 4.8157 - acc: 0.01 - ETA: 25s - loss: 4.8154 - acc: 0.01 - ETA: 25s - loss: 4.8155 - acc: 0.01 - ETA: 24s - loss: 4.8152 - acc: 0.01 - ETA: 24s - loss: 4.8154 - acc: 0.01 - ETA: 24s - loss: 4.8154 - acc: 0.01 - ETA: 23s - loss: 4.8152 - acc: 0.01 - ETA: 23s - loss: 4.8151 - acc: 0.01 - ETA: 23s - loss: 4.8150 - acc: 0.01 - ETA: 23s - loss: 4.8149 - acc: 0.01 - ETA: 22s - loss: 4.8150 - acc: 0.01 - ETA: 22s - loss: 4.8150 - acc: 0.01 - ETA: 22s - loss: 4.8153 - acc: 0.01 - ETA: 21s - loss: 4.8151 - acc: 0.01 - ETA: 21s - loss: 4.8152 - acc: 0.01 - ETA: 21s - loss: 4.8150 - acc: 0.01 - ETA: 21s - loss: 4.8148 - acc: 0.01 - ETA: 20s - loss: 4.8145 - acc: 0.01 - ETA: 20s - loss: 4.8153 - acc: 0.01 - ETA: 20s - loss: 4.8154 - acc: 0.01 - ETA: 19s - loss: 4.8153 - acc: 0.01 - ETA: 19s - loss: 4.8153 - acc: 0.01 - ETA: 19s - loss: 4.8157 - acc: 0.01 - ETA: 18s - loss: 4.8155 - acc: 0.01 - ETA: 18s - loss: 4.8153 - acc: 0.01 - ETA: 18s - loss: 4.8153 - acc: 0.01 - ETA: 18s - loss: 4.8157 - acc: 0.01 - ETA: 17s - loss: 4.8154 - acc: 0.01 - ETA: 17s - loss: 4.8153 - acc: 0.01 - ETA: 17s - loss: 4.8154 - acc: 0.01 - ETA: 16s - loss: 4.8159 - acc: 0.01 - ETA: 16s - loss: 4.8161 - acc: 0.01 - ETA: 16s - loss: 4.8160 - acc: 0.01 - ETA: 15s - loss: 4.8161 - acc: 0.01 - ETA: 15s - loss: 4.8165 - acc: 0.01 - ETA: 15s - loss: 4.8161 - acc: 0.01 - ETA: 15s - loss: 4.8158 - acc: 0.01 - ETA: 14s - loss: 4.8155 - acc: 0.01 - ETA: 14s - loss: 4.8153 - acc: 0.01 - ETA: 14s - loss: 4.8151 - acc: 0.01 - ETA: 13s - loss: 4.8152 - acc: 0.01 - ETA: 13s - loss: 4.8150 - acc: 0.01 - ETA: 13s - loss: 4.8155 - acc: 0.01 - ETA: 13s - loss: 4.8152 - acc: 0.01 - ETA: 12s - loss: 4.8157 - acc: 0.01 - ETA: 12s - loss: 4.8150 - acc: 0.01 - ETA: 12s - loss: 4.8147 - acc: 0.01 - ETA: 11s - loss: 4.8147 - acc: 0.01 - ETA: 11s - loss: 4.8149 - acc: 0.01 - ETA: 11s - loss: 4.8144 - acc: 0.01 - ETA: 10s - loss: 4.8143 - acc: 0.01 - ETA: 10s - loss: 4.8148 - acc: 0.01 - ETA: 10s - loss: 4.8151 - acc: 0.01 - ETA: 10s - loss: 4.8150 - acc: 0.01 - ETA: 9s - loss: 4.8154 - acc: 0.0164 - ETA: 9s - loss: 4.8159 - acc: 0.016 - ETA: 9s - loss: 4.8157 - acc: 0.016 - ETA: 8s - loss: 4.8154 - acc: 0.016 - ETA: 8s - loss: 4.8153 - acc: 0.016 - ETA: 8s - loss: 4.8154 - acc: 0.016 - ETA: 7s - loss: 4.8153 - acc: 0.016 - ETA: 7s - loss: 4.8152 - acc: 0.016 - ETA: 7s - loss: 4.8149 - acc: 0.016 - ETA: 7s - loss: 4.8150 - acc: 0.016 - ETA: 6s - loss: 4.8150 - acc: 0.016 - ETA: 6s - loss: 4.8151 - acc: 0.016 - ETA: 6s - loss: 4.8148 - acc: 0.016 - ETA: 5s - loss: 4.8152 - acc: 0.016 - ETA: 5s - loss: 4.8153 - acc: 0.016 - ETA: 5s - loss: 4.8148 - acc: 0.016 - ETA: 5s - loss: 4.8146 - acc: 0.016 - ETA: 4s - loss: 4.8145 - acc: 0.016 - ETA: 4s - loss: 4.8144 - acc: 0.016 - ETA: 4s - loss: 4.8145 - acc: 0.016 - ETA: 3s - loss: 4.8143 - acc: 0.016 - ETA: 3s - loss: 4.8143 - acc: 0.016 - ETA: 3s - loss: 4.8142 - acc: 0.016 - ETA: 2s - loss: 4.8141 - acc: 0.016 - ETA: 2s - loss: 4.8137 - acc: 0.016 - ETA: 2s - loss: 4.8139 - acc: 0.016 - ETA: 2s - loss: 4.8138 - acc: 0.016 - ETA: 1s - loss: 4.8142 - acc: 0.016 - ETA: 1s - loss: 4.8135 - acc: 0.016 - ETA: 1s - loss: 4.8131 - acc: 0.016 - ETA: 0s - loss: 4.8126 - acc: 0.016 - ETA: 0s - loss: 4.8124 - acc: 0.016 - ETA: 0s - loss: 4.8119 - acc: 0.0164Epoch 00001: val_loss improved from 4.84389 to 4.82919, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 101s - loss: 4.8116 - acc: 0.0163 - val_loss: 4.8292 - val_acc: 0.0120\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 95s - loss: 4.6127 - acc: 0.10 - ETA: 97s - loss: 4.7237 - acc: 0.05 - ETA: 97s - loss: 4.7765 - acc: 0.03 - ETA: 96s - loss: 4.7730 - acc: 0.02 - ETA: 96s - loss: 4.7801 - acc: 0.02 - ETA: 96s - loss: 4.7843 - acc: 0.01 - ETA: 96s - loss: 4.7711 - acc: 0.01 - ETA: 96s - loss: 4.7811 - acc: 0.01 - ETA: 96s - loss: 4.7805 - acc: 0.01 - ETA: 96s - loss: 4.7783 - acc: 0.01 - ETA: 95s - loss: 4.7635 - acc: 0.01 - ETA: 95s - loss: 4.7742 - acc: 0.01 - ETA: 95s - loss: 4.7858 - acc: 0.01 - ETA: 94s - loss: 4.7933 - acc: 0.01 - ETA: 94s - loss: 4.7966 - acc: 0.01 - ETA: 94s - loss: 4.7895 - acc: 0.00 - ETA: 93s - loss: 4.7808 - acc: 0.01 - ETA: 93s - loss: 4.7841 - acc: 0.01 - ETA: 93s - loss: 4.7855 - acc: 0.01 - ETA: 92s - loss: 4.7864 - acc: 0.01 - ETA: 92s - loss: 4.7842 - acc: 0.00 - ETA: 92s - loss: 4.7855 - acc: 0.00 - ETA: 91s - loss: 4.7911 - acc: 0.00 - ETA: 91s - loss: 4.7957 - acc: 0.01 - ETA: 91s - loss: 4.7916 - acc: 0.01 - ETA: 91s - loss: 4.7929 - acc: 0.01 - ETA: 90s - loss: 4.7921 - acc: 0.01 - ETA: 90s - loss: 4.7908 - acc: 0.01 - ETA: 90s - loss: 4.7879 - acc: 0.01 - ETA: 89s - loss: 4.7938 - acc: 0.01 - ETA: 89s - loss: 4.7923 - acc: 0.01 - ETA: 89s - loss: 4.7884 - acc: 0.01 - ETA: 88s - loss: 4.7869 - acc: 0.01 - ETA: 88s - loss: 4.7821 - acc: 0.01 - ETA: 88s - loss: 4.7809 - acc: 0.01 - ETA: 88s - loss: 4.7778 - acc: 0.01 - ETA: 87s - loss: 4.7756 - acc: 0.01 - ETA: 87s - loss: 4.7689 - acc: 0.01 - ETA: 87s - loss: 4.7701 - acc: 0.01 - ETA: 86s - loss: 4.7751 - acc: 0.01 - ETA: 86s - loss: 4.7775 - acc: 0.01 - ETA: 86s - loss: 4.7797 - acc: 0.01 - ETA: 85s - loss: 4.7787 - acc: 0.01 - ETA: 85s - loss: 4.7806 - acc: 0.01 - ETA: 85s - loss: 4.7820 - acc: 0.01 - ETA: 85s - loss: 4.7808 - acc: 0.01 - ETA: 84s - loss: 4.7775 - acc: 0.01 - ETA: 84s - loss: 4.7776 - acc: 0.01 - ETA: 84s - loss: 4.7777 - acc: 0.01 - ETA: 83s - loss: 4.7756 - acc: 0.01 - ETA: 83s - loss: 4.7749 - acc: 0.01 - ETA: 83s - loss: 4.7741 - acc: 0.01 - ETA: 83s - loss: 4.7748 - acc: 0.01 - ETA: 82s - loss: 4.7730 - acc: 0.01 - ETA: 82s - loss: 4.7730 - acc: 0.01 - ETA: 82s - loss: 4.7746 - acc: 0.01 - ETA: 81s - loss: 4.7765 - acc: 0.01 - ETA: 81s - loss: 4.7783 - acc: 0.01 - ETA: 81s - loss: 4.7788 - acc: 0.01 - ETA: 80s - loss: 4.7753 - acc: 0.01 - ETA: 80s - loss: 4.7751 - acc: 0.01 - ETA: 80s - loss: 4.7751 - acc: 0.01 - ETA: 80s - loss: 4.7748 - acc: 0.01 - ETA: 79s - loss: 4.7747 - acc: 0.01 - ETA: 79s - loss: 4.7730 - acc: 0.01 - ETA: 79s - loss: 4.7725 - acc: 0.01 - ETA: 78s - loss: 4.7746 - acc: 0.01 - ETA: 78s - loss: 4.7751 - acc: 0.01 - ETA: 78s - loss: 4.7749 - acc: 0.01 - ETA: 77s - loss: 4.7739 - acc: 0.01 - ETA: 77s - loss: 4.7772 - acc: 0.01 - ETA: 77s - loss: 4.7793 - acc: 0.01 - ETA: 77s - loss: 4.7810 - acc: 0.01 - ETA: 76s - loss: 4.7820 - acc: 0.01 - ETA: 76s - loss: 4.7820 - acc: 0.01 - ETA: 76s - loss: 4.7823 - acc: 0.01 - ETA: 75s - loss: 4.7842 - acc: 0.01 - ETA: 75s - loss: 4.7839 - acc: 0.01 - ETA: 75s - loss: 4.7838 - acc: 0.01 - ETA: 74s - loss: 4.7824 - acc: 0.01 - ETA: 74s - loss: 4.7823 - acc: 0.01 - ETA: 74s - loss: 4.7818 - acc: 0.01 - ETA: 74s - loss: 4.7829 - acc: 0.01 - ETA: 73s - loss: 4.7804 - acc: 0.01 - ETA: 73s - loss: 4.7806 - acc: 0.01 - ETA: 73s - loss: 4.7804 - acc: 0.01 - ETA: 72s - loss: 4.7799 - acc: 0.01 - ETA: 72s - loss: 4.7800 - acc: 0.01 - ETA: 72s - loss: 4.7817 - acc: 0.01 - ETA: 72s - loss: 4.7797 - acc: 0.01 - ETA: 71s - loss: 4.7807 - acc: 0.01 - ETA: 71s - loss: 4.7840 - acc: 0.01 - ETA: 71s - loss: 4.7832 - acc: 0.01 - ETA: 70s - loss: 4.7841 - acc: 0.01 - ETA: 70s - loss: 4.7857 - acc: 0.01 - ETA: 70s - loss: 4.7849 - acc: 0.01 - ETA: 69s - loss: 4.7843 - acc: 0.01 - ETA: 69s - loss: 4.7841 - acc: 0.01 - ETA: 69s - loss: 4.7867 - acc: 0.01 - ETA: 69s - loss: 4.7872 - acc: 0.01 - ETA: 68s - loss: 4.7871 - acc: 0.01 - ETA: 68s - loss: 4.7865 - acc: 0.01 - ETA: 68s - loss: 4.7857 - acc: 0.01 - ETA: 67s - loss: 4.7875 - acc: 0.01 - ETA: 67s - loss: 4.7885 - acc: 0.01 - ETA: 67s - loss: 4.7890 - acc: 0.01 - ETA: 67s - loss: 4.7898 - acc: 0.01 - ETA: 66s - loss: 4.7908 - acc: 0.01 - ETA: 66s - loss: 4.7911 - acc: 0.01 - ETA: 66s - loss: 4.7900 - acc: 0.01 - ETA: 65s - loss: 4.7913 - acc: 0.01 - ETA: 65s - loss: 4.7916 - acc: 0.01 - ETA: 65s - loss: 4.7915 - acc: 0.01 - ETA: 64s - loss: 4.7913 - acc: 0.01 - ETA: 64s - loss: 4.7912 - acc: 0.01 - ETA: 64s - loss: 4.7905 - acc: 0.01 - ETA: 64s - loss: 4.7883 - acc: 0.01 - ETA: 63s - loss: 4.7896 - acc: 0.01 - ETA: 63s - loss: 4.7868 - acc: 0.01 - ETA: 63s - loss: 4.7876 - acc: 0.01 - ETA: 62s - loss: 4.7878 - acc: 0.01 - ETA: 62s - loss: 4.7873 - acc: 0.01 - ETA: 62s - loss: 4.7870 - acc: 0.01 - ETA: 62s - loss: 4.7881 - acc: 0.01 - ETA: 61s - loss: 4.7888 - acc: 0.01 - ETA: 61s - loss: 4.7892 - acc: 0.01 - ETA: 61s - loss: 4.7884 - acc: 0.01 - ETA: 60s - loss: 4.7888 - acc: 0.01 - ETA: 60s - loss: 4.7885 - acc: 0.01 - ETA: 60s - loss: 4.7884 - acc: 0.01 - ETA: 59s - loss: 4.7881 - acc: 0.01 - ETA: 59s - loss: 4.7879 - acc: 0.01 - ETA: 59s - loss: 4.7886 - acc: 0.01 - ETA: 59s - loss: 4.7893 - acc: 0.01 - ETA: 58s - loss: 4.7890 - acc: 0.01 - ETA: 58s - loss: 4.7883 - acc: 0.01 - ETA: 58s - loss: 4.7875 - acc: 0.01 - ETA: 57s - loss: 4.7888 - acc: 0.01 - ETA: 57s - loss: 4.7877 - acc: 0.01 - ETA: 57s - loss: 4.7877 - acc: 0.01 - ETA: 56s - loss: 4.7883 - acc: 0.02 - ETA: 56s - loss: 4.7880 - acc: 0.02 - ETA: 56s - loss: 4.7871 - acc: 0.02 - ETA: 56s - loss: 4.7869 - acc: 0.02 - ETA: 55s - loss: 4.7866 - acc: 0.02 - ETA: 55s - loss: 4.7869 - acc: 0.02 - ETA: 55s - loss: 4.7874 - acc: 0.02 - ETA: 54s - loss: 4.7883 - acc: 0.01 - ETA: 54s - loss: 4.7879 - acc: 0.01 - ETA: 54s - loss: 4.7874 - acc: 0.02 - ETA: 54s - loss: 4.7871 - acc: 0.02 - ETA: 53s - loss: 4.7878 - acc: 0.02 - ETA: 53s - loss: 4.7878 - acc: 0.02 - ETA: 53s - loss: 4.7879 - acc: 0.02 - ETA: 52s - loss: 4.7883 - acc: 0.02 - ETA: 52s - loss: 4.7877 - acc: 0.02 - ETA: 52s - loss: 4.7873 - acc: 0.02 - ETA: 51s - loss: 4.7875 - acc: 0.02 - ETA: 51s - loss: 4.7867 - acc: 0.02 - ETA: 51s - loss: 4.7863 - acc: 0.02 - ETA: 51s - loss: 4.7860 - acc: 0.01 - ETA: 50s - loss: 4.7861 - acc: 0.01 - ETA: 50s - loss: 4.7864 - acc: 0.01 - ETA: 50s - loss: 4.7861 - acc: 0.01 - ETA: 49s - loss: 4.7868 - acc: 0.01 - ETA: 49s - loss: 4.7867 - acc: 0.01 - ETA: 49s - loss: 4.7868 - acc: 0.01 - ETA: 49s - loss: 4.7867 - acc: 0.01 - ETA: 48s - loss: 4.7859 - acc: 0.01 - ETA: 48s - loss: 4.7853 - acc: 0.01 - ETA: 48s - loss: 4.7859 - acc: 0.02 - ETA: 47s - loss: 4.7858 - acc: 0.02 - ETA: 47s - loss: 4.7862 - acc: 0.01 - ETA: 47s - loss: 4.7858 - acc: 0.01 - ETA: 47s - loss: 4.7858 - acc: 0.01 - ETA: 46s - loss: 4.7851 - acc: 0.01 - ETA: 46s - loss: 4.7847 - acc: 0.01 - ETA: 46s - loss: 4.7861 - acc: 0.01 - ETA: 45s - loss: 4.7857 - acc: 0.01 - ETA: 45s - loss: 4.7857 - acc: 0.01 - ETA: 45s - loss: 4.7854 - acc: 0.01 - ETA: 44s - loss: 4.7854 - acc: 0.01 - ETA: 44s - loss: 4.7849 - acc: 0.01 - ETA: 44s - loss: 4.7853 - acc: 0.01 - ETA: 44s - loss: 4.7847 - acc: 0.01 - ETA: 43s - loss: 4.7841 - acc: 0.01 - ETA: 43s - loss: 4.7843 - acc: 0.01 - ETA: 43s - loss: 4.7840 - acc: 0.01 - ETA: 42s - loss: 4.7844 - acc: 0.01 - ETA: 42s - loss: 4.7850 - acc: 0.01 - ETA: 42s - loss: 4.7859 - acc: 0.01 - ETA: 41s - loss: 4.7869 - acc: 0.01 - ETA: 41s - loss: 4.7872 - acc: 0.01 - ETA: 41s - loss: 4.7869 - acc: 0.01 - ETA: 41s - loss: 4.7870 - acc: 0.01 - ETA: 40s - loss: 4.7865 - acc: 0.01 - ETA: 40s - loss: 4.7861 - acc: 0.01 - ETA: 40s - loss: 4.7860 - acc: 0.01 - ETA: 39s - loss: 4.7857 - acc: 0.01 - ETA: 39s - loss: 4.7859 - acc: 0.01 - ETA: 39s - loss: 4.7860 - acc: 0.01 - ETA: 38s - loss: 4.7865 - acc: 0.01 - ETA: 38s - loss: 4.7865 - acc: 0.01 - ETA: 38s - loss: 4.7858 - acc: 0.01 - ETA: 38s - loss: 4.7847 - acc: 0.01 - ETA: 37s - loss: 4.7853 - acc: 0.01 - ETA: 37s - loss: 4.7851 - acc: 0.01 - ETA: 37s - loss: 4.7841 - acc: 0.01 - ETA: 36s - loss: 4.7840 - acc: 0.01 - ETA: 36s - loss: 4.7834 - acc: 0.01 - ETA: 36s - loss: 4.7828 - acc: 0.01 - ETA: 36s - loss: 4.7830 - acc: 0.01 - ETA: 35s - loss: 4.7822 - acc: 0.01 - ETA: 35s - loss: 4.7836 - acc: 0.01 - ETA: 35s - loss: 4.7836 - acc: 0.01956660/6680 [============================>.] - ETA: 34s - loss: 4.7838 - acc: 0.01 - ETA: 34s - loss: 4.7839 - acc: 0.01 - ETA: 34s - loss: 4.7836 - acc: 0.02 - ETA: 33s - loss: 4.7831 - acc: 0.02 - ETA: 33s - loss: 4.7832 - acc: 0.02 - ETA: 33s - loss: 4.7829 - acc: 0.02 - ETA: 33s - loss: 4.7838 - acc: 0.02 - ETA: 32s - loss: 4.7838 - acc: 0.02 - ETA: 32s - loss: 4.7836 - acc: 0.01 - ETA: 32s - loss: 4.7835 - acc: 0.02 - ETA: 31s - loss: 4.7831 - acc: 0.01 - ETA: 31s - loss: 4.7832 - acc: 0.01 - ETA: 31s - loss: 4.7833 - acc: 0.01 - ETA: 30s - loss: 4.7835 - acc: 0.01 - ETA: 30s - loss: 4.7837 - acc: 0.01 - ETA: 30s - loss: 4.7832 - acc: 0.01 - ETA: 30s - loss: 4.7830 - acc: 0.01 - ETA: 29s - loss: 4.7831 - acc: 0.01 - ETA: 29s - loss: 4.7826 - acc: 0.01 - ETA: 29s - loss: 4.7823 - acc: 0.01 - ETA: 28s - loss: 4.7821 - acc: 0.01 - ETA: 28s - loss: 4.7812 - acc: 0.01 - ETA: 28s - loss: 4.7816 - acc: 0.01 - ETA: 28s - loss: 4.7815 - acc: 0.01 - ETA: 27s - loss: 4.7817 - acc: 0.01 - ETA: 27s - loss: 4.7815 - acc: 0.01 - ETA: 27s - loss: 4.7815 - acc: 0.01 - ETA: 26s - loss: 4.7810 - acc: 0.02 - ETA: 26s - loss: 4.7800 - acc: 0.02 - ETA: 26s - loss: 4.7800 - acc: 0.02 - ETA: 25s - loss: 4.7799 - acc: 0.01 - ETA: 25s - loss: 4.7793 - acc: 0.02 - ETA: 25s - loss: 4.7795 - acc: 0.02 - ETA: 25s - loss: 4.7789 - acc: 0.01 - ETA: 24s - loss: 4.7786 - acc: 0.02 - ETA: 24s - loss: 4.7781 - acc: 0.02 - ETA: 24s - loss: 4.7782 - acc: 0.02 - ETA: 23s - loss: 4.7780 - acc: 0.02 - ETA: 23s - loss: 4.7780 - acc: 0.01 - ETA: 23s - loss: 4.7783 - acc: 0.01 - ETA: 23s - loss: 4.7778 - acc: 0.01 - ETA: 22s - loss: 4.7781 - acc: 0.01 - ETA: 22s - loss: 4.7787 - acc: 0.01 - ETA: 22s - loss: 4.7785 - acc: 0.01 - ETA: 21s - loss: 4.7777 - acc: 0.02 - ETA: 21s - loss: 4.7777 - acc: 0.02 - ETA: 21s - loss: 4.7772 - acc: 0.02 - ETA: 20s - loss: 4.7777 - acc: 0.02 - ETA: 20s - loss: 4.7781 - acc: 0.02 - ETA: 20s - loss: 4.7781 - acc: 0.02 - ETA: 20s - loss: 4.7780 - acc: 0.02 - ETA: 19s - loss: 4.7784 - acc: 0.02 - ETA: 19s - loss: 4.7776 - acc: 0.02 - ETA: 19s - loss: 4.7776 - acc: 0.02 - ETA: 18s - loss: 4.7774 - acc: 0.02 - ETA: 18s - loss: 4.7771 - acc: 0.02 - ETA: 18s - loss: 4.7772 - acc: 0.02 - ETA: 17s - loss: 4.7766 - acc: 0.02 - ETA: 17s - loss: 4.7764 - acc: 0.02 - ETA: 17s - loss: 4.7768 - acc: 0.02 - ETA: 17s - loss: 4.7776 - acc: 0.02 - ETA: 16s - loss: 4.7778 - acc: 0.02 - ETA: 16s - loss: 4.7777 - acc: 0.02 - ETA: 16s - loss: 4.7779 - acc: 0.02 - ETA: 15s - loss: 4.7776 - acc: 0.02 - ETA: 15s - loss: 4.7774 - acc: 0.02 - ETA: 15s - loss: 4.7777 - acc: 0.02 - ETA: 15s - loss: 4.7779 - acc: 0.02 - ETA: 14s - loss: 4.7780 - acc: 0.02 - ETA: 14s - loss: 4.7784 - acc: 0.02 - ETA: 14s - loss: 4.7787 - acc: 0.02 - ETA: 13s - loss: 4.7793 - acc: 0.02 - ETA: 13s - loss: 4.7792 - acc: 0.02 - ETA: 13s - loss: 4.7793 - acc: 0.02 - ETA: 12s - loss: 4.7789 - acc: 0.02 - ETA: 12s - loss: 4.7780 - acc: 0.02 - ETA: 12s - loss: 4.7789 - acc: 0.02 - ETA: 12s - loss: 4.7789 - acc: 0.02 - ETA: 11s - loss: 4.7792 - acc: 0.02 - ETA: 11s - loss: 4.7793 - acc: 0.02 - ETA: 11s - loss: 4.7793 - acc: 0.02 - ETA: 10s - loss: 4.7794 - acc: 0.02 - ETA: 10s - loss: 4.7796 - acc: 0.02 - ETA: 10s - loss: 4.7794 - acc: 0.02 - ETA: 10s - loss: 4.7793 - acc: 0.02 - ETA: 9s - loss: 4.7794 - acc: 0.0203 - ETA: 9s - loss: 4.7798 - acc: 0.020 - ETA: 9s - loss: 4.7803 - acc: 0.020 - ETA: 8s - loss: 4.7802 - acc: 0.020 - ETA: 8s - loss: 4.7805 - acc: 0.020 - ETA: 8s - loss: 4.7807 - acc: 0.019 - ETA: 7s - loss: 4.7809 - acc: 0.020 - ETA: 7s - loss: 4.7807 - acc: 0.020 - ETA: 7s - loss: 4.7806 - acc: 0.019 - ETA: 7s - loss: 4.7811 - acc: 0.019 - ETA: 6s - loss: 4.7814 - acc: 0.019 - ETA: 6s - loss: 4.7813 - acc: 0.019 - ETA: 6s - loss: 4.7811 - acc: 0.019 - ETA: 5s - loss: 4.7814 - acc: 0.019 - ETA: 5s - loss: 4.7816 - acc: 0.019 - ETA: 5s - loss: 4.7815 - acc: 0.019 - ETA: 5s - loss: 4.7810 - acc: 0.019 - ETA: 4s - loss: 4.7812 - acc: 0.019 - ETA: 4s - loss: 4.7809 - acc: 0.019 - ETA: 4s - loss: 4.7808 - acc: 0.019 - ETA: 3s - loss: 4.7810 - acc: 0.019 - ETA: 3s - loss: 4.7813 - acc: 0.019 - ETA: 3s - loss: 4.7814 - acc: 0.020 - ETA: 2s - loss: 4.7813 - acc: 0.019 - ETA: 2s - loss: 4.7810 - acc: 0.020 - ETA: 2s - loss: 4.7809 - acc: 0.020 - ETA: 2s - loss: 4.7810 - acc: 0.020 - ETA: 1s - loss: 4.7812 - acc: 0.020 - ETA: 1s - loss: 4.7811 - acc: 0.020 - ETA: 1s - loss: 4.7810 - acc: 0.020 - ETA: 0s - loss: 4.7808 - acc: 0.020 - ETA: 0s - loss: 4.7814 - acc: 0.020 - ETA: 0s - loss: 4.7816 - acc: 0.0200Epoch 00002: val_loss improved from 4.82919 to 4.79492, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 100s - loss: 4.7814 - acc: 0.0199 - val_loss: 4.7949 - val_acc: 0.0192\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 93s - loss: 4.7173 - acc: 0.10 - ETA: 96s - loss: 4.7331 - acc: 0.05 - ETA: 95s - loss: 4.8096 - acc: 0.03 - ETA: 95s - loss: 4.7853 - acc: 0.02 - ETA: 96s - loss: 4.7771 - acc: 0.04 - ETA: 95s - loss: 4.7904 - acc: 0.03 - ETA: 95s - loss: 4.7650 - acc: 0.02 - ETA: 95s - loss: 4.7476 - acc: 0.03 - ETA: 95s - loss: 4.7228 - acc: 0.02 - ETA: 94s - loss: 4.7444 - acc: 0.03 - ETA: 94s - loss: 4.7535 - acc: 0.02 - ETA: 94s - loss: 4.7485 - acc: 0.02 - ETA: 93s - loss: 4.7280 - acc: 0.02 - ETA: 93s - loss: 4.7364 - acc: 0.02 - ETA: 93s - loss: 4.7473 - acc: 0.02 - ETA: 93s - loss: 4.7494 - acc: 0.02 - ETA: 93s - loss: 4.7523 - acc: 0.02 - ETA: 92s - loss: 4.7625 - acc: 0.01 - ETA: 92s - loss: 4.7572 - acc: 0.01 - ETA: 92s - loss: 4.7498 - acc: 0.02 - ETA: 91s - loss: 4.7437 - acc: 0.01 - ETA: 91s - loss: 4.7412 - acc: 0.02 - ETA: 91s - loss: 4.7480 - acc: 0.01 - ETA: 91s - loss: 4.7509 - acc: 0.01 - ETA: 90s - loss: 4.7520 - acc: 0.01 - ETA: 90s - loss: 4.7490 - acc: 0.01 - ETA: 90s - loss: 4.7437 - acc: 0.01 - ETA: 90s - loss: 4.7444 - acc: 0.01 - ETA: 89s - loss: 4.7489 - acc: 0.01 - ETA: 89s - loss: 4.7484 - acc: 0.01 - ETA: 89s - loss: 4.7441 - acc: 0.02 - ETA: 88s - loss: 4.7489 - acc: 0.02 - ETA: 88s - loss: 4.7478 - acc: 0.01 - ETA: 88s - loss: 4.7420 - acc: 0.02 - ETA: 87s - loss: 4.7407 - acc: 0.02 - ETA: 87s - loss: 4.7433 - acc: 0.01 - ETA: 87s - loss: 4.7495 - acc: 0.01 - ETA: 87s - loss: 4.7428 - acc: 0.01 - ETA: 86s - loss: 4.7442 - acc: 0.01 - ETA: 86s - loss: 4.7467 - acc: 0.01 - ETA: 86s - loss: 4.7498 - acc: 0.01 - ETA: 85s - loss: 4.7447 - acc: 0.01 - ETA: 85s - loss: 4.7419 - acc: 0.01 - ETA: 85s - loss: 4.7414 - acc: 0.01 - ETA: 85s - loss: 4.7428 - acc: 0.02 - ETA: 84s - loss: 4.7469 - acc: 0.02 - ETA: 84s - loss: 4.7478 - acc: 0.02 - ETA: 84s - loss: 4.7521 - acc: 0.01 - ETA: 83s - loss: 4.7508 - acc: 0.01 - ETA: 83s - loss: 4.7490 - acc: 0.01 - ETA: 83s - loss: 4.7464 - acc: 0.02 - ETA: 82s - loss: 4.7482 - acc: 0.02 - ETA: 82s - loss: 4.7513 - acc: 0.02 - ETA: 82s - loss: 4.7497 - acc: 0.02 - ETA: 81s - loss: 4.7491 - acc: 0.02 - ETA: 81s - loss: 4.7472 - acc: 0.02 - ETA: 81s - loss: 4.7498 - acc: 0.02 - ETA: 81s - loss: 4.7504 - acc: 0.02 - ETA: 80s - loss: 4.7563 - acc: 0.02 - ETA: 80s - loss: 4.7558 - acc: 0.02 - ETA: 80s - loss: 4.7588 - acc: 0.02 - ETA: 79s - loss: 4.7571 - acc: 0.02 - ETA: 79s - loss: 4.7546 - acc: 0.02 - ETA: 79s - loss: 4.7559 - acc: 0.02 - ETA: 79s - loss: 4.7569 - acc: 0.02 - ETA: 78s - loss: 4.7569 - acc: 0.02 - ETA: 78s - loss: 4.7557 - acc: 0.02 - ETA: 78s - loss: 4.7540 - acc: 0.02 - ETA: 77s - loss: 4.7551 - acc: 0.02 - ETA: 77s - loss: 4.7555 - acc: 0.02 - ETA: 77s - loss: 4.7555 - acc: 0.02 - ETA: 77s - loss: 4.7558 - acc: 0.02 - ETA: 76s - loss: 4.7566 - acc: 0.02 - ETA: 76s - loss: 4.7571 - acc: 0.02 - ETA: 76s - loss: 4.7568 - acc: 0.02 - ETA: 75s - loss: 4.7587 - acc: 0.02 - ETA: 75s - loss: 4.7570 - acc: 0.02 - ETA: 75s - loss: 4.7584 - acc: 0.02 - ETA: 75s - loss: 4.7580 - acc: 0.02 - ETA: 74s - loss: 4.7580 - acc: 0.02 - ETA: 74s - loss: 4.7574 - acc: 0.02 - ETA: 74s - loss: 4.7566 - acc: 0.02 - ETA: 73s - loss: 4.7575 - acc: 0.02 - ETA: 73s - loss: 4.7607 - acc: 0.02 - ETA: 73s - loss: 4.7611 - acc: 0.02 - ETA: 73s - loss: 4.7592 - acc: 0.02 - ETA: 72s - loss: 4.7604 - acc: 0.02 - ETA: 72s - loss: 4.7601 - acc: 0.02 - ETA: 72s - loss: 4.7604 - acc: 0.02 - ETA: 71s - loss: 4.7597 - acc: 0.02 - ETA: 71s - loss: 4.7608 - acc: 0.02 - ETA: 71s - loss: 4.7600 - acc: 0.02 - ETA: 71s - loss: 4.7606 - acc: 0.02 - ETA: 70s - loss: 4.7616 - acc: 0.02 - ETA: 70s - loss: 4.7622 - acc: 0.02 - ETA: 70s - loss: 4.7614 - acc: 0.02 - ETA: 69s - loss: 4.7622 - acc: 0.02 - ETA: 69s - loss: 4.7606 - acc: 0.02 - ETA: 69s - loss: 4.7612 - acc: 0.02 - ETA: 68s - loss: 4.7609 - acc: 0.02 - ETA: 68s - loss: 4.7610 - acc: 0.02 - ETA: 68s - loss: 4.7624 - acc: 0.02 - ETA: 68s - loss: 4.7626 - acc: 0.02 - ETA: 67s - loss: 4.7628 - acc: 0.02 - ETA: 67s - loss: 4.7638 - acc: 0.02 - ETA: 67s - loss: 4.7654 - acc: 0.02 - ETA: 66s - loss: 4.7648 - acc: 0.02 - ETA: 66s - loss: 4.7635 - acc: 0.02 - ETA: 66s - loss: 4.7641 - acc: 0.02 - ETA: 65s - loss: 4.7631 - acc: 0.02 - ETA: 65s - loss: 4.7627 - acc: 0.02 - ETA: 65s - loss: 4.7638 - acc: 0.02 - ETA: 65s - loss: 4.7653 - acc: 0.02 - ETA: 64s - loss: 4.7644 - acc: 0.02 - ETA: 64s - loss: 4.7645 - acc: 0.02 - ETA: 64s - loss: 4.7639 - acc: 0.02 - ETA: 63s - loss: 4.7619 - acc: 0.02 - ETA: 63s - loss: 4.7652 - acc: 0.02 - ETA: 63s - loss: 4.7655 - acc: 0.02 - ETA: 63s - loss: 4.7651 - acc: 0.02 - ETA: 62s - loss: 4.7660 - acc: 0.02 - ETA: 62s - loss: 4.7669 - acc: 0.02 - ETA: 62s - loss: 4.7667 - acc: 0.02 - ETA: 61s - loss: 4.7671 - acc: 0.02 - ETA: 61s - loss: 4.7667 - acc: 0.02 - ETA: 61s - loss: 4.7662 - acc: 0.01 - ETA: 60s - loss: 4.7664 - acc: 0.02 - ETA: 60s - loss: 4.7669 - acc: 0.01 - ETA: 60s - loss: 4.7654 - acc: 0.02 - ETA: 60s - loss: 4.7670 - acc: 0.02 - ETA: 59s - loss: 4.7657 - acc: 0.02 - ETA: 59s - loss: 4.7644 - acc: 0.02 - ETA: 59s - loss: 4.7650 - acc: 0.02 - ETA: 58s - loss: 4.7644 - acc: 0.02 - ETA: 58s - loss: 4.7640 - acc: 0.02 - ETA: 58s - loss: 4.7632 - acc: 0.02 - ETA: 57s - loss: 4.7624 - acc: 0.02 - ETA: 57s - loss: 4.7629 - acc: 0.02 - ETA: 57s - loss: 4.7639 - acc: 0.02 - ETA: 57s - loss: 4.7648 - acc: 0.02 - ETA: 56s - loss: 4.7653 - acc: 0.02 - ETA: 56s - loss: 4.7655 - acc: 0.02 - ETA: 56s - loss: 4.7648 - acc: 0.02 - ETA: 55s - loss: 4.7640 - acc: 0.02 - ETA: 55s - loss: 4.7642 - acc: 0.02 - ETA: 55s - loss: 4.7648 - acc: 0.02 - ETA: 55s - loss: 4.7645 - acc: 0.02 - ETA: 54s - loss: 4.7654 - acc: 0.02 - ETA: 54s - loss: 4.7661 - acc: 0.02 - ETA: 54s - loss: 4.7667 - acc: 0.02 - ETA: 53s - loss: 4.7658 - acc: 0.02 - ETA: 53s - loss: 4.7648 - acc: 0.02 - ETA: 53s - loss: 4.7647 - acc: 0.01 - ETA: 52s - loss: 4.7647 - acc: 0.01 - ETA: 52s - loss: 4.7643 - acc: 0.02 - ETA: 52s - loss: 4.7632 - acc: 0.02 - ETA: 52s - loss: 4.7628 - acc: 0.02 - ETA: 51s - loss: 4.7637 - acc: 0.02 - ETA: 51s - loss: 4.7639 - acc: 0.02 - ETA: 51s - loss: 4.7645 - acc: 0.02 - ETA: 50s - loss: 4.7646 - acc: 0.02 - ETA: 50s - loss: 4.7644 - acc: 0.02 - ETA: 50s - loss: 4.7636 - acc: 0.02 - ETA: 50s - loss: 4.7648 - acc: 0.02 - ETA: 49s - loss: 4.7641 - acc: 0.02 - ETA: 49s - loss: 4.7639 - acc: 0.02 - ETA: 49s - loss: 4.7630 - acc: 0.02 - ETA: 48s - loss: 4.7621 - acc: 0.02 - ETA: 48s - loss: 4.7623 - acc: 0.02 - ETA: 48s - loss: 4.7612 - acc: 0.02 - ETA: 47s - loss: 4.7610 - acc: 0.02 - ETA: 47s - loss: 4.7600 - acc: 0.02 - ETA: 47s - loss: 4.7598 - acc: 0.02 - ETA: 47s - loss: 4.7587 - acc: 0.02 - ETA: 46s - loss: 4.7588 - acc: 0.02 - ETA: 46s - loss: 4.7596 - acc: 0.02 - ETA: 46s - loss: 4.7599 - acc: 0.02 - ETA: 45s - loss: 4.7589 - acc: 0.02 - ETA: 45s - loss: 4.7584 - acc: 0.02 - ETA: 45s - loss: 4.7583 - acc: 0.02 - ETA: 45s - loss: 4.7559 - acc: 0.02 - ETA: 44s - loss: 4.7558 - acc: 0.02 - ETA: 44s - loss: 4.7561 - acc: 0.02 - ETA: 44s - loss: 4.7548 - acc: 0.02 - ETA: 43s - loss: 4.7546 - acc: 0.02 - ETA: 43s - loss: 4.7544 - acc: 0.02 - ETA: 43s - loss: 4.7552 - acc: 0.02 - ETA: 42s - loss: 4.7554 - acc: 0.02 - ETA: 42s - loss: 4.7556 - acc: 0.02 - ETA: 42s - loss: 4.7559 - acc: 0.02 - ETA: 42s - loss: 4.7556 - acc: 0.02 - ETA: 41s - loss: 4.7558 - acc: 0.02 - ETA: 41s - loss: 4.7551 - acc: 0.02 - ETA: 41s - loss: 4.7547 - acc: 0.02 - ETA: 40s - loss: 4.7539 - acc: 0.02 - ETA: 40s - loss: 4.7537 - acc: 0.02 - ETA: 40s - loss: 4.7532 - acc: 0.02 - ETA: 40s - loss: 4.7537 - acc: 0.02 - ETA: 39s - loss: 4.7535 - acc: 0.02 - ETA: 39s - loss: 4.7544 - acc: 0.02 - ETA: 39s - loss: 4.7551 - acc: 0.02 - ETA: 38s - loss: 4.7550 - acc: 0.02 - ETA: 38s - loss: 4.7546 - acc: 0.02 - ETA: 38s - loss: 4.7541 - acc: 0.02 - ETA: 37s - loss: 4.7544 - acc: 0.02 - ETA: 37s - loss: 4.7546 - acc: 0.02 - ETA: 37s - loss: 4.7542 - acc: 0.02 - ETA: 37s - loss: 4.7532 - acc: 0.02 - ETA: 36s - loss: 4.7539 - acc: 0.02 - ETA: 36s - loss: 4.7528 - acc: 0.02 - ETA: 36s - loss: 4.7533 - acc: 0.02 - ETA: 35s - loss: 4.7532 - acc: 0.02 - ETA: 35s - loss: 4.7533 - acc: 0.02 - ETA: 35s - loss: 4.7540 - acc: 0.02 - ETA: 35s - loss: 4.7542 - acc: 0.0223"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 34s - loss: 4.7547 - acc: 0.02 - ETA: 34s - loss: 4.7547 - acc: 0.02 - ETA: 34s - loss: 4.7542 - acc: 0.02 - ETA: 33s - loss: 4.7526 - acc: 0.02 - ETA: 33s - loss: 4.7529 - acc: 0.02 - ETA: 33s - loss: 4.7523 - acc: 0.02 - ETA: 32s - loss: 4.7520 - acc: 0.02 - ETA: 32s - loss: 4.7523 - acc: 0.02 - ETA: 32s - loss: 4.7530 - acc: 0.02 - ETA: 32s - loss: 4.7538 - acc: 0.02 - ETA: 31s - loss: 4.7538 - acc: 0.02 - ETA: 31s - loss: 4.7543 - acc: 0.02 - ETA: 31s - loss: 4.7541 - acc: 0.02 - ETA: 30s - loss: 4.7539 - acc: 0.02 - ETA: 30s - loss: 4.7537 - acc: 0.02 - ETA: 30s - loss: 4.7543 - acc: 0.02 - ETA: 29s - loss: 4.7543 - acc: 0.02 - ETA: 29s - loss: 4.7543 - acc: 0.02 - ETA: 29s - loss: 4.7540 - acc: 0.02 - ETA: 29s - loss: 4.7547 - acc: 0.02 - ETA: 28s - loss: 4.7544 - acc: 0.02 - ETA: 28s - loss: 4.7545 - acc: 0.02 - ETA: 28s - loss: 4.7539 - acc: 0.02 - ETA: 27s - loss: 4.7541 - acc: 0.02 - ETA: 27s - loss: 4.7541 - acc: 0.02 - ETA: 27s - loss: 4.7537 - acc: 0.02 - ETA: 27s - loss: 4.7541 - acc: 0.02 - ETA: 26s - loss: 4.7545 - acc: 0.02 - ETA: 26s - loss: 4.7545 - acc: 0.02 - ETA: 26s - loss: 4.7547 - acc: 0.02 - ETA: 25s - loss: 4.7544 - acc: 0.02 - ETA: 25s - loss: 4.7542 - acc: 0.02 - ETA: 25s - loss: 4.7535 - acc: 0.02 - ETA: 25s - loss: 4.7535 - acc: 0.02 - ETA: 24s - loss: 4.7540 - acc: 0.02 - ETA: 24s - loss: 4.7541 - acc: 0.02 - ETA: 24s - loss: 4.7541 - acc: 0.02 - ETA: 23s - loss: 4.7535 - acc: 0.02 - ETA: 23s - loss: 4.7546 - acc: 0.02 - ETA: 23s - loss: 4.7546 - acc: 0.02 - ETA: 22s - loss: 4.7545 - acc: 0.02 - ETA: 22s - loss: 4.7529 - acc: 0.02 - ETA: 22s - loss: 4.7524 - acc: 0.02 - ETA: 22s - loss: 4.7518 - acc: 0.02 - ETA: 21s - loss: 4.7512 - acc: 0.02 - ETA: 21s - loss: 4.7511 - acc: 0.02 - ETA: 21s - loss: 4.7508 - acc: 0.02 - ETA: 20s - loss: 4.7509 - acc: 0.02 - ETA: 20s - loss: 4.7510 - acc: 0.02 - ETA: 20s - loss: 4.7518 - acc: 0.02 - ETA: 19s - loss: 4.7522 - acc: 0.02 - ETA: 19s - loss: 4.7525 - acc: 0.02 - ETA: 19s - loss: 4.7523 - acc: 0.02 - ETA: 19s - loss: 4.7524 - acc: 0.02 - ETA: 18s - loss: 4.7524 - acc: 0.02 - ETA: 18s - loss: 4.7531 - acc: 0.02 - ETA: 18s - loss: 4.7537 - acc: 0.02 - ETA: 17s - loss: 4.7535 - acc: 0.02 - ETA: 17s - loss: 4.7533 - acc: 0.02 - ETA: 17s - loss: 4.7530 - acc: 0.02 - ETA: 17s - loss: 4.7526 - acc: 0.02 - ETA: 16s - loss: 4.7523 - acc: 0.02 - ETA: 16s - loss: 4.7525 - acc: 0.02 - ETA: 16s - loss: 4.7517 - acc: 0.02 - ETA: 15s - loss: 4.7508 - acc: 0.02 - ETA: 15s - loss: 4.7508 - acc: 0.02 - ETA: 15s - loss: 4.7510 - acc: 0.02 - ETA: 14s - loss: 4.7508 - acc: 0.02 - ETA: 14s - loss: 4.7513 - acc: 0.02 - ETA: 14s - loss: 4.7513 - acc: 0.02 - ETA: 14s - loss: 4.7516 - acc: 0.02 - ETA: 13s - loss: 4.7516 - acc: 0.02 - ETA: 13s - loss: 4.7520 - acc: 0.02 - ETA: 13s - loss: 4.7518 - acc: 0.02 - ETA: 12s - loss: 4.7510 - acc: 0.02 - ETA: 12s - loss: 4.7516 - acc: 0.02 - ETA: 12s - loss: 4.7509 - acc: 0.02 - ETA: 12s - loss: 4.7510 - acc: 0.02 - ETA: 11s - loss: 4.7506 - acc: 0.02 - ETA: 11s - loss: 4.7503 - acc: 0.02 - ETA: 11s - loss: 4.7505 - acc: 0.02 - ETA: 10s - loss: 4.7495 - acc: 0.02 - ETA: 10s - loss: 4.7490 - acc: 0.02 - ETA: 10s - loss: 4.7487 - acc: 0.02 - ETA: 9s - loss: 4.7488 - acc: 0.0220 - ETA: 9s - loss: 4.7483 - acc: 0.021 - ETA: 9s - loss: 4.7489 - acc: 0.021 - ETA: 9s - loss: 4.7481 - acc: 0.021 - ETA: 8s - loss: 4.7482 - acc: 0.021 - ETA: 8s - loss: 4.7480 - acc: 0.021 - ETA: 8s - loss: 4.7484 - acc: 0.021 - ETA: 7s - loss: 4.7489 - acc: 0.021 - ETA: 7s - loss: 4.7489 - acc: 0.021 - ETA: 7s - loss: 4.7494 - acc: 0.021 - ETA: 7s - loss: 4.7497 - acc: 0.021 - ETA: 6s - loss: 4.7493 - acc: 0.021 - ETA: 6s - loss: 4.7488 - acc: 0.022 - ETA: 6s - loss: 4.7488 - acc: 0.022 - ETA: 5s - loss: 4.7485 - acc: 0.022 - ETA: 5s - loss: 4.7489 - acc: 0.022 - ETA: 5s - loss: 4.7489 - acc: 0.022 - ETA: 4s - loss: 4.7486 - acc: 0.021 - ETA: 4s - loss: 4.7492 - acc: 0.021 - ETA: 4s - loss: 4.7495 - acc: 0.021 - ETA: 4s - loss: 4.7497 - acc: 0.021 - ETA: 3s - loss: 4.7492 - acc: 0.021 - ETA: 3s - loss: 4.7492 - acc: 0.021 - ETA: 3s - loss: 4.7486 - acc: 0.021 - ETA: 2s - loss: 4.7486 - acc: 0.021 - ETA: 2s - loss: 4.7482 - acc: 0.021 - ETA: 2s - loss: 4.7479 - acc: 0.021 - ETA: 2s - loss: 4.7478 - acc: 0.021 - ETA: 1s - loss: 4.7472 - acc: 0.021 - ETA: 1s - loss: 4.7474 - acc: 0.021 - ETA: 1s - loss: 4.7468 - acc: 0.021 - ETA: 0s - loss: 4.7468 - acc: 0.021 - ETA: 0s - loss: 4.7479 - acc: 0.021 - ETA: 0s - loss: 4.7478 - acc: 0.0215Epoch 00003: val_loss improved from 4.79492 to 4.76371, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 100s - loss: 4.7482 - acc: 0.0216 - val_loss: 4.7637 - val_acc: 0.0263\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 94s - loss: 4.8938 - acc: 0.05 - ETA: 94s - loss: 4.7726 - acc: 0.05 - ETA: 95s - loss: 4.7643 - acc: 0.03 - ETA: 96s - loss: 4.7284 - acc: 0.05 - ETA: 96s - loss: 4.7081 - acc: 0.05 - ETA: 95s - loss: 4.6947 - acc: 0.05 - ETA: 95s - loss: 4.6872 - acc: 0.04 - ETA: 95s - loss: 4.6918 - acc: 0.03 - ETA: 95s - loss: 4.6990 - acc: 0.03 - ETA: 95s - loss: 4.6947 - acc: 0.03 - ETA: 95s - loss: 4.7011 - acc: 0.03 - ETA: 94s - loss: 4.6950 - acc: 0.02 - ETA: 94s - loss: 4.6911 - acc: 0.03 - ETA: 93s - loss: 4.6827 - acc: 0.03 - ETA: 93s - loss: 4.6735 - acc: 0.03 - ETA: 93s - loss: 4.6684 - acc: 0.03 - ETA: 93s - loss: 4.6867 - acc: 0.03 - ETA: 92s - loss: 4.6887 - acc: 0.03 - ETA: 92s - loss: 4.6841 - acc: 0.03 - ETA: 92s - loss: 4.6925 - acc: 0.03 - ETA: 92s - loss: 4.6890 - acc: 0.03 - ETA: 91s - loss: 4.6852 - acc: 0.03 - ETA: 91s - loss: 4.6848 - acc: 0.03 - ETA: 91s - loss: 4.6861 - acc: 0.03 - ETA: 90s - loss: 4.6916 - acc: 0.03 - ETA: 90s - loss: 4.6976 - acc: 0.03 - ETA: 90s - loss: 4.7013 - acc: 0.02 - ETA: 89s - loss: 4.6932 - acc: 0.03 - ETA: 89s - loss: 4.6931 - acc: 0.02 - ETA: 89s - loss: 4.6914 - acc: 0.03 - ETA: 89s - loss: 4.6974 - acc: 0.03 - ETA: 88s - loss: 4.6939 - acc: 0.03 - ETA: 88s - loss: 4.6912 - acc: 0.03 - ETA: 88s - loss: 4.6909 - acc: 0.03 - ETA: 87s - loss: 4.6878 - acc: 0.03 - ETA: 87s - loss: 4.6894 - acc: 0.02 - ETA: 87s - loss: 4.6928 - acc: 0.02 - ETA: 86s - loss: 4.6946 - acc: 0.03 - ETA: 86s - loss: 4.7005 - acc: 0.03 - ETA: 86s - loss: 4.7076 - acc: 0.03 - ETA: 86s - loss: 4.7072 - acc: 0.02 - ETA: 85s - loss: 4.7074 - acc: 0.02 - ETA: 85s - loss: 4.7106 - acc: 0.02 - ETA: 85s - loss: 4.7091 - acc: 0.02 - ETA: 84s - loss: 4.7066 - acc: 0.03 - ETA: 84s - loss: 4.7062 - acc: 0.02 - ETA: 84s - loss: 4.7064 - acc: 0.03 - ETA: 84s - loss: 4.7068 - acc: 0.03 - ETA: 83s - loss: 4.7042 - acc: 0.02 - ETA: 83s - loss: 4.7052 - acc: 0.02 - ETA: 83s - loss: 4.7100 - acc: 0.02 - ETA: 82s - loss: 4.7109 - acc: 0.02 - ETA: 82s - loss: 4.7168 - acc: 0.02 - ETA: 82s - loss: 4.7176 - acc: 0.02 - ETA: 81s - loss: 4.7167 - acc: 0.02 - ETA: 81s - loss: 4.7160 - acc: 0.02 - ETA: 81s - loss: 4.7133 - acc: 0.02 - ETA: 81s - loss: 4.7096 - acc: 0.02 - ETA: 80s - loss: 4.7090 - acc: 0.02 - ETA: 80s - loss: 4.7130 - acc: 0.02 - ETA: 80s - loss: 4.7117 - acc: 0.02 - ETA: 79s - loss: 4.7125 - acc: 0.03 - ETA: 79s - loss: 4.7132 - acc: 0.03 - ETA: 79s - loss: 4.7128 - acc: 0.03 - ETA: 79s - loss: 4.7116 - acc: 0.03 - ETA: 78s - loss: 4.7113 - acc: 0.03 - ETA: 78s - loss: 4.7105 - acc: 0.03 - ETA: 78s - loss: 4.7131 - acc: 0.03 - ETA: 77s - loss: 4.7154 - acc: 0.03 - ETA: 77s - loss: 4.7156 - acc: 0.03 - ETA: 77s - loss: 4.7158 - acc: 0.03 - ETA: 76s - loss: 4.7172 - acc: 0.03 - ETA: 76s - loss: 4.7169 - acc: 0.03 - ETA: 76s - loss: 4.7179 - acc: 0.02 - ETA: 76s - loss: 4.7158 - acc: 0.03 - ETA: 75s - loss: 4.7149 - acc: 0.02 - ETA: 75s - loss: 4.7122 - acc: 0.02 - ETA: 75s - loss: 4.7127 - acc: 0.02 - ETA: 74s - loss: 4.7142 - acc: 0.02 - ETA: 74s - loss: 4.7145 - acc: 0.02 - ETA: 74s - loss: 4.7149 - acc: 0.02 - ETA: 74s - loss: 4.7137 - acc: 0.02 - ETA: 73s - loss: 4.7113 - acc: 0.02 - ETA: 73s - loss: 4.7127 - acc: 0.02 - ETA: 73s - loss: 4.7149 - acc: 0.02 - ETA: 72s - loss: 4.7138 - acc: 0.02 - ETA: 72s - loss: 4.7136 - acc: 0.02 - ETA: 72s - loss: 4.7161 - acc: 0.02 - ETA: 72s - loss: 4.7170 - acc: 0.02 - ETA: 71s - loss: 4.7197 - acc: 0.02 - ETA: 71s - loss: 4.7185 - acc: 0.02 - ETA: 71s - loss: 4.7194 - acc: 0.02 - ETA: 70s - loss: 4.7164 - acc: 0.02 - ETA: 70s - loss: 4.7173 - acc: 0.02 - ETA: 70s - loss: 4.7182 - acc: 0.02 - ETA: 69s - loss: 4.7175 - acc: 0.02 - ETA: 69s - loss: 4.7177 - acc: 0.02 - ETA: 69s - loss: 4.7158 - acc: 0.02 - ETA: 69s - loss: 4.7169 - acc: 0.02 - ETA: 68s - loss: 4.7181 - acc: 0.02 - ETA: 68s - loss: 4.7166 - acc: 0.02 - ETA: 68s - loss: 4.7174 - acc: 0.02 - ETA: 67s - loss: 4.7181 - acc: 0.02 - ETA: 67s - loss: 4.7176 - acc: 0.02 - ETA: 67s - loss: 4.7186 - acc: 0.02 - ETA: 67s - loss: 4.7191 - acc: 0.02 - ETA: 66s - loss: 4.7196 - acc: 0.02 - ETA: 66s - loss: 4.7198 - acc: 0.02 - ETA: 66s - loss: 4.7192 - acc: 0.02 - ETA: 65s - loss: 4.7185 - acc: 0.02 - ETA: 65s - loss: 4.7184 - acc: 0.02 - ETA: 65s - loss: 4.7174 - acc: 0.02 - ETA: 64s - loss: 4.7180 - acc: 0.02 - ETA: 64s - loss: 4.7196 - acc: 0.02 - ETA: 64s - loss: 4.7196 - acc: 0.02 - ETA: 64s - loss: 4.7205 - acc: 0.02 - ETA: 63s - loss: 4.7218 - acc: 0.02 - ETA: 63s - loss: 4.7212 - acc: 0.02 - ETA: 63s - loss: 4.7217 - acc: 0.02 - ETA: 62s - loss: 4.7227 - acc: 0.02 - ETA: 62s - loss: 4.7231 - acc: 0.02 - ETA: 62s - loss: 4.7223 - acc: 0.02 - ETA: 62s - loss: 4.7221 - acc: 0.02 - ETA: 61s - loss: 4.7207 - acc: 0.02 - ETA: 61s - loss: 4.7190 - acc: 0.02 - ETA: 61s - loss: 4.7192 - acc: 0.02 - ETA: 60s - loss: 4.7187 - acc: 0.02 - ETA: 60s - loss: 4.7158 - acc: 0.02 - ETA: 60s - loss: 4.7170 - acc: 0.02 - ETA: 59s - loss: 4.7169 - acc: 0.02 - ETA: 59s - loss: 4.7168 - acc: 0.02 - ETA: 59s - loss: 4.7162 - acc: 0.02 - ETA: 59s - loss: 4.7160 - acc: 0.02 - ETA: 58s - loss: 4.7167 - acc: 0.02 - ETA: 58s - loss: 4.7183 - acc: 0.02 - ETA: 58s - loss: 4.7189 - acc: 0.02 - ETA: 57s - loss: 4.7202 - acc: 0.02 - ETA: 57s - loss: 4.7207 - acc: 0.02 - ETA: 57s - loss: 4.7206 - acc: 0.02 - ETA: 57s - loss: 4.7205 - acc: 0.02 - ETA: 56s - loss: 4.7198 - acc: 0.02 - ETA: 56s - loss: 4.7200 - acc: 0.02 - ETA: 56s - loss: 4.7196 - acc: 0.02 - ETA: 55s - loss: 4.7199 - acc: 0.02 - ETA: 55s - loss: 4.7195 - acc: 0.02 - ETA: 55s - loss: 4.7177 - acc: 0.02 - ETA: 54s - loss: 4.7180 - acc: 0.02 - ETA: 54s - loss: 4.7180 - acc: 0.02 - ETA: 54s - loss: 4.7181 - acc: 0.02 - ETA: 54s - loss: 4.7177 - acc: 0.02 - ETA: 53s - loss: 4.7168 - acc: 0.02 - ETA: 53s - loss: 4.7164 - acc: 0.02 - ETA: 53s - loss: 4.7165 - acc: 0.02 - ETA: 52s - loss: 4.7158 - acc: 0.02 - ETA: 52s - loss: 4.7161 - acc: 0.02 - ETA: 52s - loss: 4.7171 - acc: 0.02 - ETA: 52s - loss: 4.7159 - acc: 0.02 - ETA: 51s - loss: 4.7166 - acc: 0.02 - ETA: 51s - loss: 4.7166 - acc: 0.02 - ETA: 51s - loss: 4.7164 - acc: 0.02 - ETA: 50s - loss: 4.7163 - acc: 0.02 - ETA: 50s - loss: 4.7166 - acc: 0.02 - ETA: 50s - loss: 4.7166 - acc: 0.02 - ETA: 49s - loss: 4.7172 - acc: 0.02 - ETA: 49s - loss: 4.7177 - acc: 0.02 - ETA: 49s - loss: 4.7166 - acc: 0.02 - ETA: 49s - loss: 4.7171 - acc: 0.02 - ETA: 48s - loss: 4.7160 - acc: 0.02 - ETA: 48s - loss: 4.7164 - acc: 0.02 - ETA: 48s - loss: 4.7172 - acc: 0.02 - ETA: 47s - loss: 4.7171 - acc: 0.02 - ETA: 47s - loss: 4.7178 - acc: 0.02 - ETA: 47s - loss: 4.7180 - acc: 0.02 - ETA: 47s - loss: 4.7180 - acc: 0.02 - ETA: 46s - loss: 4.7182 - acc: 0.02 - ETA: 46s - loss: 4.7189 - acc: 0.02 - ETA: 46s - loss: 4.7198 - acc: 0.02 - ETA: 45s - loss: 4.7190 - acc: 0.02 - ETA: 45s - loss: 4.7181 - acc: 0.02 - ETA: 45s - loss: 4.7170 - acc: 0.02 - ETA: 44s - loss: 4.7162 - acc: 0.02 - ETA: 44s - loss: 4.7157 - acc: 0.02 - ETA: 44s - loss: 4.7166 - acc: 0.02 - ETA: 44s - loss: 4.7198 - acc: 0.02 - ETA: 43s - loss: 4.7192 - acc: 0.02 - ETA: 43s - loss: 4.7193 - acc: 0.02 - ETA: 43s - loss: 4.7197 - acc: 0.02 - ETA: 42s - loss: 4.7197 - acc: 0.02 - ETA: 42s - loss: 4.7185 - acc: 0.02 - ETA: 42s - loss: 4.7181 - acc: 0.02 - ETA: 42s - loss: 4.7188 - acc: 0.02 - ETA: 41s - loss: 4.7187 - acc: 0.02 - ETA: 41s - loss: 4.7188 - acc: 0.02 - ETA: 41s - loss: 4.7187 - acc: 0.02 - ETA: 40s - loss: 4.7187 - acc: 0.02 - ETA: 40s - loss: 4.7188 - acc: 0.02 - ETA: 40s - loss: 4.7189 - acc: 0.02 - ETA: 39s - loss: 4.7194 - acc: 0.02 - ETA: 39s - loss: 4.7188 - acc: 0.02 - ETA: 39s - loss: 4.7191 - acc: 0.02 - ETA: 39s - loss: 4.7192 - acc: 0.02 - ETA: 38s - loss: 4.7191 - acc: 0.02 - ETA: 38s - loss: 4.7197 - acc: 0.02 - ETA: 38s - loss: 4.7202 - acc: 0.02 - ETA: 37s - loss: 4.7195 - acc: 0.02 - ETA: 37s - loss: 4.7187 - acc: 0.02 - ETA: 37s - loss: 4.7181 - acc: 0.02 - ETA: 37s - loss: 4.7177 - acc: 0.02 - ETA: 36s - loss: 4.7192 - acc: 0.02 - ETA: 36s - loss: 4.7197 - acc: 0.02 - ETA: 36s - loss: 4.7203 - acc: 0.02 - ETA: 35s - loss: 4.7201 - acc: 0.02 - ETA: 35s - loss: 4.7205 - acc: 0.02 - ETA: 35s - loss: 4.7198 - acc: 0.02 - ETA: 34s - loss: 4.7203 - acc: 0.02676660/6680 [============================>.] - ETA: 34s - loss: 4.7197 - acc: 0.02 - ETA: 34s - loss: 4.7204 - acc: 0.02 - ETA: 34s - loss: 4.7199 - acc: 0.02 - ETA: 33s - loss: 4.7192 - acc: 0.02 - ETA: 33s - loss: 4.7199 - acc: 0.02 - ETA: 33s - loss: 4.7202 - acc: 0.02 - ETA: 32s - loss: 4.7199 - acc: 0.02 - ETA: 32s - loss: 4.7195 - acc: 0.02 - ETA: 32s - loss: 4.7188 - acc: 0.02 - ETA: 32s - loss: 4.7192 - acc: 0.02 - ETA: 31s - loss: 4.7182 - acc: 0.02 - ETA: 31s - loss: 4.7183 - acc: 0.02 - ETA: 31s - loss: 4.7185 - acc: 0.02 - ETA: 30s - loss: 4.7180 - acc: 0.02 - ETA: 30s - loss: 4.7168 - acc: 0.02 - ETA: 30s - loss: 4.7164 - acc: 0.02 - ETA: 29s - loss: 4.7165 - acc: 0.02 - ETA: 29s - loss: 4.7160 - acc: 0.02 - ETA: 29s - loss: 4.7161 - acc: 0.02 - ETA: 29s - loss: 4.7161 - acc: 0.02 - ETA: 28s - loss: 4.7167 - acc: 0.02 - ETA: 28s - loss: 4.7176 - acc: 0.02 - ETA: 28s - loss: 4.7172 - acc: 0.02 - ETA: 27s - loss: 4.7170 - acc: 0.02 - ETA: 27s - loss: 4.7162 - acc: 0.02 - ETA: 27s - loss: 4.7178 - acc: 0.02 - ETA: 27s - loss: 4.7179 - acc: 0.02 - ETA: 26s - loss: 4.7175 - acc: 0.02 - ETA: 26s - loss: 4.7176 - acc: 0.02 - ETA: 26s - loss: 4.7182 - acc: 0.02 - ETA: 25s - loss: 4.7183 - acc: 0.02 - ETA: 25s - loss: 4.7189 - acc: 0.02 - ETA: 25s - loss: 4.7185 - acc: 0.02 - ETA: 24s - loss: 4.7181 - acc: 0.02 - ETA: 24s - loss: 4.7174 - acc: 0.02 - ETA: 24s - loss: 4.7178 - acc: 0.02 - ETA: 24s - loss: 4.7173 - acc: 0.02 - ETA: 23s - loss: 4.7176 - acc: 0.02 - ETA: 23s - loss: 4.7173 - acc: 0.02 - ETA: 23s - loss: 4.7167 - acc: 0.02 - ETA: 22s - loss: 4.7159 - acc: 0.02 - ETA: 22s - loss: 4.7160 - acc: 0.02 - ETA: 22s - loss: 4.7167 - acc: 0.02 - ETA: 22s - loss: 4.7161 - acc: 0.02 - ETA: 21s - loss: 4.7159 - acc: 0.02 - ETA: 21s - loss: 4.7161 - acc: 0.02 - ETA: 21s - loss: 4.7154 - acc: 0.02 - ETA: 20s - loss: 4.7152 - acc: 0.02 - ETA: 20s - loss: 4.7135 - acc: 0.02 - ETA: 20s - loss: 4.7128 - acc: 0.02 - ETA: 19s - loss: 4.7136 - acc: 0.02 - ETA: 19s - loss: 4.7138 - acc: 0.02 - ETA: 19s - loss: 4.7138 - acc: 0.02 - ETA: 19s - loss: 4.7131 - acc: 0.02 - ETA: 18s - loss: 4.7133 - acc: 0.02 - ETA: 18s - loss: 4.7138 - acc: 0.02 - ETA: 18s - loss: 4.7138 - acc: 0.02 - ETA: 17s - loss: 4.7141 - acc: 0.02 - ETA: 17s - loss: 4.7138 - acc: 0.02 - ETA: 17s - loss: 4.7145 - acc: 0.02 - ETA: 17s - loss: 4.7144 - acc: 0.02 - ETA: 16s - loss: 4.7149 - acc: 0.02 - ETA: 16s - loss: 4.7144 - acc: 0.02 - ETA: 16s - loss: 4.7152 - acc: 0.02 - ETA: 15s - loss: 4.7150 - acc: 0.02 - ETA: 15s - loss: 4.7153 - acc: 0.02 - ETA: 15s - loss: 4.7155 - acc: 0.02 - ETA: 14s - loss: 4.7156 - acc: 0.02 - ETA: 14s - loss: 4.7156 - acc: 0.02 - ETA: 14s - loss: 4.7153 - acc: 0.02 - ETA: 14s - loss: 4.7164 - acc: 0.02 - ETA: 13s - loss: 4.7156 - acc: 0.02 - ETA: 13s - loss: 4.7158 - acc: 0.02 - ETA: 13s - loss: 4.7156 - acc: 0.02 - ETA: 12s - loss: 4.7154 - acc: 0.02 - ETA: 12s - loss: 4.7146 - acc: 0.02 - ETA: 12s - loss: 4.7145 - acc: 0.02 - ETA: 12s - loss: 4.7142 - acc: 0.02 - ETA: 11s - loss: 4.7151 - acc: 0.02 - ETA: 11s - loss: 4.7159 - acc: 0.02 - ETA: 11s - loss: 4.7161 - acc: 0.02 - ETA: 10s - loss: 4.7160 - acc: 0.02 - ETA: 10s - loss: 4.7157 - acc: 0.02 - ETA: 10s - loss: 4.7158 - acc: 0.02 - ETA: 9s - loss: 4.7159 - acc: 0.0272 - ETA: 9s - loss: 4.7151 - acc: 0.027 - ETA: 9s - loss: 4.7146 - acc: 0.027 - ETA: 9s - loss: 4.7150 - acc: 0.027 - ETA: 8s - loss: 4.7146 - acc: 0.027 - ETA: 8s - loss: 4.7149 - acc: 0.027 - ETA: 8s - loss: 4.7144 - acc: 0.027 - ETA: 7s - loss: 4.7149 - acc: 0.027 - ETA: 7s - loss: 4.7152 - acc: 0.027 - ETA: 7s - loss: 4.7148 - acc: 0.027 - ETA: 7s - loss: 4.7151 - acc: 0.027 - ETA: 6s - loss: 4.7155 - acc: 0.027 - ETA: 6s - loss: 4.7153 - acc: 0.027 - ETA: 6s - loss: 4.7150 - acc: 0.027 - ETA: 5s - loss: 4.7163 - acc: 0.027 - ETA: 5s - loss: 4.7169 - acc: 0.027 - ETA: 5s - loss: 4.7174 - acc: 0.027 - ETA: 5s - loss: 4.7175 - acc: 0.027 - ETA: 4s - loss: 4.7178 - acc: 0.027 - ETA: 4s - loss: 4.7179 - acc: 0.027 - ETA: 4s - loss: 4.7178 - acc: 0.027 - ETA: 3s - loss: 4.7177 - acc: 0.027 - ETA: 3s - loss: 4.7176 - acc: 0.027 - ETA: 3s - loss: 4.7177 - acc: 0.027 - ETA: 2s - loss: 4.7173 - acc: 0.027 - ETA: 2s - loss: 4.7169 - acc: 0.027 - ETA: 2s - loss: 4.7171 - acc: 0.027 - ETA: 2s - loss: 4.7176 - acc: 0.027 - ETA: 1s - loss: 4.7176 - acc: 0.027 - ETA: 1s - loss: 4.7184 - acc: 0.027 - ETA: 1s - loss: 4.7186 - acc: 0.027 - ETA: 0s - loss: 4.7185 - acc: 0.026 - ETA: 0s - loss: 4.7185 - acc: 0.026 - ETA: 0s - loss: 4.7184 - acc: 0.0267Epoch 00004: val_loss improved from 4.76371 to 4.74242, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 100s - loss: 4.7188 - acc: 0.0268 - val_loss: 4.7424 - val_acc: 0.0216\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 96s - loss: 4.7973 - acc: 0.05 - ETA: 94s - loss: 4.7465 - acc: 0.02 - ETA: 95s - loss: 4.8105 - acc: 0.01 - ETA: 96s - loss: 4.7343 - acc: 0.02 - ETA: 95s - loss: 4.6916 - acc: 0.02 - ETA: 95s - loss: 4.6857 - acc: 0.01 - ETA: 95s - loss: 4.6737 - acc: 0.02 - ETA: 95s - loss: 4.6741 - acc: 0.01 - ETA: 94s - loss: 4.6706 - acc: 0.02 - ETA: 94s - loss: 4.6793 - acc: 0.02 - ETA: 94s - loss: 4.6974 - acc: 0.02 - ETA: 94s - loss: 4.7093 - acc: 0.02 - ETA: 94s - loss: 4.7044 - acc: 0.02 - ETA: 93s - loss: 4.7223 - acc: 0.02 - ETA: 93s - loss: 4.7304 - acc: 0.02 - ETA: 93s - loss: 4.7348 - acc: 0.02 - ETA: 92s - loss: 4.7383 - acc: 0.02 - ETA: 92s - loss: 4.7260 - acc: 0.01 - ETA: 92s - loss: 4.7213 - acc: 0.01 - ETA: 92s - loss: 4.7178 - acc: 0.01 - ETA: 91s - loss: 4.7128 - acc: 0.01 - ETA: 91s - loss: 4.7114 - acc: 0.01 - ETA: 91s - loss: 4.7109 - acc: 0.01 - ETA: 91s - loss: 4.7178 - acc: 0.01 - ETA: 90s - loss: 4.7093 - acc: 0.01 - ETA: 90s - loss: 4.7135 - acc: 0.01 - ETA: 90s - loss: 4.7131 - acc: 0.01 - ETA: 89s - loss: 4.7175 - acc: 0.01 - ETA: 89s - loss: 4.7248 - acc: 0.01 - ETA: 89s - loss: 4.7246 - acc: 0.01 - ETA: 88s - loss: 4.7218 - acc: 0.01 - ETA: 88s - loss: 4.7198 - acc: 0.01 - ETA: 88s - loss: 4.7204 - acc: 0.01 - ETA: 88s - loss: 4.7170 - acc: 0.01 - ETA: 87s - loss: 4.7159 - acc: 0.01 - ETA: 87s - loss: 4.7131 - acc: 0.01 - ETA: 87s - loss: 4.7104 - acc: 0.02 - ETA: 86s - loss: 4.7133 - acc: 0.02 - ETA: 86s - loss: 4.7114 - acc: 0.02 - ETA: 86s - loss: 4.7096 - acc: 0.02 - ETA: 86s - loss: 4.7097 - acc: 0.02 - ETA: 85s - loss: 4.7055 - acc: 0.02 - ETA: 85s - loss: 4.7123 - acc: 0.02 - ETA: 85s - loss: 4.7107 - acc: 0.02 - ETA: 84s - loss: 4.7114 - acc: 0.02 - ETA: 84s - loss: 4.7074 - acc: 0.02 - ETA: 84s - loss: 4.7114 - acc: 0.02 - ETA: 84s - loss: 4.7110 - acc: 0.02 - ETA: 83s - loss: 4.7121 - acc: 0.02 - ETA: 83s - loss: 4.7124 - acc: 0.02 - ETA: 83s - loss: 4.7091 - acc: 0.02 - ETA: 82s - loss: 4.7082 - acc: 0.02 - ETA: 82s - loss: 4.7082 - acc: 0.02 - ETA: 82s - loss: 4.7038 - acc: 0.02 - ETA: 82s - loss: 4.7003 - acc: 0.02 - ETA: 81s - loss: 4.7000 - acc: 0.02 - ETA: 81s - loss: 4.7009 - acc: 0.02 - ETA: 81s - loss: 4.6980 - acc: 0.02 - ETA: 80s - loss: 4.7000 - acc: 0.02 - ETA: 80s - loss: 4.7030 - acc: 0.02 - ETA: 80s - loss: 4.6981 - acc: 0.02 - ETA: 79s - loss: 4.6983 - acc: 0.02 - ETA: 79s - loss: 4.7002 - acc: 0.02 - ETA: 79s - loss: 4.6968 - acc: 0.02 - ETA: 79s - loss: 4.6942 - acc: 0.02 - ETA: 78s - loss: 4.6899 - acc: 0.02 - ETA: 78s - loss: 4.6890 - acc: 0.02 - ETA: 78s - loss: 4.6896 - acc: 0.02 - ETA: 77s - loss: 4.6926 - acc: 0.02 - ETA: 77s - loss: 4.6921 - acc: 0.02 - ETA: 77s - loss: 4.6924 - acc: 0.02 - ETA: 77s - loss: 4.6914 - acc: 0.02 - ETA: 76s - loss: 4.6922 - acc: 0.02 - ETA: 76s - loss: 4.6908 - acc: 0.02 - ETA: 76s - loss: 4.6887 - acc: 0.02 - ETA: 75s - loss: 4.6893 - acc: 0.02 - ETA: 75s - loss: 4.6922 - acc: 0.02 - ETA: 75s - loss: 4.6923 - acc: 0.02 - ETA: 74s - loss: 4.6900 - acc: 0.02 - ETA: 74s - loss: 4.6874 - acc: 0.02 - ETA: 74s - loss: 4.6903 - acc: 0.02 - ETA: 74s - loss: 4.6904 - acc: 0.02 - ETA: 73s - loss: 4.6904 - acc: 0.02 - ETA: 73s - loss: 4.6896 - acc: 0.02 - ETA: 73s - loss: 4.6910 - acc: 0.02 - ETA: 72s - loss: 4.6902 - acc: 0.02 - ETA: 72s - loss: 4.6873 - acc: 0.02 - ETA: 72s - loss: 4.6856 - acc: 0.02 - ETA: 72s - loss: 4.6861 - acc: 0.02 - ETA: 71s - loss: 4.6844 - acc: 0.02 - ETA: 71s - loss: 4.6834 - acc: 0.02 - ETA: 71s - loss: 4.6833 - acc: 0.02 - ETA: 70s - loss: 4.6834 - acc: 0.02 - ETA: 70s - loss: 4.6841 - acc: 0.02 - ETA: 70s - loss: 4.6835 - acc: 0.02 - ETA: 69s - loss: 4.6831 - acc: 0.02 - ETA: 69s - loss: 4.6818 - acc: 0.02 - ETA: 69s - loss: 4.6806 - acc: 0.02 - ETA: 69s - loss: 4.6825 - acc: 0.02 - ETA: 68s - loss: 4.6826 - acc: 0.02 - ETA: 68s - loss: 4.6845 - acc: 0.02 - ETA: 68s - loss: 4.6861 - acc: 0.02 - ETA: 67s - loss: 4.6834 - acc: 0.02 - ETA: 67s - loss: 4.6831 - acc: 0.02 - ETA: 67s - loss: 4.6825 - acc: 0.02 - ETA: 67s - loss: 4.6816 - acc: 0.02 - ETA: 66s - loss: 4.6829 - acc: 0.02 - ETA: 66s - loss: 4.6825 - acc: 0.02 - ETA: 66s - loss: 4.6808 - acc: 0.02 - ETA: 65s - loss: 4.6803 - acc: 0.02 - ETA: 65s - loss: 4.6780 - acc: 0.02 - ETA: 65s - loss: 4.6770 - acc: 0.02 - ETA: 64s - loss: 4.6787 - acc: 0.02 - ETA: 64s - loss: 4.6797 - acc: 0.02 - ETA: 64s - loss: 4.6789 - acc: 0.02 - ETA: 64s - loss: 4.6812 - acc: 0.02 - ETA: 63s - loss: 4.6818 - acc: 0.02 - ETA: 63s - loss: 4.6827 - acc: 0.02 - ETA: 63s - loss: 4.6836 - acc: 0.02 - ETA: 62s - loss: 4.6848 - acc: 0.02 - ETA: 62s - loss: 4.6832 - acc: 0.02 - ETA: 62s - loss: 4.6830 - acc: 0.02 - ETA: 62s - loss: 4.6829 - acc: 0.02 - ETA: 61s - loss: 4.6819 - acc: 0.02 - ETA: 61s - loss: 4.6812 - acc: 0.02 - ETA: 61s - loss: 4.6820 - acc: 0.02 - ETA: 60s - loss: 4.6823 - acc: 0.02 - ETA: 60s - loss: 4.6824 - acc: 0.02 - ETA: 60s - loss: 4.6823 - acc: 0.02 - ETA: 60s - loss: 4.6821 - acc: 0.02 - ETA: 59s - loss: 4.6822 - acc: 0.02 - ETA: 59s - loss: 4.6839 - acc: 0.02 - ETA: 59s - loss: 4.6838 - acc: 0.02 - ETA: 58s - loss: 4.6839 - acc: 0.02 - ETA: 58s - loss: 4.6856 - acc: 0.02 - ETA: 58s - loss: 4.6849 - acc: 0.02 - ETA: 57s - loss: 4.6832 - acc: 0.02 - ETA: 57s - loss: 4.6839 - acc: 0.02 - ETA: 57s - loss: 4.6829 - acc: 0.02 - ETA: 57s - loss: 4.6816 - acc: 0.02 - ETA: 56s - loss: 4.6808 - acc: 0.02 - ETA: 56s - loss: 4.6800 - acc: 0.02 - ETA: 56s - loss: 4.6808 - acc: 0.02 - ETA: 55s - loss: 4.6802 - acc: 0.02 - ETA: 55s - loss: 4.6796 - acc: 0.02 - ETA: 55s - loss: 4.6797 - acc: 0.02 - ETA: 55s - loss: 4.6806 - acc: 0.02 - ETA: 54s - loss: 4.6799 - acc: 0.02 - ETA: 54s - loss: 4.6794 - acc: 0.02 - ETA: 54s - loss: 4.6804 - acc: 0.02 - ETA: 53s - loss: 4.6805 - acc: 0.02 - ETA: 53s - loss: 4.6811 - acc: 0.02 - ETA: 53s - loss: 4.6816 - acc: 0.02 - ETA: 52s - loss: 4.6822 - acc: 0.02 - ETA: 52s - loss: 4.6827 - acc: 0.02 - ETA: 52s - loss: 4.6832 - acc: 0.02 - ETA: 52s - loss: 4.6830 - acc: 0.02 - ETA: 51s - loss: 4.6841 - acc: 0.02 - ETA: 51s - loss: 4.6846 - acc: 0.02 - ETA: 51s - loss: 4.6844 - acc: 0.02 - ETA: 50s - loss: 4.6833 - acc: 0.02 - ETA: 50s - loss: 4.6830 - acc: 0.02 - ETA: 50s - loss: 4.6820 - acc: 0.02 - ETA: 50s - loss: 4.6822 - acc: 0.02 - ETA: 49s - loss: 4.6815 - acc: 0.02 - ETA: 49s - loss: 4.6821 - acc: 0.02 - ETA: 49s - loss: 4.6831 - acc: 0.02 - ETA: 48s - loss: 4.6830 - acc: 0.02 - ETA: 48s - loss: 4.6842 - acc: 0.02 - ETA: 48s - loss: 4.6847 - acc: 0.02 - ETA: 47s - loss: 4.6853 - acc: 0.02 - ETA: 47s - loss: 4.6855 - acc: 0.02 - ETA: 47s - loss: 4.6853 - acc: 0.02 - ETA: 47s - loss: 4.6854 - acc: 0.02 - ETA: 46s - loss: 4.6853 - acc: 0.02 - ETA: 46s - loss: 4.6843 - acc: 0.02 - ETA: 46s - loss: 4.6837 - acc: 0.02 - ETA: 45s - loss: 4.6847 - acc: 0.02 - ETA: 45s - loss: 4.6852 - acc: 0.02 - ETA: 45s - loss: 4.6844 - acc: 0.02 - ETA: 45s - loss: 4.6845 - acc: 0.02 - ETA: 44s - loss: 4.6840 - acc: 0.02 - ETA: 44s - loss: 4.6848 - acc: 0.02 - ETA: 44s - loss: 4.6853 - acc: 0.02 - ETA: 43s - loss: 4.6855 - acc: 0.02 - ETA: 43s - loss: 4.6858 - acc: 0.02 - ETA: 43s - loss: 4.6853 - acc: 0.02 - ETA: 42s - loss: 4.6861 - acc: 0.02 - ETA: 42s - loss: 4.6859 - acc: 0.02 - ETA: 42s - loss: 4.6861 - acc: 0.02 - ETA: 42s - loss: 4.6851 - acc: 0.02 - ETA: 41s - loss: 4.6855 - acc: 0.02 - ETA: 41s - loss: 4.6845 - acc: 0.02 - ETA: 41s - loss: 4.6858 - acc: 0.02 - ETA: 40s - loss: 4.6876 - acc: 0.02 - ETA: 40s - loss: 4.6888 - acc: 0.02 - ETA: 40s - loss: 4.6897 - acc: 0.02 - ETA: 39s - loss: 4.6902 - acc: 0.02 - ETA: 39s - loss: 4.6903 - acc: 0.02 - ETA: 39s - loss: 4.6906 - acc: 0.02 - ETA: 39s - loss: 4.6912 - acc: 0.02 - ETA: 38s - loss: 4.6912 - acc: 0.02 - ETA: 38s - loss: 4.6917 - acc: 0.02 - ETA: 38s - loss: 4.6914 - acc: 0.02 - ETA: 37s - loss: 4.6914 - acc: 0.02 - ETA: 37s - loss: 4.6915 - acc: 0.02 - ETA: 37s - loss: 4.6913 - acc: 0.02 - ETA: 37s - loss: 4.6918 - acc: 0.02 - ETA: 36s - loss: 4.6917 - acc: 0.02 - ETA: 36s - loss: 4.6913 - acc: 0.02 - ETA: 36s - loss: 4.6910 - acc: 0.02 - ETA: 35s - loss: 4.6915 - acc: 0.02 - ETA: 35s - loss: 4.6922 - acc: 0.02 - ETA: 35s - loss: 4.6913 - acc: 0.02 - ETA: 34s - loss: 4.6907 - acc: 0.0279"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 34s - loss: 4.6910 - acc: 0.02 - ETA: 34s - loss: 4.6905 - acc: 0.02 - ETA: 34s - loss: 4.6921 - acc: 0.02 - ETA: 33s - loss: 4.6922 - acc: 0.02 - ETA: 33s - loss: 4.6927 - acc: 0.02 - ETA: 33s - loss: 4.6915 - acc: 0.02 - ETA: 32s - loss: 4.6920 - acc: 0.02 - ETA: 32s - loss: 4.6911 - acc: 0.02 - ETA: 32s - loss: 4.6898 - acc: 0.02 - ETA: 32s - loss: 4.6908 - acc: 0.02 - ETA: 31s - loss: 4.6917 - acc: 0.02 - ETA: 31s - loss: 4.6913 - acc: 0.02 - ETA: 31s - loss: 4.6916 - acc: 0.02 - ETA: 30s - loss: 4.6909 - acc: 0.02 - ETA: 30s - loss: 4.6905 - acc: 0.02 - ETA: 30s - loss: 4.6903 - acc: 0.02 - ETA: 29s - loss: 4.6906 - acc: 0.02 - ETA: 29s - loss: 4.6915 - acc: 0.02 - ETA: 29s - loss: 4.6908 - acc: 0.02 - ETA: 29s - loss: 4.6903 - acc: 0.02 - ETA: 28s - loss: 4.6909 - acc: 0.02 - ETA: 28s - loss: 4.6914 - acc: 0.02 - ETA: 28s - loss: 4.6919 - acc: 0.02 - ETA: 27s - loss: 4.6912 - acc: 0.02 - ETA: 27s - loss: 4.6908 - acc: 0.02 - ETA: 27s - loss: 4.6918 - acc: 0.02 - ETA: 27s - loss: 4.6913 - acc: 0.02 - ETA: 26s - loss: 4.6915 - acc: 0.02 - ETA: 26s - loss: 4.6914 - acc: 0.02 - ETA: 26s - loss: 4.6912 - acc: 0.02 - ETA: 25s - loss: 4.6918 - acc: 0.02 - ETA: 25s - loss: 4.6910 - acc: 0.02 - ETA: 25s - loss: 4.6917 - acc: 0.02 - ETA: 24s - loss: 4.6917 - acc: 0.02 - ETA: 24s - loss: 4.6923 - acc: 0.02 - ETA: 24s - loss: 4.6925 - acc: 0.02 - ETA: 24s - loss: 4.6930 - acc: 0.02 - ETA: 23s - loss: 4.6927 - acc: 0.02 - ETA: 23s - loss: 4.6930 - acc: 0.02 - ETA: 23s - loss: 4.6929 - acc: 0.02 - ETA: 22s - loss: 4.6924 - acc: 0.02 - ETA: 22s - loss: 4.6919 - acc: 0.02 - ETA: 22s - loss: 4.6910 - acc: 0.02 - ETA: 22s - loss: 4.6914 - acc: 0.02 - ETA: 21s - loss: 4.6916 - acc: 0.02 - ETA: 21s - loss: 4.6924 - acc: 0.02 - ETA: 21s - loss: 4.6926 - acc: 0.02 - ETA: 20s - loss: 4.6924 - acc: 0.02 - ETA: 20s - loss: 4.6928 - acc: 0.02 - ETA: 20s - loss: 4.6928 - acc: 0.02 - ETA: 19s - loss: 4.6928 - acc: 0.02 - ETA: 19s - loss: 4.6925 - acc: 0.02 - ETA: 19s - loss: 4.6930 - acc: 0.02 - ETA: 19s - loss: 4.6929 - acc: 0.02 - ETA: 18s - loss: 4.6922 - acc: 0.02 - ETA: 18s - loss: 4.6923 - acc: 0.02 - ETA: 18s - loss: 4.6929 - acc: 0.02 - ETA: 17s - loss: 4.6937 - acc: 0.02 - ETA: 17s - loss: 4.6939 - acc: 0.02 - ETA: 17s - loss: 4.6946 - acc: 0.02 - ETA: 17s - loss: 4.6943 - acc: 0.02 - ETA: 16s - loss: 4.6946 - acc: 0.02 - ETA: 16s - loss: 4.6946 - acc: 0.02 - ETA: 16s - loss: 4.6945 - acc: 0.02 - ETA: 15s - loss: 4.6938 - acc: 0.02 - ETA: 15s - loss: 4.6941 - acc: 0.02 - ETA: 15s - loss: 4.6936 - acc: 0.02 - ETA: 14s - loss: 4.6933 - acc: 0.02 - ETA: 14s - loss: 4.6933 - acc: 0.02 - ETA: 14s - loss: 4.6927 - acc: 0.02 - ETA: 14s - loss: 4.6928 - acc: 0.02 - ETA: 13s - loss: 4.6933 - acc: 0.02 - ETA: 13s - loss: 4.6935 - acc: 0.02 - ETA: 13s - loss: 4.6941 - acc: 0.02 - ETA: 12s - loss: 4.6937 - acc: 0.02 - ETA: 12s - loss: 4.6942 - acc: 0.02 - ETA: 12s - loss: 4.6936 - acc: 0.02 - ETA: 12s - loss: 4.6934 - acc: 0.02 - ETA: 11s - loss: 4.6931 - acc: 0.02 - ETA: 11s - loss: 4.6937 - acc: 0.02 - ETA: 11s - loss: 4.6945 - acc: 0.02 - ETA: 10s - loss: 4.6940 - acc: 0.02 - ETA: 10s - loss: 4.6935 - acc: 0.02 - ETA: 10s - loss: 4.6941 - acc: 0.02 - ETA: 9s - loss: 4.6934 - acc: 0.0275 - ETA: 9s - loss: 4.6932 - acc: 0.027 - ETA: 9s - loss: 4.6937 - acc: 0.027 - ETA: 9s - loss: 4.6930 - acc: 0.027 - ETA: 8s - loss: 4.6931 - acc: 0.027 - ETA: 8s - loss: 4.6936 - acc: 0.027 - ETA: 8s - loss: 4.6926 - acc: 0.027 - ETA: 7s - loss: 4.6914 - acc: 0.027 - ETA: 7s - loss: 4.6918 - acc: 0.027 - ETA: 7s - loss: 4.6915 - acc: 0.027 - ETA: 7s - loss: 4.6920 - acc: 0.027 - ETA: 6s - loss: 4.6915 - acc: 0.027 - ETA: 6s - loss: 4.6913 - acc: 0.027 - ETA: 6s - loss: 4.6915 - acc: 0.027 - ETA: 5s - loss: 4.6911 - acc: 0.028 - ETA: 5s - loss: 4.6909 - acc: 0.028 - ETA: 5s - loss: 4.6919 - acc: 0.028 - ETA: 4s - loss: 4.6921 - acc: 0.028 - ETA: 4s - loss: 4.6919 - acc: 0.028 - ETA: 4s - loss: 4.6922 - acc: 0.028 - ETA: 4s - loss: 4.6928 - acc: 0.028 - ETA: 3s - loss: 4.6928 - acc: 0.028 - ETA: 3s - loss: 4.6932 - acc: 0.028 - ETA: 3s - loss: 4.6928 - acc: 0.028 - ETA: 2s - loss: 4.6926 - acc: 0.028 - ETA: 2s - loss: 4.6931 - acc: 0.028 - ETA: 2s - loss: 4.6935 - acc: 0.027 - ETA: 2s - loss: 4.6934 - acc: 0.028 - ETA: 1s - loss: 4.6933 - acc: 0.028 - ETA: 1s - loss: 4.6931 - acc: 0.028 - ETA: 1s - loss: 4.6929 - acc: 0.027 - ETA: 0s - loss: 4.6926 - acc: 0.027 - ETA: 0s - loss: 4.6926 - acc: 0.028 - ETA: 0s - loss: 4.6929 - acc: 0.0279Epoch 00005: val_loss improved from 4.74242 to 4.72525, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 100s - loss: 4.6925 - acc: 0.0278 - val_loss: 4.7253 - val_acc: 0.0263\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 94s - loss: 4.6705 - acc: 0.05 - ETA: 93s - loss: 4.5951 - acc: 0.05 - ETA: 95s - loss: 4.6971 - acc: 0.03 - ETA: 95s - loss: 4.7098 - acc: 0.02 - ETA: 95s - loss: 4.7226 - acc: 0.03 - ETA: 95s - loss: 4.7174 - acc: 0.02 - ETA: 95s - loss: 4.6911 - acc: 0.03 - ETA: 94s - loss: 4.7028 - acc: 0.03 - ETA: 94s - loss: 4.7060 - acc: 0.03 - ETA: 95s - loss: 4.7191 - acc: 0.03 - ETA: 94s - loss: 4.7099 - acc: 0.03 - ETA: 94s - loss: 4.7116 - acc: 0.03 - ETA: 93s - loss: 4.7000 - acc: 0.03 - ETA: 93s - loss: 4.6995 - acc: 0.03 - ETA: 93s - loss: 4.6924 - acc: 0.03 - ETA: 93s - loss: 4.6979 - acc: 0.03 - ETA: 93s - loss: 4.6901 - acc: 0.03 - ETA: 92s - loss: 4.6756 - acc: 0.03 - ETA: 92s - loss: 4.6764 - acc: 0.03 - ETA: 92s - loss: 4.6703 - acc: 0.03 - ETA: 91s - loss: 4.6819 - acc: 0.03 - ETA: 91s - loss: 4.6741 - acc: 0.03 - ETA: 91s - loss: 4.6752 - acc: 0.03 - ETA: 91s - loss: 4.6726 - acc: 0.03 - ETA: 90s - loss: 4.6705 - acc: 0.03 - ETA: 90s - loss: 4.6746 - acc: 0.03 - ETA: 90s - loss: 4.6808 - acc: 0.03 - ETA: 89s - loss: 4.6762 - acc: 0.03 - ETA: 89s - loss: 4.6720 - acc: 0.03 - ETA: 89s - loss: 4.6626 - acc: 0.03 - ETA: 88s - loss: 4.6659 - acc: 0.03 - ETA: 88s - loss: 4.6638 - acc: 0.03 - ETA: 88s - loss: 4.6649 - acc: 0.03 - ETA: 87s - loss: 4.6632 - acc: 0.03 - ETA: 87s - loss: 4.6615 - acc: 0.03 - ETA: 87s - loss: 4.6551 - acc: 0.03 - ETA: 87s - loss: 4.6535 - acc: 0.02 - ETA: 86s - loss: 4.6703 - acc: 0.02 - ETA: 86s - loss: 4.6739 - acc: 0.02 - ETA: 86s - loss: 4.6785 - acc: 0.02 - ETA: 85s - loss: 4.6718 - acc: 0.02 - ETA: 85s - loss: 4.6666 - acc: 0.02 - ETA: 85s - loss: 4.6672 - acc: 0.02 - ETA: 85s - loss: 4.6634 - acc: 0.03 - ETA: 84s - loss: 4.6631 - acc: 0.03 - ETA: 84s - loss: 4.6651 - acc: 0.03 - ETA: 84s - loss: 4.6577 - acc: 0.03 - ETA: 83s - loss: 4.6562 - acc: 0.03 - ETA: 83s - loss: 4.6559 - acc: 0.03 - ETA: 83s - loss: 4.6542 - acc: 0.03 - ETA: 82s - loss: 4.6502 - acc: 0.03 - ETA: 82s - loss: 4.6549 - acc: 0.03 - ETA: 82s - loss: 4.6524 - acc: 0.03 - ETA: 82s - loss: 4.6486 - acc: 0.03 - ETA: 81s - loss: 4.6522 - acc: 0.03 - ETA: 81s - loss: 4.6522 - acc: 0.03 - ETA: 81s - loss: 4.6503 - acc: 0.03 - ETA: 80s - loss: 4.6548 - acc: 0.03 - ETA: 80s - loss: 4.6520 - acc: 0.03 - ETA: 80s - loss: 4.6555 - acc: 0.03 - ETA: 80s - loss: 4.6563 - acc: 0.03 - ETA: 79s - loss: 4.6514 - acc: 0.03 - ETA: 79s - loss: 4.6516 - acc: 0.03 - ETA: 79s - loss: 4.6507 - acc: 0.03 - ETA: 78s - loss: 4.6544 - acc: 0.03 - ETA: 78s - loss: 4.6525 - acc: 0.03 - ETA: 78s - loss: 4.6512 - acc: 0.03 - ETA: 78s - loss: 4.6533 - acc: 0.03 - ETA: 77s - loss: 4.6531 - acc: 0.03 - ETA: 77s - loss: 4.6522 - acc: 0.03 - ETA: 77s - loss: 4.6499 - acc: 0.03 - ETA: 76s - loss: 4.6494 - acc: 0.03 - ETA: 76s - loss: 4.6528 - acc: 0.03 - ETA: 76s - loss: 4.6557 - acc: 0.03 - ETA: 75s - loss: 4.6555 - acc: 0.03 - ETA: 75s - loss: 4.6578 - acc: 0.03 - ETA: 75s - loss: 4.6559 - acc: 0.03 - ETA: 75s - loss: 4.6563 - acc: 0.03 - ETA: 74s - loss: 4.6552 - acc: 0.03 - ETA: 74s - loss: 4.6521 - acc: 0.03 - ETA: 74s - loss: 4.6525 - acc: 0.03 - ETA: 73s - loss: 4.6527 - acc: 0.03 - ETA: 73s - loss: 4.6497 - acc: 0.03 - ETA: 73s - loss: 4.6499 - acc: 0.03 - ETA: 73s - loss: 4.6501 - acc: 0.03 - ETA: 72s - loss: 4.6504 - acc: 0.03 - ETA: 72s - loss: 4.6519 - acc: 0.02 - ETA: 72s - loss: 4.6525 - acc: 0.03 - ETA: 71s - loss: 4.6525 - acc: 0.03 - ETA: 71s - loss: 4.6513 - acc: 0.03 - ETA: 71s - loss: 4.6491 - acc: 0.03 - ETA: 71s - loss: 4.6497 - acc: 0.03 - ETA: 70s - loss: 4.6495 - acc: 0.03 - ETA: 70s - loss: 4.6506 - acc: 0.03 - ETA: 70s - loss: 4.6519 - acc: 0.03 - ETA: 69s - loss: 4.6517 - acc: 0.03 - ETA: 69s - loss: 4.6527 - acc: 0.02 - ETA: 69s - loss: 4.6533 - acc: 0.02 - ETA: 68s - loss: 4.6551 - acc: 0.02 - ETA: 68s - loss: 4.6560 - acc: 0.02 - ETA: 68s - loss: 4.6557 - acc: 0.02 - ETA: 68s - loss: 4.6554 - acc: 0.02 - ETA: 67s - loss: 4.6561 - acc: 0.02 - ETA: 67s - loss: 4.6562 - acc: 0.02 - ETA: 67s - loss: 4.6562 - acc: 0.02 - ETA: 66s - loss: 4.6563 - acc: 0.02 - ETA: 66s - loss: 4.6564 - acc: 0.03 - ETA: 66s - loss: 4.6594 - acc: 0.03 - ETA: 66s - loss: 4.6623 - acc: 0.02 - ETA: 65s - loss: 4.6612 - acc: 0.03 - ETA: 65s - loss: 4.6614 - acc: 0.02 - ETA: 65s - loss: 4.6602 - acc: 0.02 - ETA: 64s - loss: 4.6627 - acc: 0.02 - ETA: 64s - loss: 4.6627 - acc: 0.02 - ETA: 64s - loss: 4.6613 - acc: 0.03 - ETA: 64s - loss: 4.6619 - acc: 0.02 - ETA: 63s - loss: 4.6620 - acc: 0.02 - ETA: 63s - loss: 4.6627 - acc: 0.02 - ETA: 63s - loss: 4.6615 - acc: 0.02 - ETA: 62s - loss: 4.6618 - acc: 0.02 - ETA: 62s - loss: 4.6615 - acc: 0.02 - ETA: 62s - loss: 4.6635 - acc: 0.02 - ETA: 61s - loss: 4.6606 - acc: 0.03 - ETA: 61s - loss: 4.6606 - acc: 0.03 - ETA: 61s - loss: 4.6579 - acc: 0.03 - ETA: 61s - loss: 4.6600 - acc: 0.03 - ETA: 60s - loss: 4.6614 - acc: 0.03 - ETA: 60s - loss: 4.6617 - acc: 0.03 - ETA: 60s - loss: 4.6618 - acc: 0.03 - ETA: 59s - loss: 4.6619 - acc: 0.03 - ETA: 59s - loss: 4.6622 - acc: 0.03 - ETA: 59s - loss: 4.6636 - acc: 0.03 - ETA: 58s - loss: 4.6630 - acc: 0.03 - ETA: 58s - loss: 4.6627 - acc: 0.03 - ETA: 58s - loss: 4.6625 - acc: 0.03 - ETA: 58s - loss: 4.6634 - acc: 0.03 - ETA: 57s - loss: 4.6656 - acc: 0.03 - ETA: 57s - loss: 4.6657 - acc: 0.03 - ETA: 57s - loss: 4.6651 - acc: 0.03 - ETA: 56s - loss: 4.6665 - acc: 0.03 - ETA: 56s - loss: 4.6657 - acc: 0.03 - ETA: 56s - loss: 4.6657 - acc: 0.03 - ETA: 56s - loss: 4.6649 - acc: 0.03 - ETA: 56s - loss: 4.6660 - acc: 0.03 - ETA: 55s - loss: 4.6663 - acc: 0.03 - ETA: 55s - loss: 4.6694 - acc: 0.03 - ETA: 55s - loss: 4.6688 - acc: 0.03 - ETA: 54s - loss: 4.6684 - acc: 0.03 - ETA: 54s - loss: 4.6687 - acc: 0.03 - ETA: 54s - loss: 4.6682 - acc: 0.03 - ETA: 53s - loss: 4.6692 - acc: 0.03 - ETA: 53s - loss: 4.6676 - acc: 0.03 - ETA: 53s - loss: 4.6672 - acc: 0.03 - ETA: 53s - loss: 4.6660 - acc: 0.03 - ETA: 52s - loss: 4.6657 - acc: 0.03 - ETA: 52s - loss: 4.6661 - acc: 0.03 - ETA: 52s - loss: 4.6658 - acc: 0.03 - ETA: 51s - loss: 4.6659 - acc: 0.03 - ETA: 51s - loss: 4.6676 - acc: 0.03 - ETA: 51s - loss: 4.6670 - acc: 0.03 - ETA: 51s - loss: 4.6670 - acc: 0.03 - ETA: 50s - loss: 4.6668 - acc: 0.03 - ETA: 50s - loss: 4.6658 - acc: 0.03 - ETA: 50s - loss: 4.6653 - acc: 0.03 - ETA: 49s - loss: 4.6656 - acc: 0.03 - ETA: 49s - loss: 4.6644 - acc: 0.03 - ETA: 49s - loss: 4.6659 - acc: 0.03 - ETA: 48s - loss: 4.6667 - acc: 0.03 - ETA: 48s - loss: 4.6662 - acc: 0.03 - ETA: 48s - loss: 4.6669 - acc: 0.03 - ETA: 48s - loss: 4.6662 - acc: 0.03 - ETA: 47s - loss: 4.6657 - acc: 0.03 - ETA: 47s - loss: 4.6675 - acc: 0.03 - ETA: 47s - loss: 4.6658 - acc: 0.03 - ETA: 46s - loss: 4.6653 - acc: 0.03 - ETA: 46s - loss: 4.6670 - acc: 0.03 - ETA: 46s - loss: 4.6673 - acc: 0.03 - ETA: 46s - loss: 4.6691 - acc: 0.03 - ETA: 45s - loss: 4.6688 - acc: 0.03 - ETA: 45s - loss: 4.6679 - acc: 0.03 - ETA: 45s - loss: 4.6676 - acc: 0.03 - ETA: 44s - loss: 4.6674 - acc: 0.03 - ETA: 44s - loss: 4.6672 - acc: 0.03 - ETA: 44s - loss: 4.6666 - acc: 0.03 - ETA: 44s - loss: 4.6671 - acc: 0.03 - ETA: 43s - loss: 4.6683 - acc: 0.03 - ETA: 43s - loss: 4.6692 - acc: 0.03 - ETA: 43s - loss: 4.6682 - acc: 0.03 - ETA: 42s - loss: 4.6676 - acc: 0.03 - ETA: 42s - loss: 4.6676 - acc: 0.03 - ETA: 42s - loss: 4.6680 - acc: 0.03 - ETA: 41s - loss: 4.6681 - acc: 0.03 - ETA: 41s - loss: 4.6671 - acc: 0.03 - ETA: 41s - loss: 4.6676 - acc: 0.03 - ETA: 41s - loss: 4.6678 - acc: 0.03 - ETA: 40s - loss: 4.6684 - acc: 0.03 - ETA: 40s - loss: 4.6681 - acc: 0.03 - ETA: 40s - loss: 4.6681 - acc: 0.03 - ETA: 39s - loss: 4.6692 - acc: 0.03 - ETA: 39s - loss: 4.6689 - acc: 0.03 - ETA: 39s - loss: 4.6692 - acc: 0.03 - ETA: 39s - loss: 4.6697 - acc: 0.03 - ETA: 38s - loss: 4.6681 - acc: 0.03 - ETA: 38s - loss: 4.6687 - acc: 0.03 - ETA: 38s - loss: 4.6693 - acc: 0.03 - ETA: 37s - loss: 4.6703 - acc: 0.03 - ETA: 37s - loss: 4.6708 - acc: 0.03 - ETA: 37s - loss: 4.6705 - acc: 0.03 - ETA: 36s - loss: 4.6704 - acc: 0.03 - ETA: 36s - loss: 4.6703 - acc: 0.03 - ETA: 36s - loss: 4.6709 - acc: 0.03 - ETA: 36s - loss: 4.6703 - acc: 0.03 - ETA: 35s - loss: 4.6704 - acc: 0.03 - ETA: 35s - loss: 4.6701 - acc: 0.03 - ETA: 35s - loss: 4.6700 - acc: 0.03236660/6680 [============================>.] - ETA: 34s - loss: 4.6699 - acc: 0.03 - ETA: 34s - loss: 4.6713 - acc: 0.03 - ETA: 34s - loss: 4.6705 - acc: 0.03 - ETA: 34s - loss: 4.6708 - acc: 0.03 - ETA: 33s - loss: 4.6701 - acc: 0.03 - ETA: 33s - loss: 4.6703 - acc: 0.03 - ETA: 33s - loss: 4.6711 - acc: 0.03 - ETA: 32s - loss: 4.6706 - acc: 0.03 - ETA: 32s - loss: 4.6703 - acc: 0.03 - ETA: 32s - loss: 4.6699 - acc: 0.03 - ETA: 31s - loss: 4.6701 - acc: 0.03 - ETA: 31s - loss: 4.6698 - acc: 0.03 - ETA: 31s - loss: 4.6700 - acc: 0.03 - ETA: 31s - loss: 4.6703 - acc: 0.03 - ETA: 30s - loss: 4.6699 - acc: 0.03 - ETA: 30s - loss: 4.6712 - acc: 0.03 - ETA: 30s - loss: 4.6708 - acc: 0.03 - ETA: 29s - loss: 4.6719 - acc: 0.03 - ETA: 29s - loss: 4.6715 - acc: 0.03 - ETA: 29s - loss: 4.6710 - acc: 0.03 - ETA: 29s - loss: 4.6709 - acc: 0.03 - ETA: 28s - loss: 4.6711 - acc: 0.03 - ETA: 28s - loss: 4.6710 - acc: 0.03 - ETA: 28s - loss: 4.6713 - acc: 0.03 - ETA: 27s - loss: 4.6713 - acc: 0.03 - ETA: 27s - loss: 4.6726 - acc: 0.03 - ETA: 27s - loss: 4.6717 - acc: 0.03 - ETA: 26s - loss: 4.6719 - acc: 0.03 - ETA: 26s - loss: 4.6720 - acc: 0.03 - ETA: 26s - loss: 4.6725 - acc: 0.03 - ETA: 26s - loss: 4.6719 - acc: 0.03 - ETA: 25s - loss: 4.6709 - acc: 0.03 - ETA: 25s - loss: 4.6699 - acc: 0.03 - ETA: 25s - loss: 4.6705 - acc: 0.03 - ETA: 24s - loss: 4.6718 - acc: 0.03 - ETA: 24s - loss: 4.6724 - acc: 0.03 - ETA: 24s - loss: 4.6728 - acc: 0.03 - ETA: 23s - loss: 4.6727 - acc: 0.03 - ETA: 23s - loss: 4.6727 - acc: 0.03 - ETA: 23s - loss: 4.6732 - acc: 0.03 - ETA: 23s - loss: 4.6737 - acc: 0.03 - ETA: 22s - loss: 4.6733 - acc: 0.03 - ETA: 22s - loss: 4.6737 - acc: 0.03 - ETA: 22s - loss: 4.6743 - acc: 0.03 - ETA: 21s - loss: 4.6747 - acc: 0.03 - ETA: 21s - loss: 4.6752 - acc: 0.02 - ETA: 21s - loss: 4.6749 - acc: 0.02 - ETA: 21s - loss: 4.6752 - acc: 0.02 - ETA: 20s - loss: 4.6754 - acc: 0.02 - ETA: 20s - loss: 4.6750 - acc: 0.02 - ETA: 20s - loss: 4.6753 - acc: 0.02 - ETA: 19s - loss: 4.6746 - acc: 0.02 - ETA: 19s - loss: 4.6757 - acc: 0.02 - ETA: 19s - loss: 4.6751 - acc: 0.02 - ETA: 18s - loss: 4.6742 - acc: 0.02 - ETA: 18s - loss: 4.6745 - acc: 0.02 - ETA: 18s - loss: 4.6752 - acc: 0.02 - ETA: 18s - loss: 4.6753 - acc: 0.02 - ETA: 17s - loss: 4.6753 - acc: 0.02 - ETA: 17s - loss: 4.6763 - acc: 0.02 - ETA: 17s - loss: 4.6768 - acc: 0.02 - ETA: 16s - loss: 4.6766 - acc: 0.02 - ETA: 16s - loss: 4.6766 - acc: 0.02 - ETA: 16s - loss: 4.6766 - acc: 0.02 - ETA: 16s - loss: 4.6771 - acc: 0.02 - ETA: 15s - loss: 4.6771 - acc: 0.02 - ETA: 15s - loss: 4.6770 - acc: 0.02 - ETA: 15s - loss: 4.6777 - acc: 0.02 - ETA: 14s - loss: 4.6779 - acc: 0.02 - ETA: 14s - loss: 4.6771 - acc: 0.02 - ETA: 14s - loss: 4.6771 - acc: 0.02 - ETA: 13s - loss: 4.6760 - acc: 0.02 - ETA: 13s - loss: 4.6750 - acc: 0.02 - ETA: 13s - loss: 4.6744 - acc: 0.02 - ETA: 13s - loss: 4.6746 - acc: 0.02 - ETA: 12s - loss: 4.6738 - acc: 0.02 - ETA: 12s - loss: 4.6735 - acc: 0.02 - ETA: 12s - loss: 4.6728 - acc: 0.02 - ETA: 11s - loss: 4.6729 - acc: 0.02 - ETA: 11s - loss: 4.6717 - acc: 0.02 - ETA: 11s - loss: 4.6717 - acc: 0.02 - ETA: 10s - loss: 4.6718 - acc: 0.02 - ETA: 10s - loss: 4.6725 - acc: 0.02 - ETA: 10s - loss: 4.6730 - acc: 0.02 - ETA: 10s - loss: 4.6734 - acc: 0.02 - ETA: 9s - loss: 4.6738 - acc: 0.0294 - ETA: 9s - loss: 4.6737 - acc: 0.029 - ETA: 9s - loss: 4.6743 - acc: 0.029 - ETA: 8s - loss: 4.6748 - acc: 0.029 - ETA: 8s - loss: 4.6744 - acc: 0.029 - ETA: 8s - loss: 4.6742 - acc: 0.029 - ETA: 8s - loss: 4.6749 - acc: 0.029 - ETA: 7s - loss: 4.6745 - acc: 0.029 - ETA: 7s - loss: 4.6742 - acc: 0.029 - ETA: 7s - loss: 4.6742 - acc: 0.029 - ETA: 6s - loss: 4.6741 - acc: 0.029 - ETA: 6s - loss: 4.6746 - acc: 0.029 - ETA: 6s - loss: 4.6738 - acc: 0.029 - ETA: 5s - loss: 4.6727 - acc: 0.029 - ETA: 5s - loss: 4.6721 - acc: 0.029 - ETA: 5s - loss: 4.6721 - acc: 0.029 - ETA: 5s - loss: 4.6725 - acc: 0.029 - ETA: 4s - loss: 4.6712 - acc: 0.029 - ETA: 4s - loss: 4.6717 - acc: 0.029 - ETA: 4s - loss: 4.6718 - acc: 0.029 - ETA: 3s - loss: 4.6725 - acc: 0.029 - ETA: 3s - loss: 4.6720 - acc: 0.029 - ETA: 3s - loss: 4.6723 - acc: 0.029 - ETA: 2s - loss: 4.6726 - acc: 0.029 - ETA: 2s - loss: 4.6727 - acc: 0.029 - ETA: 2s - loss: 4.6730 - acc: 0.029 - ETA: 2s - loss: 4.6729 - acc: 0.029 - ETA: 1s - loss: 4.6733 - acc: 0.029 - ETA: 1s - loss: 4.6736 - acc: 0.029 - ETA: 1s - loss: 4.6731 - acc: 0.029 - ETA: 0s - loss: 4.6735 - acc: 0.029 - ETA: 0s - loss: 4.6734 - acc: 0.029 - ETA: 0s - loss: 4.6735 - acc: 0.0290Epoch 00006: val_loss improved from 4.72525 to 4.71741, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 101s - loss: 4.6732 - acc: 0.0289 - val_loss: 4.7174 - val_acc: 0.0323\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 94s - loss: 4.4817 - acc: 0.10 - ETA: 97s - loss: 4.5635 - acc: 0.05 - ETA: 97s - loss: 4.6251 - acc: 0.03 - ETA: 97s - loss: 4.6351 - acc: 0.02 - ETA: 97s - loss: 4.6693 - acc: 0.03 - ETA: 96s - loss: 4.6683 - acc: 0.03 - ETA: 96s - loss: 4.6943 - acc: 0.02 - ETA: 96s - loss: 4.6884 - acc: 0.02 - ETA: 96s - loss: 4.7011 - acc: 0.02 - ETA: 95s - loss: 4.6969 - acc: 0.02 - ETA: 95s - loss: 4.6887 - acc: 0.02 - ETA: 95s - loss: 4.6970 - acc: 0.02 - ETA: 95s - loss: 4.7039 - acc: 0.02 - ETA: 94s - loss: 4.6882 - acc: 0.02 - ETA: 94s - loss: 4.6904 - acc: 0.03 - ETA: 94s - loss: 4.6893 - acc: 0.02 - ETA: 93s - loss: 4.6879 - acc: 0.02 - ETA: 93s - loss: 4.6895 - acc: 0.03 - ETA: 93s - loss: 4.6877 - acc: 0.03 - ETA: 93s - loss: 4.6885 - acc: 0.03 - ETA: 92s - loss: 4.6848 - acc: 0.03 - ETA: 92s - loss: 4.6762 - acc: 0.03 - ETA: 92s - loss: 4.6769 - acc: 0.03 - ETA: 92s - loss: 4.6864 - acc: 0.03 - ETA: 91s - loss: 4.6787 - acc: 0.03 - ETA: 91s - loss: 4.6757 - acc: 0.03 - ETA: 91s - loss: 4.6748 - acc: 0.03 - ETA: 91s - loss: 4.6723 - acc: 0.03 - ETA: 90s - loss: 4.6730 - acc: 0.03 - ETA: 90s - loss: 4.6774 - acc: 0.03 - ETA: 90s - loss: 4.6741 - acc: 0.03 - ETA: 89s - loss: 4.6694 - acc: 0.03 - ETA: 89s - loss: 4.6687 - acc: 0.03 - ETA: 89s - loss: 4.6666 - acc: 0.03 - ETA: 88s - loss: 4.6707 - acc: 0.03 - ETA: 88s - loss: 4.6723 - acc: 0.03 - ETA: 88s - loss: 4.6697 - acc: 0.03 - ETA: 87s - loss: 4.6624 - acc: 0.03 - ETA: 87s - loss: 4.6576 - acc: 0.03 - ETA: 87s - loss: 4.6519 - acc: 0.03 - ETA: 86s - loss: 4.6472 - acc: 0.03 - ETA: 86s - loss: 4.6410 - acc: 0.03 - ETA: 86s - loss: 4.6492 - acc: 0.03 - ETA: 85s - loss: 4.6500 - acc: 0.03 - ETA: 85s - loss: 4.6504 - acc: 0.03 - ETA: 85s - loss: 4.6475 - acc: 0.03 - ETA: 85s - loss: 4.6460 - acc: 0.03 - ETA: 84s - loss: 4.6485 - acc: 0.03 - ETA: 84s - loss: 4.6452 - acc: 0.03 - ETA: 84s - loss: 4.6474 - acc: 0.03 - ETA: 83s - loss: 4.6440 - acc: 0.03 - ETA: 83s - loss: 4.6400 - acc: 0.03 - ETA: 83s - loss: 4.6371 - acc: 0.03 - ETA: 82s - loss: 4.6409 - acc: 0.03 - ETA: 82s - loss: 4.6458 - acc: 0.03 - ETA: 82s - loss: 4.6455 - acc: 0.03 - ETA: 82s - loss: 4.6481 - acc: 0.03 - ETA: 81s - loss: 4.6453 - acc: 0.03 - ETA: 81s - loss: 4.6407 - acc: 0.03 - ETA: 81s - loss: 4.6406 - acc: 0.03 - ETA: 81s - loss: 4.6430 - acc: 0.03 - ETA: 80s - loss: 4.6464 - acc: 0.03 - ETA: 80s - loss: 4.6495 - acc: 0.03 - ETA: 80s - loss: 4.6479 - acc: 0.03 - ETA: 79s - loss: 4.6473 - acc: 0.03 - ETA: 79s - loss: 4.6459 - acc: 0.03 - ETA: 79s - loss: 4.6449 - acc: 0.03 - ETA: 78s - loss: 4.6455 - acc: 0.03 - ETA: 78s - loss: 4.6432 - acc: 0.03 - ETA: 78s - loss: 4.6456 - acc: 0.03 - ETA: 77s - loss: 4.6450 - acc: 0.03 - ETA: 77s - loss: 4.6466 - acc: 0.03 - ETA: 77s - loss: 4.6476 - acc: 0.03 - ETA: 77s - loss: 4.6450 - acc: 0.03 - ETA: 76s - loss: 4.6461 - acc: 0.03 - ETA: 76s - loss: 4.6456 - acc: 0.03 - ETA: 76s - loss: 4.6467 - acc: 0.03 - ETA: 75s - loss: 4.6474 - acc: 0.03 - ETA: 75s - loss: 4.6452 - acc: 0.03 - ETA: 75s - loss: 4.6459 - acc: 0.03 - ETA: 75s - loss: 4.6437 - acc: 0.03 - ETA: 74s - loss: 4.6453 - acc: 0.03 - ETA: 74s - loss: 4.6461 - acc: 0.03 - ETA: 74s - loss: 4.6446 - acc: 0.03 - ETA: 74s - loss: 4.6464 - acc: 0.03 - ETA: 73s - loss: 4.6453 - acc: 0.03 - ETA: 73s - loss: 4.6464 - acc: 0.03 - ETA: 73s - loss: 4.6482 - acc: 0.03 - ETA: 72s - loss: 4.6462 - acc: 0.03 - ETA: 72s - loss: 4.6463 - acc: 0.03 - ETA: 72s - loss: 4.6514 - acc: 0.03 - ETA: 71s - loss: 4.6527 - acc: 0.03 - ETA: 71s - loss: 4.6520 - acc: 0.03 - ETA: 71s - loss: 4.6529 - acc: 0.03 - ETA: 71s - loss: 4.6515 - acc: 0.03 - ETA: 70s - loss: 4.6508 - acc: 0.03 - ETA: 70s - loss: 4.6497 - acc: 0.03 - ETA: 70s - loss: 4.6510 - acc: 0.03 - ETA: 69s - loss: 4.6489 - acc: 0.03 - ETA: 69s - loss: 4.6481 - acc: 0.03 - ETA: 69s - loss: 4.6489 - acc: 0.03 - ETA: 68s - loss: 4.6517 - acc: 0.03 - ETA: 68s - loss: 4.6488 - acc: 0.03 - ETA: 68s - loss: 4.6500 - acc: 0.03 - ETA: 68s - loss: 4.6513 - acc: 0.03 - ETA: 67s - loss: 4.6527 - acc: 0.03 - ETA: 67s - loss: 4.6540 - acc: 0.03 - ETA: 67s - loss: 4.6529 - acc: 0.03 - ETA: 66s - loss: 4.6530 - acc: 0.03 - ETA: 66s - loss: 4.6529 - acc: 0.03 - ETA: 66s - loss: 4.6551 - acc: 0.03 - ETA: 65s - loss: 4.6578 - acc: 0.03 - ETA: 65s - loss: 4.6576 - acc: 0.03 - ETA: 65s - loss: 4.6569 - acc: 0.03 - ETA: 64s - loss: 4.6572 - acc: 0.03 - ETA: 64s - loss: 4.6546 - acc: 0.03 - ETA: 64s - loss: 4.6560 - acc: 0.03 - ETA: 64s - loss: 4.6567 - acc: 0.03 - ETA: 63s - loss: 4.6589 - acc: 0.03 - ETA: 63s - loss: 4.6600 - acc: 0.03 - ETA: 63s - loss: 4.6602 - acc: 0.03 - ETA: 62s - loss: 4.6593 - acc: 0.03 - ETA: 62s - loss: 4.6588 - acc: 0.03 - ETA: 62s - loss: 4.6600 - acc: 0.03 - ETA: 61s - loss: 4.6590 - acc: 0.03 - ETA: 61s - loss: 4.6596 - acc: 0.03 - ETA: 61s - loss: 4.6599 - acc: 0.03 - ETA: 61s - loss: 4.6601 - acc: 0.03 - ETA: 60s - loss: 4.6587 - acc: 0.03 - ETA: 60s - loss: 4.6559 - acc: 0.03 - ETA: 60s - loss: 4.6551 - acc: 0.03 - ETA: 59s - loss: 4.6545 - acc: 0.03 - ETA: 59s - loss: 4.6531 - acc: 0.03 - ETA: 59s - loss: 4.6533 - acc: 0.03 - ETA: 59s - loss: 4.6543 - acc: 0.03 - ETA: 58s - loss: 4.6549 - acc: 0.03 - ETA: 58s - loss: 4.6548 - acc: 0.03 - ETA: 58s - loss: 4.6544 - acc: 0.03 - ETA: 57s - loss: 4.6541 - acc: 0.03 - ETA: 57s - loss: 4.6561 - acc: 0.03 - ETA: 57s - loss: 4.6559 - acc: 0.03 - ETA: 56s - loss: 4.6563 - acc: 0.03 - ETA: 56s - loss: 4.6558 - acc: 0.03 - ETA: 56s - loss: 4.6561 - acc: 0.03 - ETA: 56s - loss: 4.6571 - acc: 0.03 - ETA: 55s - loss: 4.6563 - acc: 0.03 - ETA: 55s - loss: 4.6554 - acc: 0.03 - ETA: 55s - loss: 4.6557 - acc: 0.03 - ETA: 54s - loss: 4.6564 - acc: 0.03 - ETA: 54s - loss: 4.6554 - acc: 0.03 - ETA: 54s - loss: 4.6565 - acc: 0.03 - ETA: 53s - loss: 4.6561 - acc: 0.03 - ETA: 53s - loss: 4.6564 - acc: 0.03 - ETA: 53s - loss: 4.6556 - acc: 0.03 - ETA: 53s - loss: 4.6583 - acc: 0.03 - ETA: 52s - loss: 4.6576 - acc: 0.03 - ETA: 52s - loss: 4.6597 - acc: 0.03 - ETA: 52s - loss: 4.6598 - acc: 0.03 - ETA: 51s - loss: 4.6606 - acc: 0.03 - ETA: 51s - loss: 4.6615 - acc: 0.03 - ETA: 51s - loss: 4.6616 - acc: 0.03 - ETA: 50s - loss: 4.6593 - acc: 0.03 - ETA: 50s - loss: 4.6590 - acc: 0.03 - ETA: 50s - loss: 4.6589 - acc: 0.03 - ETA: 50s - loss: 4.6588 - acc: 0.03 - ETA: 49s - loss: 4.6598 - acc: 0.03 - ETA: 49s - loss: 4.6589 - acc: 0.03 - ETA: 49s - loss: 4.6581 - acc: 0.03 - ETA: 48s - loss: 4.6562 - acc: 0.03 - ETA: 48s - loss: 4.6560 - acc: 0.03 - ETA: 48s - loss: 4.6559 - acc: 0.03 - ETA: 47s - loss: 4.6550 - acc: 0.03 - ETA: 47s - loss: 4.6543 - acc: 0.03 - ETA: 47s - loss: 4.6545 - acc: 0.03 - ETA: 47s - loss: 4.6544 - acc: 0.03 - ETA: 46s - loss: 4.6538 - acc: 0.03 - ETA: 46s - loss: 4.6544 - acc: 0.03 - ETA: 46s - loss: 4.6537 - acc: 0.03 - ETA: 45s - loss: 4.6541 - acc: 0.03 - ETA: 45s - loss: 4.6557 - acc: 0.03 - ETA: 45s - loss: 4.6548 - acc: 0.03 - ETA: 45s - loss: 4.6545 - acc: 0.03 - ETA: 44s - loss: 4.6551 - acc: 0.03 - ETA: 44s - loss: 4.6550 - acc: 0.03 - ETA: 44s - loss: 4.6556 - acc: 0.03 - ETA: 43s - loss: 4.6540 - acc: 0.03 - ETA: 43s - loss: 4.6526 - acc: 0.03 - ETA: 43s - loss: 4.6531 - acc: 0.03 - ETA: 42s - loss: 4.6534 - acc: 0.03 - ETA: 42s - loss: 4.6532 - acc: 0.03 - ETA: 42s - loss: 4.6533 - acc: 0.03 - ETA: 42s - loss: 4.6540 - acc: 0.03 - ETA: 41s - loss: 4.6528 - acc: 0.03 - ETA: 41s - loss: 4.6517 - acc: 0.03 - ETA: 41s - loss: 4.6510 - acc: 0.03 - ETA: 40s - loss: 4.6511 - acc: 0.03 - ETA: 40s - loss: 4.6517 - acc: 0.03 - ETA: 40s - loss: 4.6515 - acc: 0.03 - ETA: 39s - loss: 4.6502 - acc: 0.03 - ETA: 39s - loss: 4.6510 - acc: 0.03 - ETA: 39s - loss: 4.6521 - acc: 0.03 - ETA: 39s - loss: 4.6531 - acc: 0.03 - ETA: 38s - loss: 4.6528 - acc: 0.03 - ETA: 38s - loss: 4.6536 - acc: 0.03 - ETA: 38s - loss: 4.6541 - acc: 0.03 - ETA: 37s - loss: 4.6534 - acc: 0.03 - ETA: 37s - loss: 4.6531 - acc: 0.03 - ETA: 37s - loss: 4.6533 - acc: 0.03 - ETA: 37s - loss: 4.6524 - acc: 0.03 - ETA: 36s - loss: 4.6532 - acc: 0.03 - ETA: 36s - loss: 4.6524 - acc: 0.03 - ETA: 36s - loss: 4.6518 - acc: 0.03 - ETA: 35s - loss: 4.6514 - acc: 0.03 - ETA: 35s - loss: 4.6524 - acc: 0.03 - ETA: 35s - loss: 4.6525 - acc: 0.0330"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 34s - loss: 4.6509 - acc: 0.03 - ETA: 34s - loss: 4.6507 - acc: 0.03 - ETA: 34s - loss: 4.6523 - acc: 0.03 - ETA: 34s - loss: 4.6508 - acc: 0.03 - ETA: 33s - loss: 4.6511 - acc: 0.03 - ETA: 33s - loss: 4.6511 - acc: 0.03 - ETA: 33s - loss: 4.6503 - acc: 0.03 - ETA: 32s - loss: 4.6497 - acc: 0.03 - ETA: 32s - loss: 4.6489 - acc: 0.03 - ETA: 32s - loss: 4.6498 - acc: 0.03 - ETA: 31s - loss: 4.6496 - acc: 0.03 - ETA: 31s - loss: 4.6505 - acc: 0.03 - ETA: 31s - loss: 4.6508 - acc: 0.03 - ETA: 31s - loss: 4.6510 - acc: 0.03 - ETA: 30s - loss: 4.6501 - acc: 0.03 - ETA: 30s - loss: 4.6504 - acc: 0.03 - ETA: 30s - loss: 4.6497 - acc: 0.03 - ETA: 29s - loss: 4.6499 - acc: 0.03 - ETA: 29s - loss: 4.6495 - acc: 0.03 - ETA: 29s - loss: 4.6487 - acc: 0.03 - ETA: 29s - loss: 4.6482 - acc: 0.03 - ETA: 28s - loss: 4.6477 - acc: 0.03 - ETA: 28s - loss: 4.6472 - acc: 0.03 - ETA: 28s - loss: 4.6470 - acc: 0.03 - ETA: 27s - loss: 4.6477 - acc: 0.03 - ETA: 27s - loss: 4.6479 - acc: 0.03 - ETA: 27s - loss: 4.6475 - acc: 0.03 - ETA: 26s - loss: 4.6472 - acc: 0.03 - ETA: 26s - loss: 4.6468 - acc: 0.03 - ETA: 26s - loss: 4.6472 - acc: 0.03 - ETA: 26s - loss: 4.6457 - acc: 0.03 - ETA: 25s - loss: 4.6460 - acc: 0.03 - ETA: 25s - loss: 4.6461 - acc: 0.03 - ETA: 25s - loss: 4.6461 - acc: 0.03 - ETA: 24s - loss: 4.6467 - acc: 0.03 - ETA: 24s - loss: 4.6471 - acc: 0.03 - ETA: 24s - loss: 4.6466 - acc: 0.03 - ETA: 23s - loss: 4.6470 - acc: 0.03 - ETA: 23s - loss: 4.6471 - acc: 0.03 - ETA: 23s - loss: 4.6477 - acc: 0.03 - ETA: 23s - loss: 4.6478 - acc: 0.03 - ETA: 22s - loss: 4.6472 - acc: 0.03 - ETA: 22s - loss: 4.6472 - acc: 0.03 - ETA: 22s - loss: 4.6478 - acc: 0.03 - ETA: 21s - loss: 4.6479 - acc: 0.03 - ETA: 21s - loss: 4.6463 - acc: 0.03 - ETA: 21s - loss: 4.6462 - acc: 0.03 - ETA: 21s - loss: 4.6457 - acc: 0.03 - ETA: 20s - loss: 4.6457 - acc: 0.03 - ETA: 20s - loss: 4.6453 - acc: 0.03 - ETA: 20s - loss: 4.6453 - acc: 0.03 - ETA: 19s - loss: 4.6465 - acc: 0.03 - ETA: 19s - loss: 4.6464 - acc: 0.03 - ETA: 19s - loss: 4.6458 - acc: 0.03 - ETA: 18s - loss: 4.6458 - acc: 0.03 - ETA: 18s - loss: 4.6453 - acc: 0.03 - ETA: 18s - loss: 4.6458 - acc: 0.03 - ETA: 18s - loss: 4.6453 - acc: 0.03 - ETA: 17s - loss: 4.6461 - acc: 0.03 - ETA: 17s - loss: 4.6460 - acc: 0.03 - ETA: 17s - loss: 4.6460 - acc: 0.03 - ETA: 16s - loss: 4.6456 - acc: 0.03 - ETA: 16s - loss: 4.6453 - acc: 0.03 - ETA: 16s - loss: 4.6449 - acc: 0.03 - ETA: 15s - loss: 4.6457 - acc: 0.03 - ETA: 15s - loss: 4.6459 - acc: 0.03 - ETA: 15s - loss: 4.6458 - acc: 0.03 - ETA: 15s - loss: 4.6462 - acc: 0.03 - ETA: 14s - loss: 4.6460 - acc: 0.03 - ETA: 14s - loss: 4.6456 - acc: 0.03 - ETA: 14s - loss: 4.6456 - acc: 0.03 - ETA: 13s - loss: 4.6460 - acc: 0.03 - ETA: 13s - loss: 4.6458 - acc: 0.03 - ETA: 13s - loss: 4.6459 - acc: 0.03 - ETA: 13s - loss: 4.6469 - acc: 0.03 - ETA: 12s - loss: 4.6469 - acc: 0.03 - ETA: 12s - loss: 4.6475 - acc: 0.03 - ETA: 12s - loss: 4.6480 - acc: 0.03 - ETA: 11s - loss: 4.6478 - acc: 0.03 - ETA: 11s - loss: 4.6486 - acc: 0.03 - ETA: 11s - loss: 4.6492 - acc: 0.03 - ETA: 10s - loss: 4.6486 - acc: 0.03 - ETA: 10s - loss: 4.6503 - acc: 0.03 - ETA: 10s - loss: 4.6504 - acc: 0.03 - ETA: 10s - loss: 4.6510 - acc: 0.03 - ETA: 9s - loss: 4.6510 - acc: 0.0331 - ETA: 9s - loss: 4.6507 - acc: 0.032 - ETA: 9s - loss: 4.6495 - acc: 0.033 - ETA: 8s - loss: 4.6497 - acc: 0.033 - ETA: 8s - loss: 4.6496 - acc: 0.033 - ETA: 8s - loss: 4.6488 - acc: 0.033 - ETA: 7s - loss: 4.6487 - acc: 0.033 - ETA: 7s - loss: 4.6500 - acc: 0.033 - ETA: 7s - loss: 4.6503 - acc: 0.033 - ETA: 7s - loss: 4.6512 - acc: 0.033 - ETA: 6s - loss: 4.6517 - acc: 0.033 - ETA: 6s - loss: 4.6507 - acc: 0.033 - ETA: 6s - loss: 4.6503 - acc: 0.033 - ETA: 5s - loss: 4.6504 - acc: 0.033 - ETA: 5s - loss: 4.6512 - acc: 0.033 - ETA: 5s - loss: 4.6510 - acc: 0.033 - ETA: 5s - loss: 4.6513 - acc: 0.033 - ETA: 4s - loss: 4.6516 - acc: 0.033 - ETA: 4s - loss: 4.6518 - acc: 0.033 - ETA: 4s - loss: 4.6522 - acc: 0.033 - ETA: 3s - loss: 4.6527 - acc: 0.033 - ETA: 3s - loss: 4.6525 - acc: 0.033 - ETA: 3s - loss: 4.6527 - acc: 0.033 - ETA: 2s - loss: 4.6534 - acc: 0.033 - ETA: 2s - loss: 4.6533 - acc: 0.033 - ETA: 2s - loss: 4.6523 - acc: 0.033 - ETA: 2s - loss: 4.6526 - acc: 0.033 - ETA: 1s - loss: 4.6533 - acc: 0.033 - ETA: 1s - loss: 4.6535 - acc: 0.033 - ETA: 1s - loss: 4.6531 - acc: 0.033 - ETA: 0s - loss: 4.6530 - acc: 0.034 - ETA: 0s - loss: 4.6529 - acc: 0.033 - ETA: 0s - loss: 4.6523 - acc: 0.0338Epoch 00007: val_loss improved from 4.71741 to 4.70066, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 101s - loss: 4.6532 - acc: 0.0337 - val_loss: 4.7007 - val_acc: 0.0263\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 97s - loss: 4.7093 - acc: 0.0000e+ - ETA: 97s - loss: 4.5815 - acc: 0.0250   - ETA: 97s - loss: 4.5554 - acc: 0.03 - ETA: 96s - loss: 4.5717 - acc: 0.03 - ETA: 97s - loss: 4.5745 - acc: 0.06 - ETA: 97s - loss: 4.5786 - acc: 0.05 - ETA: 96s - loss: 4.6131 - acc: 0.04 - ETA: 96s - loss: 4.6366 - acc: 0.03 - ETA: 96s - loss: 4.6399 - acc: 0.04 - ETA: 96s - loss: 4.6454 - acc: 0.05 - ETA: 95s - loss: 4.6457 - acc: 0.04 - ETA: 95s - loss: 4.6317 - acc: 0.04 - ETA: 95s - loss: 4.6400 - acc: 0.04 - ETA: 94s - loss: 4.6337 - acc: 0.04 - ETA: 94s - loss: 4.6236 - acc: 0.04 - ETA: 93s - loss: 4.6094 - acc: 0.04 - ETA: 94s - loss: 4.6144 - acc: 0.05 - ETA: 93s - loss: 4.6266 - acc: 0.05 - ETA: 93s - loss: 4.6270 - acc: 0.04 - ETA: 93s - loss: 4.6226 - acc: 0.04 - ETA: 92s - loss: 4.6183 - acc: 0.04 - ETA: 92s - loss: 4.6252 - acc: 0.04 - ETA: 92s - loss: 4.6197 - acc: 0.04 - ETA: 92s - loss: 4.6224 - acc: 0.04 - ETA: 91s - loss: 4.6309 - acc: 0.04 - ETA: 91s - loss: 4.6322 - acc: 0.03 - ETA: 91s - loss: 4.6249 - acc: 0.03 - ETA: 90s - loss: 4.6172 - acc: 0.03 - ETA: 90s - loss: 4.6278 - acc: 0.03 - ETA: 90s - loss: 4.6309 - acc: 0.03 - ETA: 90s - loss: 4.6266 - acc: 0.03 - ETA: 89s - loss: 4.6244 - acc: 0.03 - ETA: 89s - loss: 4.6231 - acc: 0.03 - ETA: 89s - loss: 4.6282 - acc: 0.03 - ETA: 88s - loss: 4.6387 - acc: 0.03 - ETA: 88s - loss: 4.6398 - acc: 0.03 - ETA: 88s - loss: 4.6372 - acc: 0.03 - ETA: 87s - loss: 4.6377 - acc: 0.03 - ETA: 87s - loss: 4.6338 - acc: 0.03 - ETA: 87s - loss: 4.6287 - acc: 0.03 - ETA: 86s - loss: 4.6266 - acc: 0.03 - ETA: 86s - loss: 4.6229 - acc: 0.03 - ETA: 86s - loss: 4.6227 - acc: 0.03 - ETA: 86s - loss: 4.6248 - acc: 0.03 - ETA: 85s - loss: 4.6299 - acc: 0.03 - ETA: 85s - loss: 4.6284 - acc: 0.03 - ETA: 85s - loss: 4.6336 - acc: 0.03 - ETA: 84s - loss: 4.6303 - acc: 0.03 - ETA: 84s - loss: 4.6389 - acc: 0.03 - ETA: 84s - loss: 4.6342 - acc: 0.03 - ETA: 83s - loss: 4.6371 - acc: 0.03 - ETA: 83s - loss: 4.6348 - acc: 0.03 - ETA: 83s - loss: 4.6342 - acc: 0.03 - ETA: 83s - loss: 4.6291 - acc: 0.03 - ETA: 82s - loss: 4.6362 - acc: 0.03 - ETA: 82s - loss: 4.6377 - acc: 0.03 - ETA: 82s - loss: 4.6387 - acc: 0.03 - ETA: 81s - loss: 4.6400 - acc: 0.03 - ETA: 81s - loss: 4.6404 - acc: 0.03 - ETA: 81s - loss: 4.6421 - acc: 0.03 - ETA: 80s - loss: 4.6459 - acc: 0.03 - ETA: 80s - loss: 4.6463 - acc: 0.03 - ETA: 80s - loss: 4.6465 - acc: 0.03 - ETA: 80s - loss: 4.6449 - acc: 0.03 - ETA: 79s - loss: 4.6432 - acc: 0.03 - ETA: 79s - loss: 4.6450 - acc: 0.03 - ETA: 79s - loss: 4.6446 - acc: 0.03 - ETA: 79s - loss: 4.6471 - acc: 0.03 - ETA: 78s - loss: 4.6474 - acc: 0.03 - ETA: 78s - loss: 4.6438 - acc: 0.03 - ETA: 78s - loss: 4.6439 - acc: 0.03 - ETA: 77s - loss: 4.6448 - acc: 0.03 - ETA: 77s - loss: 4.6440 - acc: 0.03 - ETA: 77s - loss: 4.6445 - acc: 0.03 - ETA: 76s - loss: 4.6460 - acc: 0.03 - ETA: 76s - loss: 4.6453 - acc: 0.03 - ETA: 76s - loss: 4.6423 - acc: 0.03 - ETA: 76s - loss: 4.6413 - acc: 0.03 - ETA: 75s - loss: 4.6433 - acc: 0.03 - ETA: 75s - loss: 4.6442 - acc: 0.03 - ETA: 75s - loss: 4.6454 - acc: 0.03 - ETA: 74s - loss: 4.6459 - acc: 0.02 - ETA: 74s - loss: 4.6459 - acc: 0.02 - ETA: 74s - loss: 4.6439 - acc: 0.03 - ETA: 73s - loss: 4.6450 - acc: 0.03 - ETA: 73s - loss: 4.6428 - acc: 0.03 - ETA: 73s - loss: 4.6423 - acc: 0.03 - ETA: 73s - loss: 4.6437 - acc: 0.03 - ETA: 72s - loss: 4.6435 - acc: 0.03 - ETA: 72s - loss: 4.6436 - acc: 0.03 - ETA: 72s - loss: 4.6449 - acc: 0.03 - ETA: 71s - loss: 4.6422 - acc: 0.03 - ETA: 71s - loss: 4.6425 - acc: 0.03 - ETA: 71s - loss: 4.6410 - acc: 0.03 - ETA: 70s - loss: 4.6421 - acc: 0.03 - ETA: 70s - loss: 4.6445 - acc: 0.03 - ETA: 70s - loss: 4.6426 - acc: 0.03 - ETA: 70s - loss: 4.6406 - acc: 0.03 - ETA: 69s - loss: 4.6382 - acc: 0.03 - ETA: 69s - loss: 4.6395 - acc: 0.03 - ETA: 69s - loss: 4.6409 - acc: 0.03 - ETA: 68s - loss: 4.6397 - acc: 0.03 - ETA: 68s - loss: 4.6401 - acc: 0.03 - ETA: 68s - loss: 4.6417 - acc: 0.03 - ETA: 67s - loss: 4.6405 - acc: 0.03 - ETA: 67s - loss: 4.6404 - acc: 0.03 - ETA: 67s - loss: 4.6402 - acc: 0.03 - ETA: 67s - loss: 4.6414 - acc: 0.03 - ETA: 66s - loss: 4.6432 - acc: 0.03 - ETA: 66s - loss: 4.6419 - acc: 0.03 - ETA: 66s - loss: 4.6405 - acc: 0.03 - ETA: 65s - loss: 4.6412 - acc: 0.03 - ETA: 65s - loss: 4.6400 - acc: 0.03 - ETA: 65s - loss: 4.6428 - acc: 0.03 - ETA: 64s - loss: 4.6431 - acc: 0.03 - ETA: 64s - loss: 4.6435 - acc: 0.03 - ETA: 64s - loss: 4.6442 - acc: 0.03 - ETA: 64s - loss: 4.6438 - acc: 0.03 - ETA: 63s - loss: 4.6426 - acc: 0.03 - ETA: 63s - loss: 4.6433 - acc: 0.03 - ETA: 63s - loss: 4.6434 - acc: 0.03 - ETA: 62s - loss: 4.6434 - acc: 0.03 - ETA: 62s - loss: 4.6449 - acc: 0.03 - ETA: 62s - loss: 4.6450 - acc: 0.03 - ETA: 62s - loss: 4.6467 - acc: 0.03 - ETA: 61s - loss: 4.6475 - acc: 0.03 - ETA: 61s - loss: 4.6468 - acc: 0.02 - ETA: 61s - loss: 4.6474 - acc: 0.02 - ETA: 60s - loss: 4.6445 - acc: 0.02 - ETA: 60s - loss: 4.6441 - acc: 0.03 - ETA: 60s - loss: 4.6426 - acc: 0.03 - ETA: 59s - loss: 4.6423 - acc: 0.03 - ETA: 59s - loss: 4.6422 - acc: 0.03 - ETA: 59s - loss: 4.6425 - acc: 0.03 - ETA: 59s - loss: 4.6424 - acc: 0.03 - ETA: 58s - loss: 4.6412 - acc: 0.03 - ETA: 58s - loss: 4.6447 - acc: 0.03 - ETA: 58s - loss: 4.6445 - acc: 0.03 - ETA: 57s - loss: 4.6429 - acc: 0.03 - ETA: 57s - loss: 4.6424 - acc: 0.03 - ETA: 57s - loss: 4.6423 - acc: 0.03 - ETA: 56s - loss: 4.6410 - acc: 0.03 - ETA: 56s - loss: 4.6414 - acc: 0.03 - ETA: 56s - loss: 4.6418 - acc: 0.03 - ETA: 56s - loss: 4.6409 - acc: 0.03 - ETA: 55s - loss: 4.6430 - acc: 0.03 - ETA: 55s - loss: 4.6433 - acc: 0.03 - ETA: 55s - loss: 4.6434 - acc: 0.03 - ETA: 54s - loss: 4.6440 - acc: 0.03 - ETA: 54s - loss: 4.6450 - acc: 0.03 - ETA: 54s - loss: 4.6463 - acc: 0.03 - ETA: 53s - loss: 4.6445 - acc: 0.03 - ETA: 53s - loss: 4.6457 - acc: 0.03 - ETA: 53s - loss: 4.6456 - acc: 0.03 - ETA: 53s - loss: 4.6470 - acc: 0.03 - ETA: 52s - loss: 4.6474 - acc: 0.03 - ETA: 52s - loss: 4.6479 - acc: 0.03 - ETA: 52s - loss: 4.6480 - acc: 0.03 - ETA: 51s - loss: 4.6494 - acc: 0.03 - ETA: 51s - loss: 4.6498 - acc: 0.03 - ETA: 51s - loss: 4.6500 - acc: 0.03 - ETA: 50s - loss: 4.6489 - acc: 0.03 - ETA: 50s - loss: 4.6475 - acc: 0.03 - ETA: 50s - loss: 4.6457 - acc: 0.03 - ETA: 50s - loss: 4.6441 - acc: 0.03 - ETA: 49s - loss: 4.6440 - acc: 0.03 - ETA: 49s - loss: 4.6430 - acc: 0.03 - ETA: 49s - loss: 4.6421 - acc: 0.03 - ETA: 48s - loss: 4.6422 - acc: 0.03 - ETA: 48s - loss: 4.6410 - acc: 0.03 - ETA: 48s - loss: 4.6422 - acc: 0.03 - ETA: 48s - loss: 4.6408 - acc: 0.03 - ETA: 47s - loss: 4.6385 - acc: 0.03 - ETA: 47s - loss: 4.6391 - acc: 0.03 - ETA: 47s - loss: 4.6386 - acc: 0.03 - ETA: 46s - loss: 4.6381 - acc: 0.03 - ETA: 46s - loss: 4.6389 - acc: 0.03 - ETA: 46s - loss: 4.6398 - acc: 0.03 - ETA: 45s - loss: 4.6394 - acc: 0.03 - ETA: 45s - loss: 4.6394 - acc: 0.03 - ETA: 45s - loss: 4.6386 - acc: 0.03 - ETA: 45s - loss: 4.6377 - acc: 0.03 - ETA: 44s - loss: 4.6362 - acc: 0.03 - ETA: 44s - loss: 4.6358 - acc: 0.03 - ETA: 44s - loss: 4.6360 - acc: 0.03 - ETA: 43s - loss: 4.6363 - acc: 0.03 - ETA: 43s - loss: 4.6362 - acc: 0.03 - ETA: 43s - loss: 4.6353 - acc: 0.03 - ETA: 42s - loss: 4.6345 - acc: 0.03 - ETA: 42s - loss: 4.6344 - acc: 0.03 - ETA: 42s - loss: 4.6358 - acc: 0.03 - ETA: 42s - loss: 4.6367 - acc: 0.03 - ETA: 41s - loss: 4.6367 - acc: 0.03 - ETA: 41s - loss: 4.6366 - acc: 0.03 - ETA: 41s - loss: 4.6357 - acc: 0.03 - ETA: 40s - loss: 4.6358 - acc: 0.03 - ETA: 40s - loss: 4.6353 - acc: 0.03 - ETA: 40s - loss: 4.6351 - acc: 0.03 - ETA: 40s - loss: 4.6354 - acc: 0.03 - ETA: 39s - loss: 4.6345 - acc: 0.03 - ETA: 39s - loss: 4.6344 - acc: 0.03 - ETA: 39s - loss: 4.6347 - acc: 0.03 - ETA: 38s - loss: 4.6339 - acc: 0.03 - ETA: 38s - loss: 4.6347 - acc: 0.03 - ETA: 38s - loss: 4.6345 - acc: 0.03 - ETA: 37s - loss: 4.6338 - acc: 0.03 - ETA: 37s - loss: 4.6335 - acc: 0.03 - ETA: 37s - loss: 4.6343 - acc: 0.03 - ETA: 37s - loss: 4.6335 - acc: 0.03 - ETA: 36s - loss: 4.6322 - acc: 0.03 - ETA: 36s - loss: 4.6322 - acc: 0.03 - ETA: 36s - loss: 4.6311 - acc: 0.03 - ETA: 35s - loss: 4.6308 - acc: 0.03 - ETA: 35s - loss: 4.6320 - acc: 0.03 - ETA: 35s - loss: 4.6322 - acc: 0.03356660/6680 [============================>.] - ETA: 34s - loss: 4.6326 - acc: 0.03 - ETA: 34s - loss: 4.6339 - acc: 0.03 - ETA: 34s - loss: 4.6343 - acc: 0.03 - ETA: 34s - loss: 4.6331 - acc: 0.03 - ETA: 33s - loss: 4.6321 - acc: 0.03 - ETA: 33s - loss: 4.6312 - acc: 0.03 - ETA: 33s - loss: 4.6335 - acc: 0.03 - ETA: 32s - loss: 4.6333 - acc: 0.03 - ETA: 32s - loss: 4.6343 - acc: 0.03 - ETA: 32s - loss: 4.6349 - acc: 0.03 - ETA: 32s - loss: 4.6336 - acc: 0.03 - ETA: 31s - loss: 4.6339 - acc: 0.03 - ETA: 31s - loss: 4.6348 - acc: 0.03 - ETA: 31s - loss: 4.6348 - acc: 0.03 - ETA: 30s - loss: 4.6346 - acc: 0.03 - ETA: 30s - loss: 4.6336 - acc: 0.03 - ETA: 30s - loss: 4.6331 - acc: 0.03 - ETA: 29s - loss: 4.6326 - acc: 0.03 - ETA: 29s - loss: 4.6329 - acc: 0.03 - ETA: 29s - loss: 4.6330 - acc: 0.03 - ETA: 29s - loss: 4.6325 - acc: 0.03 - ETA: 28s - loss: 4.6326 - acc: 0.03 - ETA: 28s - loss: 4.6323 - acc: 0.03 - ETA: 28s - loss: 4.6331 - acc: 0.03 - ETA: 27s - loss: 4.6324 - acc: 0.03 - ETA: 27s - loss: 4.6319 - acc: 0.03 - ETA: 27s - loss: 4.6332 - acc: 0.03 - ETA: 26s - loss: 4.6345 - acc: 0.03 - ETA: 26s - loss: 4.6349 - acc: 0.03 - ETA: 26s - loss: 4.6343 - acc: 0.03 - ETA: 26s - loss: 4.6336 - acc: 0.03 - ETA: 25s - loss: 4.6336 - acc: 0.03 - ETA: 25s - loss: 4.6334 - acc: 0.03 - ETA: 25s - loss: 4.6334 - acc: 0.03 - ETA: 24s - loss: 4.6330 - acc: 0.03 - ETA: 24s - loss: 4.6324 - acc: 0.03 - ETA: 24s - loss: 4.6324 - acc: 0.03 - ETA: 24s - loss: 4.6321 - acc: 0.03 - ETA: 23s - loss: 4.6323 - acc: 0.03 - ETA: 23s - loss: 4.6304 - acc: 0.03 - ETA: 23s - loss: 4.6303 - acc: 0.03 - ETA: 22s - loss: 4.6303 - acc: 0.03 - ETA: 22s - loss: 4.6296 - acc: 0.03 - ETA: 22s - loss: 4.6296 - acc: 0.03 - ETA: 21s - loss: 4.6307 - acc: 0.03 - ETA: 21s - loss: 4.6294 - acc: 0.03 - ETA: 21s - loss: 4.6292 - acc: 0.03 - ETA: 21s - loss: 4.6294 - acc: 0.03 - ETA: 20s - loss: 4.6292 - acc: 0.03 - ETA: 20s - loss: 4.6303 - acc: 0.03 - ETA: 20s - loss: 4.6293 - acc: 0.03 - ETA: 19s - loss: 4.6287 - acc: 0.03 - ETA: 19s - loss: 4.6289 - acc: 0.03 - ETA: 19s - loss: 4.6303 - acc: 0.03 - ETA: 18s - loss: 4.6307 - acc: 0.03 - ETA: 18s - loss: 4.6289 - acc: 0.03 - ETA: 18s - loss: 4.6284 - acc: 0.03 - ETA: 18s - loss: 4.6282 - acc: 0.03 - ETA: 17s - loss: 4.6269 - acc: 0.03 - ETA: 17s - loss: 4.6262 - acc: 0.03 - ETA: 17s - loss: 4.6255 - acc: 0.03 - ETA: 16s - loss: 4.6265 - acc: 0.03 - ETA: 16s - loss: 4.6275 - acc: 0.03 - ETA: 16s - loss: 4.6273 - acc: 0.03 - ETA: 16s - loss: 4.6277 - acc: 0.03 - ETA: 15s - loss: 4.6276 - acc: 0.03 - ETA: 15s - loss: 4.6290 - acc: 0.03 - ETA: 15s - loss: 4.6299 - acc: 0.03 - ETA: 14s - loss: 4.6301 - acc: 0.03 - ETA: 14s - loss: 4.6305 - acc: 0.03 - ETA: 14s - loss: 4.6310 - acc: 0.03 - ETA: 13s - loss: 4.6319 - acc: 0.03 - ETA: 13s - loss: 4.6316 - acc: 0.03 - ETA: 13s - loss: 4.6311 - acc: 0.03 - ETA: 13s - loss: 4.6316 - acc: 0.03 - ETA: 12s - loss: 4.6319 - acc: 0.03 - ETA: 12s - loss: 4.6326 - acc: 0.03 - ETA: 12s - loss: 4.6319 - acc: 0.03 - ETA: 11s - loss: 4.6312 - acc: 0.03 - ETA: 11s - loss: 4.6306 - acc: 0.03 - ETA: 11s - loss: 4.6307 - acc: 0.03 - ETA: 10s - loss: 4.6314 - acc: 0.03 - ETA: 10s - loss: 4.6314 - acc: 0.03 - ETA: 10s - loss: 4.6310 - acc: 0.03 - ETA: 10s - loss: 4.6306 - acc: 0.03 - ETA: 9s - loss: 4.6310 - acc: 0.0336 - ETA: 9s - loss: 4.6309 - acc: 0.033 - ETA: 9s - loss: 4.6313 - acc: 0.033 - ETA: 8s - loss: 4.6319 - acc: 0.033 - ETA: 8s - loss: 4.6321 - acc: 0.033 - ETA: 8s - loss: 4.6323 - acc: 0.033 - ETA: 7s - loss: 4.6317 - acc: 0.033 - ETA: 7s - loss: 4.6320 - acc: 0.033 - ETA: 7s - loss: 4.6322 - acc: 0.033 - ETA: 7s - loss: 4.6317 - acc: 0.032 - ETA: 6s - loss: 4.6310 - acc: 0.033 - ETA: 6s - loss: 4.6313 - acc: 0.033 - ETA: 6s - loss: 4.6325 - acc: 0.033 - ETA: 5s - loss: 4.6322 - acc: 0.033 - ETA: 5s - loss: 4.6327 - acc: 0.033 - ETA: 5s - loss: 4.6333 - acc: 0.032 - ETA: 5s - loss: 4.6336 - acc: 0.032 - ETA: 4s - loss: 4.6343 - acc: 0.032 - ETA: 4s - loss: 4.6344 - acc: 0.032 - ETA: 4s - loss: 4.6343 - acc: 0.032 - ETA: 3s - loss: 4.6347 - acc: 0.033 - ETA: 3s - loss: 4.6348 - acc: 0.032 - ETA: 3s - loss: 4.6353 - acc: 0.032 - ETA: 2s - loss: 4.6347 - acc: 0.032 - ETA: 2s - loss: 4.6347 - acc: 0.033 - ETA: 2s - loss: 4.6348 - acc: 0.033 - ETA: 2s - loss: 4.6352 - acc: 0.033 - ETA: 1s - loss: 4.6347 - acc: 0.033 - ETA: 1s - loss: 4.6358 - acc: 0.033 - ETA: 1s - loss: 4.6358 - acc: 0.033 - ETA: 0s - loss: 4.6359 - acc: 0.033 - ETA: 0s - loss: 4.6365 - acc: 0.033 - ETA: 0s - loss: 4.6367 - acc: 0.0332Epoch 00008: val_loss improved from 4.70066 to 4.69086, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 101s - loss: 4.6374 - acc: 0.0332 - val_loss: 4.6909 - val_acc: 0.0263\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 95s - loss: 4.8442 - acc: 0.05 - ETA: 96s - loss: 4.7412 - acc: 0.05 - ETA: 96s - loss: 4.7393 - acc: 0.03 - ETA: 96s - loss: 4.7121 - acc: 0.02 - ETA: 96s - loss: 4.6866 - acc: 0.03 - ETA: 96s - loss: 4.7119 - acc: 0.02 - ETA: 95s - loss: 4.7038 - acc: 0.03 - ETA: 95s - loss: 4.6646 - acc: 0.05 - ETA: 95s - loss: 4.6487 - acc: 0.05 - ETA: 95s - loss: 4.6423 - acc: 0.05 - ETA: 94s - loss: 4.6296 - acc: 0.04 - ETA: 94s - loss: 4.6224 - acc: 0.05 - ETA: 94s - loss: 4.6329 - acc: 0.05 - ETA: 94s - loss: 4.6471 - acc: 0.05 - ETA: 94s - loss: 4.6261 - acc: 0.04 - ETA: 94s - loss: 4.6502 - acc: 0.04 - ETA: 93s - loss: 4.6472 - acc: 0.04 - ETA: 93s - loss: 4.6429 - acc: 0.04 - ETA: 93s - loss: 4.6426 - acc: 0.04 - ETA: 92s - loss: 4.6490 - acc: 0.04 - ETA: 92s - loss: 4.6383 - acc: 0.04 - ETA: 92s - loss: 4.6380 - acc: 0.03 - ETA: 91s - loss: 4.6376 - acc: 0.03 - ETA: 91s - loss: 4.6460 - acc: 0.03 - ETA: 91s - loss: 4.6394 - acc: 0.03 - ETA: 91s - loss: 4.6412 - acc: 0.03 - ETA: 90s - loss: 4.6392 - acc: 0.03 - ETA: 90s - loss: 4.6347 - acc: 0.03 - ETA: 90s - loss: 4.6373 - acc: 0.03 - ETA: 90s - loss: 4.6357 - acc: 0.03 - ETA: 89s - loss: 4.6203 - acc: 0.03 - ETA: 89s - loss: 4.6220 - acc: 0.03 - ETA: 89s - loss: 4.6214 - acc: 0.03 - ETA: 88s - loss: 4.6207 - acc: 0.03 - ETA: 88s - loss: 4.6185 - acc: 0.03 - ETA: 88s - loss: 4.6195 - acc: 0.03 - ETA: 87s - loss: 4.6180 - acc: 0.03 - ETA: 87s - loss: 4.6141 - acc: 0.03 - ETA: 87s - loss: 4.6036 - acc: 0.03 - ETA: 87s - loss: 4.5933 - acc: 0.04 - ETA: 86s - loss: 4.5879 - acc: 0.04 - ETA: 86s - loss: 4.5862 - acc: 0.04 - ETA: 86s - loss: 4.5799 - acc: 0.04 - ETA: 86s - loss: 4.5826 - acc: 0.04 - ETA: 85s - loss: 4.5772 - acc: 0.04 - ETA: 85s - loss: 4.5795 - acc: 0.04 - ETA: 85s - loss: 4.5779 - acc: 0.04 - ETA: 84s - loss: 4.5839 - acc: 0.04 - ETA: 84s - loss: 4.5854 - acc: 0.04 - ETA: 84s - loss: 4.5862 - acc: 0.04 - ETA: 83s - loss: 4.5837 - acc: 0.04 - ETA: 83s - loss: 4.5827 - acc: 0.04 - ETA: 83s - loss: 4.5792 - acc: 0.04 - ETA: 83s - loss: 4.5820 - acc: 0.04 - ETA: 82s - loss: 4.5805 - acc: 0.04 - ETA: 82s - loss: 4.5837 - acc: 0.04 - ETA: 82s - loss: 4.5844 - acc: 0.04 - ETA: 81s - loss: 4.5839 - acc: 0.04 - ETA: 81s - loss: 4.5855 - acc: 0.04 - ETA: 81s - loss: 4.5840 - acc: 0.04 - ETA: 81s - loss: 4.5831 - acc: 0.04 - ETA: 80s - loss: 4.5804 - acc: 0.04 - ETA: 80s - loss: 4.5820 - acc: 0.04 - ETA: 80s - loss: 4.5848 - acc: 0.04 - ETA: 79s - loss: 4.5853 - acc: 0.04 - ETA: 79s - loss: 4.5851 - acc: 0.04 - ETA: 79s - loss: 4.5829 - acc: 0.04 - ETA: 78s - loss: 4.5843 - acc: 0.04 - ETA: 78s - loss: 4.5865 - acc: 0.04 - ETA: 78s - loss: 4.5850 - acc: 0.04 - ETA: 78s - loss: 4.5871 - acc: 0.04 - ETA: 77s - loss: 4.5923 - acc: 0.04 - ETA: 77s - loss: 4.5952 - acc: 0.04 - ETA: 77s - loss: 4.5948 - acc: 0.04 - ETA: 76s - loss: 4.5940 - acc: 0.04 - ETA: 76s - loss: 4.5914 - acc: 0.04 - ETA: 76s - loss: 4.5879 - acc: 0.04 - ETA: 76s - loss: 4.5908 - acc: 0.04 - ETA: 75s - loss: 4.5893 - acc: 0.04 - ETA: 75s - loss: 4.5918 - acc: 0.04 - ETA: 75s - loss: 4.5908 - acc: 0.04 - ETA: 74s - loss: 4.5936 - acc: 0.04 - ETA: 74s - loss: 4.5955 - acc: 0.04 - ETA: 74s - loss: 4.5987 - acc: 0.04 - ETA: 73s - loss: 4.5999 - acc: 0.04 - ETA: 73s - loss: 4.6003 - acc: 0.04 - ETA: 73s - loss: 4.6024 - acc: 0.04 - ETA: 73s - loss: 4.6010 - acc: 0.04 - ETA: 72s - loss: 4.5980 - acc: 0.04 - ETA: 72s - loss: 4.5996 - acc: 0.04 - ETA: 72s - loss: 4.6013 - acc: 0.04 - ETA: 71s - loss: 4.6063 - acc: 0.04 - ETA: 71s - loss: 4.6062 - acc: 0.04 - ETA: 71s - loss: 4.6049 - acc: 0.04 - ETA: 70s - loss: 4.6037 - acc: 0.04 - ETA: 70s - loss: 4.6052 - acc: 0.04 - ETA: 70s - loss: 4.6054 - acc: 0.04 - ETA: 70s - loss: 4.6061 - acc: 0.04 - ETA: 69s - loss: 4.6070 - acc: 0.04 - ETA: 69s - loss: 4.6069 - acc: 0.04 - ETA: 69s - loss: 4.6074 - acc: 0.04 - ETA: 68s - loss: 4.6047 - acc: 0.04 - ETA: 68s - loss: 4.6042 - acc: 0.04 - ETA: 68s - loss: 4.6071 - acc: 0.04 - ETA: 67s - loss: 4.6076 - acc: 0.04 - ETA: 67s - loss: 4.6081 - acc: 0.04 - ETA: 67s - loss: 4.6084 - acc: 0.04 - ETA: 67s - loss: 4.6086 - acc: 0.04 - ETA: 66s - loss: 4.6080 - acc: 0.04 - ETA: 66s - loss: 4.6069 - acc: 0.04 - ETA: 66s - loss: 4.6071 - acc: 0.04 - ETA: 65s - loss: 4.6067 - acc: 0.04 - ETA: 65s - loss: 4.6057 - acc: 0.04 - ETA: 65s - loss: 4.6073 - acc: 0.04 - ETA: 64s - loss: 4.6069 - acc: 0.04 - ETA: 64s - loss: 4.6036 - acc: 0.04 - ETA: 64s - loss: 4.6065 - acc: 0.04 - ETA: 64s - loss: 4.6063 - acc: 0.04 - ETA: 63s - loss: 4.6083 - acc: 0.04 - ETA: 63s - loss: 4.6072 - acc: 0.04 - ETA: 63s - loss: 4.6066 - acc: 0.04 - ETA: 62s - loss: 4.6061 - acc: 0.04 - ETA: 62s - loss: 4.6057 - acc: 0.04 - ETA: 62s - loss: 4.6069 - acc: 0.04 - ETA: 62s - loss: 4.6082 - acc: 0.04 - ETA: 61s - loss: 4.6084 - acc: 0.04 - ETA: 61s - loss: 4.6091 - acc: 0.04 - ETA: 61s - loss: 4.6090 - acc: 0.04 - ETA: 60s - loss: 4.6095 - acc: 0.04 - ETA: 60s - loss: 4.6071 - acc: 0.04 - ETA: 60s - loss: 4.6100 - acc: 0.04 - ETA: 59s - loss: 4.6096 - acc: 0.04 - ETA: 59s - loss: 4.6107 - acc: 0.04 - ETA: 59s - loss: 4.6111 - acc: 0.04 - ETA: 59s - loss: 4.6122 - acc: 0.04 - ETA: 58s - loss: 4.6127 - acc: 0.04 - ETA: 58s - loss: 4.6121 - acc: 0.04 - ETA: 58s - loss: 4.6130 - acc: 0.03 - ETA: 57s - loss: 4.6121 - acc: 0.03 - ETA: 57s - loss: 4.6137 - acc: 0.03 - ETA: 57s - loss: 4.6114 - acc: 0.03 - ETA: 56s - loss: 4.6111 - acc: 0.03 - ETA: 56s - loss: 4.6128 - acc: 0.03 - ETA: 56s - loss: 4.6124 - acc: 0.03 - ETA: 56s - loss: 4.6128 - acc: 0.03 - ETA: 55s - loss: 4.6121 - acc: 0.03 - ETA: 55s - loss: 4.6124 - acc: 0.03 - ETA: 55s - loss: 4.6120 - acc: 0.03 - ETA: 54s - loss: 4.6129 - acc: 0.03 - ETA: 54s - loss: 4.6120 - acc: 0.03 - ETA: 54s - loss: 4.6124 - acc: 0.03 - ETA: 54s - loss: 4.6111 - acc: 0.03 - ETA: 53s - loss: 4.6109 - acc: 0.03 - ETA: 53s - loss: 4.6119 - acc: 0.03 - ETA: 53s - loss: 4.6110 - acc: 0.03 - ETA: 52s - loss: 4.6095 - acc: 0.03 - ETA: 52s - loss: 4.6094 - acc: 0.03 - ETA: 52s - loss: 4.6086 - acc: 0.03 - ETA: 51s - loss: 4.6096 - acc: 0.03 - ETA: 51s - loss: 4.6094 - acc: 0.03 - ETA: 51s - loss: 4.6090 - acc: 0.04 - ETA: 51s - loss: 4.6107 - acc: 0.03 - ETA: 50s - loss: 4.6122 - acc: 0.03 - ETA: 50s - loss: 4.6110 - acc: 0.03 - ETA: 50s - loss: 4.6114 - acc: 0.03 - ETA: 49s - loss: 4.6094 - acc: 0.04 - ETA: 49s - loss: 4.6106 - acc: 0.04 - ETA: 49s - loss: 4.6098 - acc: 0.04 - ETA: 48s - loss: 4.6095 - acc: 0.04 - ETA: 48s - loss: 4.6101 - acc: 0.04 - ETA: 48s - loss: 4.6121 - acc: 0.04 - ETA: 48s - loss: 4.6128 - acc: 0.04 - ETA: 47s - loss: 4.6136 - acc: 0.04 - ETA: 47s - loss: 4.6127 - acc: 0.04 - ETA: 47s - loss: 4.6136 - acc: 0.04 - ETA: 46s - loss: 4.6146 - acc: 0.04 - ETA: 46s - loss: 4.6156 - acc: 0.03 - ETA: 46s - loss: 4.6150 - acc: 0.03 - ETA: 45s - loss: 4.6158 - acc: 0.04 - ETA: 45s - loss: 4.6169 - acc: 0.04 - ETA: 45s - loss: 4.6148 - acc: 0.04 - ETA: 45s - loss: 4.6145 - acc: 0.04 - ETA: 44s - loss: 4.6146 - acc: 0.04 - ETA: 44s - loss: 4.6142 - acc: 0.03 - ETA: 44s - loss: 4.6146 - acc: 0.03 - ETA: 43s - loss: 4.6145 - acc: 0.04 - ETA: 43s - loss: 4.6150 - acc: 0.03 - ETA: 43s - loss: 4.6149 - acc: 0.03 - ETA: 43s - loss: 4.6160 - acc: 0.03 - ETA: 42s - loss: 4.6161 - acc: 0.03 - ETA: 42s - loss: 4.6159 - acc: 0.03 - ETA: 42s - loss: 4.6143 - acc: 0.03 - ETA: 41s - loss: 4.6154 - acc: 0.03 - ETA: 41s - loss: 4.6148 - acc: 0.03 - ETA: 41s - loss: 4.6167 - acc: 0.03 - ETA: 40s - loss: 4.6172 - acc: 0.03 - ETA: 40s - loss: 4.6169 - acc: 0.03 - ETA: 40s - loss: 4.6157 - acc: 0.03 - ETA: 40s - loss: 4.6144 - acc: 0.03 - ETA: 39s - loss: 4.6151 - acc: 0.03 - ETA: 39s - loss: 4.6154 - acc: 0.03 - ETA: 39s - loss: 4.6144 - acc: 0.03 - ETA: 38s - loss: 4.6147 - acc: 0.03 - ETA: 38s - loss: 4.6155 - acc: 0.03 - ETA: 38s - loss: 4.6161 - acc: 0.03 - ETA: 37s - loss: 4.6170 - acc: 0.03 - ETA: 37s - loss: 4.6160 - acc: 0.03 - ETA: 37s - loss: 4.6169 - acc: 0.03 - ETA: 37s - loss: 4.6171 - acc: 0.03 - ETA: 36s - loss: 4.6180 - acc: 0.03 - ETA: 36s - loss: 4.6178 - acc: 0.03 - ETA: 36s - loss: 4.6171 - acc: 0.03 - ETA: 35s - loss: 4.6163 - acc: 0.03 - ETA: 35s - loss: 4.6153 - acc: 0.03 - ETA: 35s - loss: 4.6150 - acc: 0.0395"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 34s - loss: 4.6154 - acc: 0.03 - ETA: 34s - loss: 4.6155 - acc: 0.03 - ETA: 34s - loss: 4.6165 - acc: 0.03 - ETA: 34s - loss: 4.6162 - acc: 0.03 - ETA: 33s - loss: 4.6169 - acc: 0.03 - ETA: 33s - loss: 4.6158 - acc: 0.03 - ETA: 33s - loss: 4.6160 - acc: 0.03 - ETA: 32s - loss: 4.6151 - acc: 0.03 - ETA: 32s - loss: 4.6149 - acc: 0.03 - ETA: 32s - loss: 4.6148 - acc: 0.03 - ETA: 32s - loss: 4.6150 - acc: 0.03 - ETA: 31s - loss: 4.6154 - acc: 0.03 - ETA: 31s - loss: 4.6155 - acc: 0.03 - ETA: 31s - loss: 4.6153 - acc: 0.03 - ETA: 30s - loss: 4.6154 - acc: 0.03 - ETA: 30s - loss: 4.6168 - acc: 0.03 - ETA: 30s - loss: 4.6169 - acc: 0.03 - ETA: 29s - loss: 4.6163 - acc: 0.03 - ETA: 29s - loss: 4.6157 - acc: 0.03 - ETA: 29s - loss: 4.6158 - acc: 0.03 - ETA: 29s - loss: 4.6165 - acc: 0.03 - ETA: 28s - loss: 4.6171 - acc: 0.03 - ETA: 28s - loss: 4.6169 - acc: 0.03 - ETA: 28s - loss: 4.6162 - acc: 0.03 - ETA: 27s - loss: 4.6169 - acc: 0.03 - ETA: 27s - loss: 4.6170 - acc: 0.03 - ETA: 27s - loss: 4.6173 - acc: 0.03 - ETA: 26s - loss: 4.6172 - acc: 0.03 - ETA: 26s - loss: 4.6164 - acc: 0.03 - ETA: 26s - loss: 4.6157 - acc: 0.03 - ETA: 26s - loss: 4.6163 - acc: 0.03 - ETA: 25s - loss: 4.6161 - acc: 0.03 - ETA: 25s - loss: 4.6170 - acc: 0.03 - ETA: 25s - loss: 4.6173 - acc: 0.03 - ETA: 24s - loss: 4.6178 - acc: 0.03 - ETA: 24s - loss: 4.6193 - acc: 0.03 - ETA: 24s - loss: 4.6184 - acc: 0.03 - ETA: 24s - loss: 4.6179 - acc: 0.03 - ETA: 23s - loss: 4.6183 - acc: 0.03 - ETA: 23s - loss: 4.6182 - acc: 0.03 - ETA: 23s - loss: 4.6191 - acc: 0.03 - ETA: 22s - loss: 4.6178 - acc: 0.03 - ETA: 22s - loss: 4.6180 - acc: 0.03 - ETA: 22s - loss: 4.6173 - acc: 0.03 - ETA: 21s - loss: 4.6162 - acc: 0.03 - ETA: 21s - loss: 4.6166 - acc: 0.03 - ETA: 21s - loss: 4.6168 - acc: 0.03 - ETA: 21s - loss: 4.6167 - acc: 0.03 - ETA: 20s - loss: 4.6164 - acc: 0.03 - ETA: 20s - loss: 4.6166 - acc: 0.03 - ETA: 20s - loss: 4.6167 - acc: 0.03 - ETA: 19s - loss: 4.6170 - acc: 0.03 - ETA: 19s - loss: 4.6171 - acc: 0.03 - ETA: 19s - loss: 4.6171 - acc: 0.03 - ETA: 18s - loss: 4.6180 - acc: 0.03 - ETA: 18s - loss: 4.6183 - acc: 0.03 - ETA: 18s - loss: 4.6186 - acc: 0.03 - ETA: 18s - loss: 4.6185 - acc: 0.03 - ETA: 17s - loss: 4.6182 - acc: 0.03 - ETA: 17s - loss: 4.6189 - acc: 0.03 - ETA: 17s - loss: 4.6189 - acc: 0.03 - ETA: 16s - loss: 4.6188 - acc: 0.03 - ETA: 16s - loss: 4.6196 - acc: 0.03 - ETA: 16s - loss: 4.6190 - acc: 0.03 - ETA: 16s - loss: 4.6192 - acc: 0.03 - ETA: 15s - loss: 4.6188 - acc: 0.03 - ETA: 15s - loss: 4.6184 - acc: 0.03 - ETA: 15s - loss: 4.6185 - acc: 0.03 - ETA: 14s - loss: 4.6185 - acc: 0.03 - ETA: 14s - loss: 4.6183 - acc: 0.03 - ETA: 14s - loss: 4.6192 - acc: 0.03 - ETA: 13s - loss: 4.6191 - acc: 0.03 - ETA: 13s - loss: 4.6183 - acc: 0.03 - ETA: 13s - loss: 4.6178 - acc: 0.03 - ETA: 13s - loss: 4.6175 - acc: 0.03 - ETA: 12s - loss: 4.6173 - acc: 0.03 - ETA: 12s - loss: 4.6175 - acc: 0.03 - ETA: 12s - loss: 4.6177 - acc: 0.03 - ETA: 11s - loss: 4.6176 - acc: 0.03 - ETA: 11s - loss: 4.6173 - acc: 0.03 - ETA: 11s - loss: 4.6179 - acc: 0.03 - ETA: 10s - loss: 4.6176 - acc: 0.03 - ETA: 10s - loss: 4.6174 - acc: 0.03 - ETA: 10s - loss: 4.6176 - acc: 0.03 - ETA: 10s - loss: 4.6174 - acc: 0.03 - ETA: 9s - loss: 4.6168 - acc: 0.0364 - ETA: 9s - loss: 4.6169 - acc: 0.036 - ETA: 9s - loss: 4.6168 - acc: 0.036 - ETA: 8s - loss: 4.6170 - acc: 0.036 - ETA: 8s - loss: 4.6169 - acc: 0.036 - ETA: 8s - loss: 4.6181 - acc: 0.036 - ETA: 8s - loss: 4.6186 - acc: 0.036 - ETA: 7s - loss: 4.6188 - acc: 0.036 - ETA: 7s - loss: 4.6185 - acc: 0.036 - ETA: 7s - loss: 4.6193 - acc: 0.036 - ETA: 6s - loss: 4.6194 - acc: 0.036 - ETA: 6s - loss: 4.6192 - acc: 0.036 - ETA: 6s - loss: 4.6182 - acc: 0.036 - ETA: 5s - loss: 4.6177 - acc: 0.037 - ETA: 5s - loss: 4.6177 - acc: 0.037 - ETA: 5s - loss: 4.6182 - acc: 0.037 - ETA: 5s - loss: 4.6176 - acc: 0.037 - ETA: 4s - loss: 4.6172 - acc: 0.037 - ETA: 4s - loss: 4.6185 - acc: 0.037 - ETA: 4s - loss: 4.6184 - acc: 0.037 - ETA: 3s - loss: 4.6187 - acc: 0.037 - ETA: 3s - loss: 4.6186 - acc: 0.037 - ETA: 3s - loss: 4.6186 - acc: 0.037 - ETA: 2s - loss: 4.6191 - acc: 0.037 - ETA: 2s - loss: 4.6185 - acc: 0.037 - ETA: 2s - loss: 4.6183 - acc: 0.037 - ETA: 2s - loss: 4.6182 - acc: 0.037 - ETA: 1s - loss: 4.6186 - acc: 0.037 - ETA: 1s - loss: 4.6188 - acc: 0.037 - ETA: 1s - loss: 4.6201 - acc: 0.037 - ETA: 0s - loss: 4.6202 - acc: 0.036 - ETA: 0s - loss: 4.6198 - acc: 0.036 - ETA: 0s - loss: 4.6196 - acc: 0.0368Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 101s - loss: 4.6191 - acc: 0.0370 - val_loss: 4.6928 - val_acc: 0.0240\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 91s - loss: 4.5208 - acc: 0.0000e+ - ETA: 94s - loss: 4.4744 - acc: 0.0250   - ETA: 94s - loss: 4.4619 - acc: 0.01 - ETA: 94s - loss: 4.5590 - acc: 0.02 - ETA: 95s - loss: 4.5206 - acc: 0.02 - ETA: 95s - loss: 4.5074 - acc: 0.01 - ETA: 95s - loss: 4.5123 - acc: 0.01 - ETA: 95s - loss: 4.5248 - acc: 0.01 - ETA: 95s - loss: 4.5485 - acc: 0.01 - ETA: 95s - loss: 4.5589 - acc: 0.01 - ETA: 94s - loss: 4.5605 - acc: 0.01 - ETA: 94s - loss: 4.5618 - acc: 0.01 - ETA: 94s - loss: 4.5778 - acc: 0.01 - ETA: 94s - loss: 4.5716 - acc: 0.01 - ETA: 93s - loss: 4.5603 - acc: 0.02 - ETA: 93s - loss: 4.5679 - acc: 0.01 - ETA: 93s - loss: 4.5719 - acc: 0.01 - ETA: 93s - loss: 4.5692 - acc: 0.01 - ETA: 92s - loss: 4.5708 - acc: 0.01 - ETA: 92s - loss: 4.5534 - acc: 0.01 - ETA: 92s - loss: 4.5629 - acc: 0.01 - ETA: 91s - loss: 4.5586 - acc: 0.01 - ETA: 91s - loss: 4.5698 - acc: 0.01 - ETA: 91s - loss: 4.5583 - acc: 0.02 - ETA: 91s - loss: 4.5748 - acc: 0.02 - ETA: 90s - loss: 4.5749 - acc: 0.02 - ETA: 90s - loss: 4.5792 - acc: 0.02 - ETA: 90s - loss: 4.5877 - acc: 0.02 - ETA: 89s - loss: 4.5879 - acc: 0.02 - ETA: 89s - loss: 4.5918 - acc: 0.02 - ETA: 89s - loss: 4.5982 - acc: 0.02 - ETA: 89s - loss: 4.5947 - acc: 0.02 - ETA: 88s - loss: 4.6018 - acc: 0.02 - ETA: 88s - loss: 4.6010 - acc: 0.02 - ETA: 88s - loss: 4.6035 - acc: 0.02 - ETA: 87s - loss: 4.6010 - acc: 0.02 - ETA: 87s - loss: 4.6013 - acc: 0.02 - ETA: 87s - loss: 4.6027 - acc: 0.02 - ETA: 86s - loss: 4.6043 - acc: 0.02 - ETA: 86s - loss: 4.6006 - acc: 0.02 - ETA: 86s - loss: 4.5958 - acc: 0.03 - ETA: 86s - loss: 4.5970 - acc: 0.03 - ETA: 85s - loss: 4.5980 - acc: 0.03 - ETA: 85s - loss: 4.5939 - acc: 0.02 - ETA: 85s - loss: 4.5974 - acc: 0.02 - ETA: 84s - loss: 4.5996 - acc: 0.02 - ETA: 84s - loss: 4.6028 - acc: 0.02 - ETA: 84s - loss: 4.6025 - acc: 0.02 - ETA: 84s - loss: 4.6063 - acc: 0.02 - ETA: 83s - loss: 4.6084 - acc: 0.02 - ETA: 83s - loss: 4.6064 - acc: 0.02 - ETA: 83s - loss: 4.6079 - acc: 0.02 - ETA: 82s - loss: 4.6148 - acc: 0.02 - ETA: 82s - loss: 4.6129 - acc: 0.02 - ETA: 82s - loss: 4.6104 - acc: 0.02 - ETA: 81s - loss: 4.6116 - acc: 0.02 - ETA: 81s - loss: 4.6090 - acc: 0.02 - ETA: 81s - loss: 4.6095 - acc: 0.02 - ETA: 81s - loss: 4.6084 - acc: 0.02 - ETA: 80s - loss: 4.6078 - acc: 0.03 - ETA: 80s - loss: 4.6085 - acc: 0.03 - ETA: 80s - loss: 4.6084 - acc: 0.03 - ETA: 79s - loss: 4.6077 - acc: 0.03 - ETA: 79s - loss: 4.6076 - acc: 0.03 - ETA: 79s - loss: 4.6079 - acc: 0.03 - ETA: 79s - loss: 4.6090 - acc: 0.03 - ETA: 78s - loss: 4.6083 - acc: 0.03 - ETA: 78s - loss: 4.6066 - acc: 0.03 - ETA: 78s - loss: 4.6080 - acc: 0.03 - ETA: 77s - loss: 4.6084 - acc: 0.03 - ETA: 77s - loss: 4.6078 - acc: 0.03 - ETA: 77s - loss: 4.6080 - acc: 0.03 - ETA: 77s - loss: 4.6085 - acc: 0.03 - ETA: 76s - loss: 4.6079 - acc: 0.03 - ETA: 76s - loss: 4.6070 - acc: 0.03 - ETA: 76s - loss: 4.6030 - acc: 0.03 - ETA: 75s - loss: 4.6013 - acc: 0.03 - ETA: 75s - loss: 4.6016 - acc: 0.03 - ETA: 75s - loss: 4.6007 - acc: 0.03 - ETA: 75s - loss: 4.5991 - acc: 0.03 - ETA: 74s - loss: 4.5992 - acc: 0.03 - ETA: 74s - loss: 4.5990 - acc: 0.03 - ETA: 74s - loss: 4.6046 - acc: 0.03 - ETA: 73s - loss: 4.6044 - acc: 0.03 - ETA: 73s - loss: 4.6036 - acc: 0.03 - ETA: 73s - loss: 4.6035 - acc: 0.03 - ETA: 72s - loss: 4.6042 - acc: 0.03 - ETA: 72s - loss: 4.6066 - acc: 0.03 - ETA: 72s - loss: 4.6070 - acc: 0.03 - ETA: 72s - loss: 4.6076 - acc: 0.03 - ETA: 71s - loss: 4.6066 - acc: 0.03 - ETA: 71s - loss: 4.6065 - acc: 0.03 - ETA: 71s - loss: 4.6035 - acc: 0.03 - ETA: 70s - loss: 4.6030 - acc: 0.03 - ETA: 70s - loss: 4.6004 - acc: 0.03 - ETA: 70s - loss: 4.5995 - acc: 0.03 - ETA: 70s - loss: 4.5967 - acc: 0.03 - ETA: 69s - loss: 4.5979 - acc: 0.03 - ETA: 69s - loss: 4.5960 - acc: 0.03 - ETA: 69s - loss: 4.5988 - acc: 0.03 - ETA: 68s - loss: 4.6002 - acc: 0.03 - ETA: 68s - loss: 4.6008 - acc: 0.03 - ETA: 68s - loss: 4.6000 - acc: 0.03 - ETA: 67s - loss: 4.6002 - acc: 0.03 - ETA: 67s - loss: 4.5966 - acc: 0.03 - ETA: 67s - loss: 4.5968 - acc: 0.03 - ETA: 67s - loss: 4.5952 - acc: 0.03 - ETA: 66s - loss: 4.5960 - acc: 0.03 - ETA: 66s - loss: 4.5958 - acc: 0.03 - ETA: 66s - loss: 4.5924 - acc: 0.03 - ETA: 65s - loss: 4.5924 - acc: 0.03 - ETA: 65s - loss: 4.5933 - acc: 0.03 - ETA: 65s - loss: 4.5930 - acc: 0.03 - ETA: 65s - loss: 4.5942 - acc: 0.03 - ETA: 64s - loss: 4.5957 - acc: 0.03 - ETA: 64s - loss: 4.5945 - acc: 0.03 - ETA: 64s - loss: 4.5926 - acc: 0.03 - ETA: 63s - loss: 4.5928 - acc: 0.03 - ETA: 63s - loss: 4.5945 - acc: 0.03 - ETA: 63s - loss: 4.5952 - acc: 0.03 - ETA: 63s - loss: 4.5955 - acc: 0.03 - ETA: 62s - loss: 4.5958 - acc: 0.03 - ETA: 62s - loss: 4.5943 - acc: 0.03 - ETA: 62s - loss: 4.5936 - acc: 0.03 - ETA: 61s - loss: 4.5925 - acc: 0.03 - ETA: 61s - loss: 4.5928 - acc: 0.03 - ETA: 61s - loss: 4.5918 - acc: 0.03 - ETA: 60s - loss: 4.5913 - acc: 0.03 - ETA: 60s - loss: 4.5913 - acc: 0.03 - ETA: 60s - loss: 4.5909 - acc: 0.03 - ETA: 60s - loss: 4.5886 - acc: 0.03 - ETA: 59s - loss: 4.5903 - acc: 0.03 - ETA: 59s - loss: 4.5907 - acc: 0.03 - ETA: 59s - loss: 4.5911 - acc: 0.03 - ETA: 58s - loss: 4.5912 - acc: 0.03 - ETA: 58s - loss: 4.5929 - acc: 0.03 - ETA: 58s - loss: 4.5959 - acc: 0.03 - ETA: 58s - loss: 4.5967 - acc: 0.03 - ETA: 57s - loss: 4.5965 - acc: 0.03 - ETA: 57s - loss: 4.5965 - acc: 0.03 - ETA: 57s - loss: 4.5965 - acc: 0.03 - ETA: 56s - loss: 4.5963 - acc: 0.03 - ETA: 56s - loss: 4.5961 - acc: 0.03 - ETA: 56s - loss: 4.5938 - acc: 0.03 - ETA: 55s - loss: 4.5946 - acc: 0.03 - ETA: 55s - loss: 4.5957 - acc: 0.03 - ETA: 55s - loss: 4.5951 - acc: 0.03 - ETA: 55s - loss: 4.5966 - acc: 0.03 - ETA: 54s - loss: 4.5951 - acc: 0.03 - ETA: 54s - loss: 4.5957 - acc: 0.03 - ETA: 54s - loss: 4.5963 - acc: 0.03 - ETA: 53s - loss: 4.5955 - acc: 0.03 - ETA: 53s - loss: 4.5969 - acc: 0.03 - ETA: 53s - loss: 4.5980 - acc: 0.03 - ETA: 53s - loss: 4.5977 - acc: 0.03 - ETA: 52s - loss: 4.5974 - acc: 0.03 - ETA: 52s - loss: 4.5966 - acc: 0.03 - ETA: 52s - loss: 4.5972 - acc: 0.03 - ETA: 51s - loss: 4.5955 - acc: 0.03 - ETA: 51s - loss: 4.5951 - acc: 0.03 - ETA: 51s - loss: 4.5967 - acc: 0.03 - ETA: 50s - loss: 4.5950 - acc: 0.03 - ETA: 50s - loss: 4.5945 - acc: 0.03 - ETA: 50s - loss: 4.5955 - acc: 0.03 - ETA: 50s - loss: 4.5962 - acc: 0.03 - ETA: 49s - loss: 4.5950 - acc: 0.03 - ETA: 49s - loss: 4.5942 - acc: 0.03 - ETA: 49s - loss: 4.5937 - acc: 0.03 - ETA: 48s - loss: 4.5922 - acc: 0.03 - ETA: 48s - loss: 4.5895 - acc: 0.03 - ETA: 48s - loss: 4.5895 - acc: 0.03 - ETA: 47s - loss: 4.5896 - acc: 0.03 - ETA: 47s - loss: 4.5914 - acc: 0.03 - ETA: 47s - loss: 4.5910 - acc: 0.03 - ETA: 47s - loss: 4.5923 - acc: 0.03 - ETA: 46s - loss: 4.5909 - acc: 0.03 - ETA: 46s - loss: 4.5914 - acc: 0.03 - ETA: 46s - loss: 4.5913 - acc: 0.03 - ETA: 45s - loss: 4.5910 - acc: 0.03 - ETA: 45s - loss: 4.5913 - acc: 0.03 - ETA: 45s - loss: 4.5919 - acc: 0.03 - ETA: 44s - loss: 4.5924 - acc: 0.03 - ETA: 44s - loss: 4.5924 - acc: 0.03 - ETA: 44s - loss: 4.5921 - acc: 0.03 - ETA: 44s - loss: 4.5930 - acc: 0.03 - ETA: 43s - loss: 4.5923 - acc: 0.03 - ETA: 43s - loss: 4.5926 - acc: 0.03 - ETA: 43s - loss: 4.5929 - acc: 0.03 - ETA: 42s - loss: 4.5932 - acc: 0.03 - ETA: 42s - loss: 4.5939 - acc: 0.03 - ETA: 42s - loss: 4.5935 - acc: 0.03 - ETA: 42s - loss: 4.5939 - acc: 0.03 - ETA: 41s - loss: 4.5934 - acc: 0.03 - ETA: 41s - loss: 4.5922 - acc: 0.03 - ETA: 41s - loss: 4.5922 - acc: 0.03 - ETA: 40s - loss: 4.5938 - acc: 0.03 - ETA: 40s - loss: 4.5944 - acc: 0.03 - ETA: 40s - loss: 4.5951 - acc: 0.03 - ETA: 39s - loss: 4.5952 - acc: 0.03 - ETA: 39s - loss: 4.5951 - acc: 0.03 - ETA: 39s - loss: 4.5960 - acc: 0.03 - ETA: 39s - loss: 4.5941 - acc: 0.03 - ETA: 38s - loss: 4.5948 - acc: 0.03 - ETA: 38s - loss: 4.5959 - acc: 0.03 - ETA: 38s - loss: 4.5960 - acc: 0.03 - ETA: 37s - loss: 4.5962 - acc: 0.03 - ETA: 37s - loss: 4.5958 - acc: 0.03 - ETA: 37s - loss: 4.5963 - acc: 0.03 - ETA: 36s - loss: 4.5956 - acc: 0.03 - ETA: 36s - loss: 4.5942 - acc: 0.03 - ETA: 36s - loss: 4.5941 - acc: 0.03 - ETA: 36s - loss: 4.5935 - acc: 0.03 - ETA: 35s - loss: 4.5936 - acc: 0.03 - ETA: 35s - loss: 4.5938 - acc: 0.03 - ETA: 35s - loss: 4.5928 - acc: 0.03816660/6680 [============================>.] - ETA: 34s - loss: 4.5938 - acc: 0.03 - ETA: 34s - loss: 4.5931 - acc: 0.03 - ETA: 34s - loss: 4.5933 - acc: 0.03 - ETA: 34s - loss: 4.5943 - acc: 0.03 - ETA: 33s - loss: 4.5939 - acc: 0.03 - ETA: 33s - loss: 4.5927 - acc: 0.03 - ETA: 33s - loss: 4.5929 - acc: 0.03 - ETA: 32s - loss: 4.5936 - acc: 0.03 - ETA: 32s - loss: 4.5930 - acc: 0.03 - ETA: 32s - loss: 4.5947 - acc: 0.03 - ETA: 32s - loss: 4.5953 - acc: 0.03 - ETA: 31s - loss: 4.5949 - acc: 0.03 - ETA: 31s - loss: 4.5948 - acc: 0.03 - ETA: 31s - loss: 4.5928 - acc: 0.03 - ETA: 30s - loss: 4.5942 - acc: 0.03 - ETA: 30s - loss: 4.5953 - acc: 0.03 - ETA: 30s - loss: 4.5964 - acc: 0.03 - ETA: 29s - loss: 4.5965 - acc: 0.03 - ETA: 29s - loss: 4.5974 - acc: 0.03 - ETA: 29s - loss: 4.5986 - acc: 0.03 - ETA: 29s - loss: 4.5983 - acc: 0.03 - ETA: 28s - loss: 4.5991 - acc: 0.03 - ETA: 28s - loss: 4.5996 - acc: 0.03 - ETA: 28s - loss: 4.5999 - acc: 0.03 - ETA: 27s - loss: 4.5994 - acc: 0.03 - ETA: 27s - loss: 4.5996 - acc: 0.03 - ETA: 27s - loss: 4.5996 - acc: 0.03 - ETA: 26s - loss: 4.5995 - acc: 0.03 - ETA: 26s - loss: 4.6003 - acc: 0.03 - ETA: 26s - loss: 4.6002 - acc: 0.03 - ETA: 26s - loss: 4.5995 - acc: 0.03 - ETA: 25s - loss: 4.6005 - acc: 0.03 - ETA: 25s - loss: 4.6004 - acc: 0.03 - ETA: 25s - loss: 4.6001 - acc: 0.03 - ETA: 24s - loss: 4.5997 - acc: 0.03 - ETA: 24s - loss: 4.5991 - acc: 0.03 - ETA: 24s - loss: 4.5992 - acc: 0.03 - ETA: 24s - loss: 4.5980 - acc: 0.03 - ETA: 23s - loss: 4.5994 - acc: 0.03 - ETA: 23s - loss: 4.5996 - acc: 0.03 - ETA: 23s - loss: 4.5999 - acc: 0.03 - ETA: 22s - loss: 4.6003 - acc: 0.03 - ETA: 22s - loss: 4.5998 - acc: 0.03 - ETA: 22s - loss: 4.6009 - acc: 0.03 - ETA: 21s - loss: 4.6004 - acc: 0.03 - ETA: 21s - loss: 4.6013 - acc: 0.03 - ETA: 21s - loss: 4.6017 - acc: 0.03 - ETA: 21s - loss: 4.6021 - acc: 0.03 - ETA: 20s - loss: 4.6013 - acc: 0.03 - ETA: 20s - loss: 4.6013 - acc: 0.03 - ETA: 20s - loss: 4.6017 - acc: 0.03 - ETA: 19s - loss: 4.6010 - acc: 0.03 - ETA: 19s - loss: 4.6009 - acc: 0.03 - ETA: 19s - loss: 4.6006 - acc: 0.03 - ETA: 19s - loss: 4.5996 - acc: 0.03 - ETA: 18s - loss: 4.5997 - acc: 0.03 - ETA: 18s - loss: 4.5994 - acc: 0.03 - ETA: 18s - loss: 4.5989 - acc: 0.03 - ETA: 17s - loss: 4.5995 - acc: 0.03 - ETA: 17s - loss: 4.5984 - acc: 0.03 - ETA: 17s - loss: 4.5977 - acc: 0.03 - ETA: 16s - loss: 4.5974 - acc: 0.03 - ETA: 16s - loss: 4.5976 - acc: 0.03 - ETA: 16s - loss: 4.5984 - acc: 0.03 - ETA: 16s - loss: 4.5996 - acc: 0.03 - ETA: 15s - loss: 4.6001 - acc: 0.03 - ETA: 15s - loss: 4.6006 - acc: 0.03 - ETA: 15s - loss: 4.6009 - acc: 0.03 - ETA: 14s - loss: 4.6007 - acc: 0.03 - ETA: 14s - loss: 4.6005 - acc: 0.03 - ETA: 14s - loss: 4.5995 - acc: 0.03 - ETA: 13s - loss: 4.5998 - acc: 0.03 - ETA: 13s - loss: 4.5998 - acc: 0.03 - ETA: 13s - loss: 4.5996 - acc: 0.03 - ETA: 13s - loss: 4.5993 - acc: 0.03 - ETA: 12s - loss: 4.5987 - acc: 0.03 - ETA: 12s - loss: 4.5984 - acc: 0.03 - ETA: 12s - loss: 4.5987 - acc: 0.03 - ETA: 11s - loss: 4.5979 - acc: 0.03 - ETA: 11s - loss: 4.5985 - acc: 0.03 - ETA: 11s - loss: 4.5992 - acc: 0.03 - ETA: 10s - loss: 4.5995 - acc: 0.03 - ETA: 10s - loss: 4.5993 - acc: 0.03 - ETA: 10s - loss: 4.5991 - acc: 0.03 - ETA: 10s - loss: 4.5990 - acc: 0.03 - ETA: 9s - loss: 4.5989 - acc: 0.0377 - ETA: 9s - loss: 4.5989 - acc: 0.037 - ETA: 9s - loss: 4.5993 - acc: 0.037 - ETA: 8s - loss: 4.5993 - acc: 0.037 - ETA: 8s - loss: 4.5992 - acc: 0.037 - ETA: 8s - loss: 4.5993 - acc: 0.037 - ETA: 8s - loss: 4.5993 - acc: 0.037 - ETA: 7s - loss: 4.5991 - acc: 0.037 - ETA: 7s - loss: 4.5993 - acc: 0.037 - ETA: 7s - loss: 4.6001 - acc: 0.037 - ETA: 6s - loss: 4.6003 - acc: 0.037 - ETA: 6s - loss: 4.6005 - acc: 0.037 - ETA: 6s - loss: 4.6004 - acc: 0.037 - ETA: 5s - loss: 4.6005 - acc: 0.037 - ETA: 5s - loss: 4.6000 - acc: 0.037 - ETA: 5s - loss: 4.6010 - acc: 0.037 - ETA: 5s - loss: 4.6010 - acc: 0.037 - ETA: 4s - loss: 4.6007 - acc: 0.037 - ETA: 4s - loss: 4.6010 - acc: 0.037 - ETA: 4s - loss: 4.6006 - acc: 0.037 - ETA: 3s - loss: 4.5996 - acc: 0.036 - ETA: 3s - loss: 4.5986 - acc: 0.037 - ETA: 3s - loss: 4.5983 - acc: 0.036 - ETA: 2s - loss: 4.5987 - acc: 0.036 - ETA: 2s - loss: 4.5983 - acc: 0.036 - ETA: 2s - loss: 4.5983 - acc: 0.036 - ETA: 2s - loss: 4.5991 - acc: 0.036 - ETA: 1s - loss: 4.5987 - acc: 0.036 - ETA: 1s - loss: 4.5991 - acc: 0.036 - ETA: 1s - loss: 4.5985 - acc: 0.036 - ETA: 0s - loss: 4.6006 - acc: 0.036 - ETA: 0s - loss: 4.6001 - acc: 0.036 - ETA: 0s - loss: 4.5993 - acc: 0.0366Epoch 00010: val_loss improved from 4.69086 to 4.65504, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 101s - loss: 4.5987 - acc: 0.0367 - val_loss: 4.6550 - val_acc: 0.0311\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 96s - loss: 4.7558 - acc: 0.0000e+ - ETA: 96s - loss: 4.5920 - acc: 0.0500   - ETA: 96s - loss: 4.6193 - acc: 0.03 - ETA: 96s - loss: 4.5589 - acc: 0.03 - ETA: 96s - loss: 4.5184 - acc: 0.05 - ETA: 95s - loss: 4.5652 - acc: 0.04 - ETA: 95s - loss: 4.5583 - acc: 0.03 - ETA: 95s - loss: 4.5530 - acc: 0.03 - ETA: 95s - loss: 4.5097 - acc: 0.03 - ETA: 95s - loss: 4.5125 - acc: 0.04 - ETA: 95s - loss: 4.5482 - acc: 0.03 - ETA: 95s - loss: 4.5387 - acc: 0.03 - ETA: 94s - loss: 4.5405 - acc: 0.03 - ETA: 94s - loss: 4.5491 - acc: 0.03 - ETA: 94s - loss: 4.5510 - acc: 0.04 - ETA: 94s - loss: 4.5550 - acc: 0.03 - ETA: 93s - loss: 4.5673 - acc: 0.03 - ETA: 93s - loss: 4.5675 - acc: 0.03 - ETA: 93s - loss: 4.5775 - acc: 0.03 - ETA: 92s - loss: 4.5772 - acc: 0.03 - ETA: 92s - loss: 4.5829 - acc: 0.03 - ETA: 92s - loss: 4.5806 - acc: 0.02 - ETA: 92s - loss: 4.5803 - acc: 0.03 - ETA: 91s - loss: 4.5862 - acc: 0.02 - ETA: 91s - loss: 4.5901 - acc: 0.02 - ETA: 91s - loss: 4.5915 - acc: 0.02 - ETA: 91s - loss: 4.5886 - acc: 0.02 - ETA: 90s - loss: 4.5835 - acc: 0.02 - ETA: 90s - loss: 4.5789 - acc: 0.02 - ETA: 90s - loss: 4.5763 - acc: 0.02 - ETA: 90s - loss: 4.5832 - acc: 0.02 - ETA: 89s - loss: 4.5850 - acc: 0.02 - ETA: 89s - loss: 4.5920 - acc: 0.02 - ETA: 89s - loss: 4.5966 - acc: 0.02 - ETA: 88s - loss: 4.6038 - acc: 0.02 - ETA: 88s - loss: 4.6059 - acc: 0.02 - ETA: 88s - loss: 4.5998 - acc: 0.02 - ETA: 88s - loss: 4.5971 - acc: 0.02 - ETA: 87s - loss: 4.5988 - acc: 0.02 - ETA: 87s - loss: 4.6005 - acc: 0.02 - ETA: 87s - loss: 4.5954 - acc: 0.02 - ETA: 87s - loss: 4.5952 - acc: 0.02 - ETA: 86s - loss: 4.5939 - acc: 0.02 - ETA: 86s - loss: 4.5953 - acc: 0.02 - ETA: 86s - loss: 4.5950 - acc: 0.02 - ETA: 85s - loss: 4.5925 - acc: 0.02 - ETA: 85s - loss: 4.5987 - acc: 0.02 - ETA: 85s - loss: 4.5988 - acc: 0.02 - ETA: 85s - loss: 4.5966 - acc: 0.02 - ETA: 84s - loss: 4.5903 - acc: 0.03 - ETA: 84s - loss: 4.5867 - acc: 0.03 - ETA: 84s - loss: 4.5892 - acc: 0.03 - ETA: 83s - loss: 4.5901 - acc: 0.03 - ETA: 83s - loss: 4.5908 - acc: 0.03 - ETA: 83s - loss: 4.5883 - acc: 0.03 - ETA: 82s - loss: 4.5857 - acc: 0.03 - ETA: 82s - loss: 4.5858 - acc: 0.03 - ETA: 82s - loss: 4.5858 - acc: 0.03 - ETA: 82s - loss: 4.5926 - acc: 0.03 - ETA: 81s - loss: 4.5968 - acc: 0.03 - ETA: 81s - loss: 4.5953 - acc: 0.03 - ETA: 81s - loss: 4.5940 - acc: 0.03 - ETA: 80s - loss: 4.5947 - acc: 0.03 - ETA: 80s - loss: 4.5946 - acc: 0.03 - ETA: 80s - loss: 4.5939 - acc: 0.03 - ETA: 80s - loss: 4.5928 - acc: 0.03 - ETA: 79s - loss: 4.5969 - acc: 0.03 - ETA: 79s - loss: 4.5984 - acc: 0.03 - ETA: 79s - loss: 4.5959 - acc: 0.03 - ETA: 78s - loss: 4.5921 - acc: 0.03 - ETA: 78s - loss: 4.5917 - acc: 0.03 - ETA: 78s - loss: 4.5918 - acc: 0.03 - ETA: 78s - loss: 4.5906 - acc: 0.03 - ETA: 77s - loss: 4.5914 - acc: 0.03 - ETA: 77s - loss: 4.5930 - acc: 0.03 - ETA: 77s - loss: 4.5894 - acc: 0.03 - ETA: 76s - loss: 4.5888 - acc: 0.03 - ETA: 76s - loss: 4.5895 - acc: 0.03 - ETA: 76s - loss: 4.5867 - acc: 0.03 - ETA: 75s - loss: 4.5872 - acc: 0.03 - ETA: 75s - loss: 4.5890 - acc: 0.03 - ETA: 75s - loss: 4.5880 - acc: 0.03 - ETA: 75s - loss: 4.5835 - acc: 0.03 - ETA: 74s - loss: 4.5837 - acc: 0.03 - ETA: 74s - loss: 4.5813 - acc: 0.03 - ETA: 74s - loss: 4.5860 - acc: 0.03 - ETA: 73s - loss: 4.5874 - acc: 0.03 - ETA: 73s - loss: 4.5867 - acc: 0.03 - ETA: 73s - loss: 4.5859 - acc: 0.03 - ETA: 73s - loss: 4.5855 - acc: 0.03 - ETA: 72s - loss: 4.5849 - acc: 0.03 - ETA: 72s - loss: 4.5872 - acc: 0.03 - ETA: 72s - loss: 4.5873 - acc: 0.03 - ETA: 71s - loss: 4.5893 - acc: 0.03 - ETA: 71s - loss: 4.5870 - acc: 0.03 - ETA: 71s - loss: 4.5872 - acc: 0.03 - ETA: 70s - loss: 4.5855 - acc: 0.03 - ETA: 70s - loss: 4.5811 - acc: 0.03 - ETA: 70s - loss: 4.5792 - acc: 0.03 - ETA: 70s - loss: 4.5797 - acc: 0.03 - ETA: 69s - loss: 4.5816 - acc: 0.03 - ETA: 69s - loss: 4.5786 - acc: 0.03 - ETA: 69s - loss: 4.5826 - acc: 0.03 - ETA: 68s - loss: 4.5830 - acc: 0.03 - ETA: 68s - loss: 4.5846 - acc: 0.03 - ETA: 68s - loss: 4.5844 - acc: 0.03 - ETA: 67s - loss: 4.5808 - acc: 0.03 - ETA: 67s - loss: 4.5806 - acc: 0.03 - ETA: 67s - loss: 4.5827 - acc: 0.03 - ETA: 67s - loss: 4.5806 - acc: 0.03 - ETA: 66s - loss: 4.5793 - acc: 0.03 - ETA: 66s - loss: 4.5786 - acc: 0.03 - ETA: 66s - loss: 4.5772 - acc: 0.04 - ETA: 65s - loss: 4.5752 - acc: 0.04 - ETA: 65s - loss: 4.5738 - acc: 0.04 - ETA: 65s - loss: 4.5726 - acc: 0.04 - ETA: 64s - loss: 4.5768 - acc: 0.04 - ETA: 64s - loss: 4.5766 - acc: 0.04 - ETA: 64s - loss: 4.5773 - acc: 0.04 - ETA: 64s - loss: 4.5762 - acc: 0.04 - ETA: 63s - loss: 4.5740 - acc: 0.04 - ETA: 63s - loss: 4.5739 - acc: 0.04 - ETA: 63s - loss: 4.5724 - acc: 0.04 - ETA: 62s - loss: 4.5736 - acc: 0.04 - ETA: 62s - loss: 4.5738 - acc: 0.04 - ETA: 62s - loss: 4.5750 - acc: 0.04 - ETA: 62s - loss: 4.5737 - acc: 0.04 - ETA: 61s - loss: 4.5730 - acc: 0.04 - ETA: 61s - loss: 4.5732 - acc: 0.04 - ETA: 61s - loss: 4.5733 - acc: 0.04 - ETA: 60s - loss: 4.5753 - acc: 0.04 - ETA: 60s - loss: 4.5770 - acc: 0.04 - ETA: 60s - loss: 4.5751 - acc: 0.04 - ETA: 59s - loss: 4.5757 - acc: 0.04 - ETA: 59s - loss: 4.5758 - acc: 0.04 - ETA: 59s - loss: 4.5778 - acc: 0.04 - ETA: 59s - loss: 4.5790 - acc: 0.04 - ETA: 58s - loss: 4.5793 - acc: 0.04 - ETA: 58s - loss: 4.5788 - acc: 0.04 - ETA: 58s - loss: 4.5786 - acc: 0.04 - ETA: 57s - loss: 4.5778 - acc: 0.04 - ETA: 57s - loss: 4.5755 - acc: 0.04 - ETA: 57s - loss: 4.5756 - acc: 0.04 - ETA: 56s - loss: 4.5747 - acc: 0.04 - ETA: 56s - loss: 4.5746 - acc: 0.04 - ETA: 56s - loss: 4.5727 - acc: 0.04 - ETA: 56s - loss: 4.5742 - acc: 0.04 - ETA: 55s - loss: 4.5762 - acc: 0.04 - ETA: 55s - loss: 4.5760 - acc: 0.04 - ETA: 55s - loss: 4.5755 - acc: 0.04 - ETA: 54s - loss: 4.5754 - acc: 0.04 - ETA: 54s - loss: 4.5765 - acc: 0.04 - ETA: 54s - loss: 4.5765 - acc: 0.04 - ETA: 53s - loss: 4.5756 - acc: 0.04 - ETA: 53s - loss: 4.5763 - acc: 0.04 - ETA: 53s - loss: 4.5766 - acc: 0.04 - ETA: 53s - loss: 4.5775 - acc: 0.04 - ETA: 52s - loss: 4.5776 - acc: 0.04 - ETA: 52s - loss: 4.5774 - acc: 0.04 - ETA: 52s - loss: 4.5774 - acc: 0.04 - ETA: 51s - loss: 4.5767 - acc: 0.04 - ETA: 51s - loss: 4.5775 - acc: 0.04 - ETA: 51s - loss: 4.5783 - acc: 0.04 - ETA: 50s - loss: 4.5785 - acc: 0.04 - ETA: 50s - loss: 4.5790 - acc: 0.04 - ETA: 50s - loss: 4.5807 - acc: 0.04 - ETA: 50s - loss: 4.5805 - acc: 0.04 - ETA: 49s - loss: 4.5816 - acc: 0.04 - ETA: 49s - loss: 4.5824 - acc: 0.04 - ETA: 49s - loss: 4.5820 - acc: 0.04 - ETA: 48s - loss: 4.5819 - acc: 0.04 - ETA: 48s - loss: 4.5814 - acc: 0.04 - ETA: 48s - loss: 4.5800 - acc: 0.04 - ETA: 47s - loss: 4.5795 - acc: 0.04 - ETA: 47s - loss: 4.5796 - acc: 0.04 - ETA: 47s - loss: 4.5804 - acc: 0.04 - ETA: 47s - loss: 4.5814 - acc: 0.04 - ETA: 46s - loss: 4.5814 - acc: 0.04 - ETA: 46s - loss: 4.5800 - acc: 0.04 - ETA: 46s - loss: 4.5802 - acc: 0.04 - ETA: 45s - loss: 4.5797 - acc: 0.04 - ETA: 45s - loss: 4.5789 - acc: 0.04 - ETA: 45s - loss: 4.5788 - acc: 0.04 - ETA: 44s - loss: 4.5794 - acc: 0.04 - ETA: 44s - loss: 4.5793 - acc: 0.04 - ETA: 44s - loss: 4.5817 - acc: 0.04 - ETA: 44s - loss: 4.5817 - acc: 0.04 - ETA: 43s - loss: 4.5821 - acc: 0.04 - ETA: 43s - loss: 4.5852 - acc: 0.04 - ETA: 43s - loss: 4.5847 - acc: 0.04 - ETA: 42s - loss: 4.5846 - acc: 0.04 - ETA: 42s - loss: 4.5842 - acc: 0.04 - ETA: 42s - loss: 4.5857 - acc: 0.04 - ETA: 42s - loss: 4.5851 - acc: 0.04 - ETA: 41s - loss: 4.5845 - acc: 0.04 - ETA: 41s - loss: 4.5838 - acc: 0.04 - ETA: 41s - loss: 4.5824 - acc: 0.04 - ETA: 40s - loss: 4.5819 - acc: 0.04 - ETA: 40s - loss: 4.5828 - acc: 0.04 - ETA: 40s - loss: 4.5862 - acc: 0.04 - ETA: 39s - loss: 4.5871 - acc: 0.04 - ETA: 39s - loss: 4.5859 - acc: 0.04 - ETA: 39s - loss: 4.5856 - acc: 0.04 - ETA: 39s - loss: 4.5848 - acc: 0.04 - ETA: 38s - loss: 4.5843 - acc: 0.04 - ETA: 38s - loss: 4.5852 - acc: 0.04 - ETA: 38s - loss: 4.5848 - acc: 0.04 - ETA: 37s - loss: 4.5849 - acc: 0.04 - ETA: 37s - loss: 4.5842 - acc: 0.04 - ETA: 37s - loss: 4.5839 - acc: 0.04 - ETA: 36s - loss: 4.5834 - acc: 0.04 - ETA: 36s - loss: 4.5826 - acc: 0.04 - ETA: 36s - loss: 4.5821 - acc: 0.04 - ETA: 36s - loss: 4.5811 - acc: 0.04 - ETA: 35s - loss: 4.5805 - acc: 0.0412"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 35s - loss: 4.5803 - acc: 0.04 - ETA: 35s - loss: 4.5802 - acc: 0.04 - ETA: 34s - loss: 4.5800 - acc: 0.04 - ETA: 34s - loss: 4.5806 - acc: 0.04 - ETA: 34s - loss: 4.5807 - acc: 0.04 - ETA: 33s - loss: 4.5810 - acc: 0.04 - ETA: 33s - loss: 4.5811 - acc: 0.04 - ETA: 33s - loss: 4.5825 - acc: 0.04 - ETA: 33s - loss: 4.5808 - acc: 0.04 - ETA: 32s - loss: 4.5812 - acc: 0.04 - ETA: 32s - loss: 4.5814 - acc: 0.04 - ETA: 32s - loss: 4.5821 - acc: 0.04 - ETA: 31s - loss: 4.5816 - acc: 0.04 - ETA: 31s - loss: 4.5810 - acc: 0.04 - ETA: 31s - loss: 4.5816 - acc: 0.04 - ETA: 30s - loss: 4.5805 - acc: 0.04 - ETA: 30s - loss: 4.5809 - acc: 0.04 - ETA: 30s - loss: 4.5807 - acc: 0.04 - ETA: 30s - loss: 4.5820 - acc: 0.04 - ETA: 29s - loss: 4.5823 - acc: 0.04 - ETA: 29s - loss: 4.5817 - acc: 0.04 - ETA: 29s - loss: 4.5807 - acc: 0.04 - ETA: 28s - loss: 4.5816 - acc: 0.04 - ETA: 28s - loss: 4.5819 - acc: 0.04 - ETA: 28s - loss: 4.5816 - acc: 0.04 - ETA: 27s - loss: 4.5811 - acc: 0.04 - ETA: 27s - loss: 4.5811 - acc: 0.04 - ETA: 27s - loss: 4.5817 - acc: 0.04 - ETA: 27s - loss: 4.5810 - acc: 0.04 - ETA: 26s - loss: 4.5824 - acc: 0.04 - ETA: 26s - loss: 4.5814 - acc: 0.04 - ETA: 26s - loss: 4.5814 - acc: 0.04 - ETA: 25s - loss: 4.5814 - acc: 0.04 - ETA: 25s - loss: 4.5828 - acc: 0.04 - ETA: 25s - loss: 4.5830 - acc: 0.04 - ETA: 24s - loss: 4.5830 - acc: 0.04 - ETA: 24s - loss: 4.5841 - acc: 0.04 - ETA: 24s - loss: 4.5837 - acc: 0.04 - ETA: 24s - loss: 4.5828 - acc: 0.04 - ETA: 23s - loss: 4.5829 - acc: 0.04 - ETA: 23s - loss: 4.5832 - acc: 0.04 - ETA: 23s - loss: 4.5834 - acc: 0.04 - ETA: 22s - loss: 4.5827 - acc: 0.04 - ETA: 22s - loss: 4.5829 - acc: 0.04 - ETA: 22s - loss: 4.5830 - acc: 0.04 - ETA: 21s - loss: 4.5835 - acc: 0.04 - ETA: 21s - loss: 4.5830 - acc: 0.04 - ETA: 21s - loss: 4.5834 - acc: 0.04 - ETA: 21s - loss: 4.5827 - acc: 0.04 - ETA: 20s - loss: 4.5831 - acc: 0.04 - ETA: 20s - loss: 4.5825 - acc: 0.04 - ETA: 20s - loss: 4.5827 - acc: 0.04 - ETA: 19s - loss: 4.5814 - acc: 0.04 - ETA: 19s - loss: 4.5816 - acc: 0.04 - ETA: 19s - loss: 4.5814 - acc: 0.04 - ETA: 18s - loss: 4.5814 - acc: 0.04 - ETA: 18s - loss: 4.5807 - acc: 0.04 - ETA: 18s - loss: 4.5807 - acc: 0.04 - ETA: 18s - loss: 4.5808 - acc: 0.04 - ETA: 17s - loss: 4.5805 - acc: 0.04 - ETA: 17s - loss: 4.5803 - acc: 0.04 - ETA: 17s - loss: 4.5807 - acc: 0.04 - ETA: 16s - loss: 4.5816 - acc: 0.04 - ETA: 16s - loss: 4.5817 - acc: 0.04 - ETA: 16s - loss: 4.5808 - acc: 0.04 - ETA: 15s - loss: 4.5808 - acc: 0.04 - ETA: 15s - loss: 4.5800 - acc: 0.04 - ETA: 15s - loss: 4.5800 - acc: 0.04 - ETA: 15s - loss: 4.5794 - acc: 0.04 - ETA: 14s - loss: 4.5784 - acc: 0.04 - ETA: 14s - loss: 4.5778 - acc: 0.04 - ETA: 14s - loss: 4.5770 - acc: 0.04 - ETA: 13s - loss: 4.5767 - acc: 0.04 - ETA: 13s - loss: 4.5766 - acc: 0.04 - ETA: 13s - loss: 4.5766 - acc: 0.04 - ETA: 12s - loss: 4.5766 - acc: 0.04 - ETA: 12s - loss: 4.5772 - acc: 0.04 - ETA: 12s - loss: 4.5766 - acc: 0.04 - ETA: 12s - loss: 4.5768 - acc: 0.04 - ETA: 11s - loss: 4.5761 - acc: 0.04 - ETA: 11s - loss: 4.5760 - acc: 0.04 - ETA: 11s - loss: 4.5754 - acc: 0.04 - ETA: 10s - loss: 4.5760 - acc: 0.04 - ETA: 10s - loss: 4.5753 - acc: 0.04 - ETA: 10s - loss: 4.5752 - acc: 0.04 - ETA: 9s - loss: 4.5746 - acc: 0.0425 - ETA: 9s - loss: 4.5741 - acc: 0.042 - ETA: 9s - loss: 4.5732 - acc: 0.043 - ETA: 9s - loss: 4.5732 - acc: 0.043 - ETA: 8s - loss: 4.5725 - acc: 0.043 - ETA: 8s - loss: 4.5723 - acc: 0.043 - ETA: 8s - loss: 4.5718 - acc: 0.043 - ETA: 7s - loss: 4.5721 - acc: 0.042 - ETA: 7s - loss: 4.5723 - acc: 0.043 - ETA: 7s - loss: 4.5720 - acc: 0.043 - ETA: 6s - loss: 4.5717 - acc: 0.043 - ETA: 6s - loss: 4.5718 - acc: 0.043 - ETA: 6s - loss: 4.5727 - acc: 0.043 - ETA: 6s - loss: 4.5727 - acc: 0.043 - ETA: 5s - loss: 4.5730 - acc: 0.043 - ETA: 5s - loss: 4.5726 - acc: 0.043 - ETA: 5s - loss: 4.5724 - acc: 0.043 - ETA: 4s - loss: 4.5730 - acc: 0.043 - ETA: 4s - loss: 4.5743 - acc: 0.042 - ETA: 4s - loss: 4.5742 - acc: 0.043 - ETA: 3s - loss: 4.5738 - acc: 0.043 - ETA: 3s - loss: 4.5740 - acc: 0.043 - ETA: 3s - loss: 4.5745 - acc: 0.043 - ETA: 3s - loss: 4.5757 - acc: 0.043 - ETA: 2s - loss: 4.5760 - acc: 0.042 - ETA: 2s - loss: 4.5759 - acc: 0.042 - ETA: 2s - loss: 4.5753 - acc: 0.043 - ETA: 1s - loss: 4.5752 - acc: 0.043 - ETA: 1s - loss: 4.5772 - acc: 0.042 - ETA: 1s - loss: 4.5773 - acc: 0.042 - ETA: 0s - loss: 4.5774 - acc: 0.042 - ETA: 0s - loss: 4.5777 - acc: 0.042 - ETA: 0s - loss: 4.5778 - acc: 0.0425Epoch 00011: val_loss improved from 4.65504 to 4.64153, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 102s - loss: 4.5776 - acc: 0.0424 - val_loss: 4.6415 - val_acc: 0.0311\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 97s - loss: 4.6917 - acc: 0.0000e+ - ETA: 98s - loss: 4.6142 - acc: 0.0250   - ETA: 99s - loss: 4.5765 - acc: 0.05 - ETA: 98s - loss: 4.5861 - acc: 0.05 - ETA: 98s - loss: 4.5742 - acc: 0.05 - ETA: 98s - loss: 4.6205 - acc: 0.04 - ETA: 97s - loss: 4.6098 - acc: 0.04 - ETA: 97s - loss: 4.6471 - acc: 0.03 - ETA: 97s - loss: 4.6756 - acc: 0.04 - ETA: 97s - loss: 4.6699 - acc: 0.04 - ETA: 96s - loss: 4.6876 - acc: 0.04 - ETA: 96s - loss: 4.6739 - acc: 0.04 - ETA: 96s - loss: 4.6534 - acc: 0.04 - ETA: 96s - loss: 4.6481 - acc: 0.04 - ETA: 95s - loss: 4.6514 - acc: 0.04 - ETA: 95s - loss: 4.6541 - acc: 0.03 - ETA: 95s - loss: 4.6514 - acc: 0.03 - ETA: 94s - loss: 4.6407 - acc: 0.03 - ETA: 94s - loss: 4.6344 - acc: 0.03 - ETA: 94s - loss: 4.6349 - acc: 0.03 - ETA: 94s - loss: 4.6337 - acc: 0.03 - ETA: 93s - loss: 4.6286 - acc: 0.03 - ETA: 93s - loss: 4.6228 - acc: 0.03 - ETA: 93s - loss: 4.6164 - acc: 0.03 - ETA: 92s - loss: 4.6223 - acc: 0.03 - ETA: 92s - loss: 4.6186 - acc: 0.04 - ETA: 92s - loss: 4.6176 - acc: 0.04 - ETA: 91s - loss: 4.6269 - acc: 0.03 - ETA: 91s - loss: 4.6275 - acc: 0.03 - ETA: 91s - loss: 4.6149 - acc: 0.04 - ETA: 91s - loss: 4.6111 - acc: 0.04 - ETA: 90s - loss: 4.6245 - acc: 0.03 - ETA: 90s - loss: 4.6255 - acc: 0.03 - ETA: 90s - loss: 4.6294 - acc: 0.03 - ETA: 90s - loss: 4.6337 - acc: 0.03 - ETA: 89s - loss: 4.6286 - acc: 0.03 - ETA: 89s - loss: 4.6285 - acc: 0.03 - ETA: 89s - loss: 4.6227 - acc: 0.03 - ETA: 88s - loss: 4.6187 - acc: 0.03 - ETA: 88s - loss: 4.6170 - acc: 0.03 - ETA: 88s - loss: 4.6140 - acc: 0.03 - ETA: 87s - loss: 4.6147 - acc: 0.03 - ETA: 87s - loss: 4.6141 - acc: 0.03 - ETA: 87s - loss: 4.6082 - acc: 0.03 - ETA: 87s - loss: 4.6034 - acc: 0.03 - ETA: 86s - loss: 4.6027 - acc: 0.03 - ETA: 86s - loss: 4.6013 - acc: 0.03 - ETA: 86s - loss: 4.6013 - acc: 0.03 - ETA: 85s - loss: 4.6049 - acc: 0.03 - ETA: 85s - loss: 4.6011 - acc: 0.03 - ETA: 85s - loss: 4.6012 - acc: 0.03 - ETA: 84s - loss: 4.6008 - acc: 0.03 - ETA: 84s - loss: 4.5997 - acc: 0.03 - ETA: 84s - loss: 4.5956 - acc: 0.04 - ETA: 83s - loss: 4.5954 - acc: 0.04 - ETA: 83s - loss: 4.5978 - acc: 0.04 - ETA: 83s - loss: 4.5920 - acc: 0.04 - ETA: 83s - loss: 4.5862 - acc: 0.04 - ETA: 82s - loss: 4.5822 - acc: 0.04 - ETA: 82s - loss: 4.5855 - acc: 0.04 - ETA: 82s - loss: 4.5879 - acc: 0.04 - ETA: 81s - loss: 4.5844 - acc: 0.04 - ETA: 81s - loss: 4.5837 - acc: 0.04 - ETA: 81s - loss: 4.5883 - acc: 0.04 - ETA: 80s - loss: 4.5814 - acc: 0.04 - ETA: 80s - loss: 4.5783 - acc: 0.04 - ETA: 80s - loss: 4.5779 - acc: 0.04 - ETA: 80s - loss: 4.5836 - acc: 0.04 - ETA: 79s - loss: 4.5842 - acc: 0.04 - ETA: 79s - loss: 4.5870 - acc: 0.04 - ETA: 79s - loss: 4.5843 - acc: 0.04 - ETA: 78s - loss: 4.5887 - acc: 0.04 - ETA: 78s - loss: 4.5947 - acc: 0.04 - ETA: 78s - loss: 4.5942 - acc: 0.04 - ETA: 77s - loss: 4.5934 - acc: 0.04 - ETA: 77s - loss: 4.5879 - acc: 0.04 - ETA: 77s - loss: 4.5883 - acc: 0.04 - ETA: 77s - loss: 4.5876 - acc: 0.04 - ETA: 76s - loss: 4.5859 - acc: 0.03 - ETA: 76s - loss: 4.5830 - acc: 0.03 - ETA: 76s - loss: 4.5820 - acc: 0.03 - ETA: 75s - loss: 4.5831 - acc: 0.03 - ETA: 75s - loss: 4.5824 - acc: 0.03 - ETA: 75s - loss: 4.5822 - acc: 0.03 - ETA: 74s - loss: 4.5802 - acc: 0.03 - ETA: 74s - loss: 4.5797 - acc: 0.03 - ETA: 74s - loss: 4.5811 - acc: 0.03 - ETA: 73s - loss: 4.5806 - acc: 0.03 - ETA: 73s - loss: 4.5776 - acc: 0.03 - ETA: 73s - loss: 4.5783 - acc: 0.04 - ETA: 72s - loss: 4.5793 - acc: 0.04 - ETA: 72s - loss: 4.5787 - acc: 0.04 - ETA: 72s - loss: 4.5784 - acc: 0.04 - ETA: 72s - loss: 4.5774 - acc: 0.04 - ETA: 71s - loss: 4.5738 - acc: 0.04 - ETA: 71s - loss: 4.5747 - acc: 0.04 - ETA: 71s - loss: 4.5777 - acc: 0.04 - ETA: 70s - loss: 4.5777 - acc: 0.04 - ETA: 70s - loss: 4.5773 - acc: 0.04 - ETA: 70s - loss: 4.5789 - acc: 0.04 - ETA: 69s - loss: 4.5828 - acc: 0.04 - ETA: 69s - loss: 4.5825 - acc: 0.04 - ETA: 69s - loss: 4.5817 - acc: 0.04 - ETA: 68s - loss: 4.5820 - acc: 0.04 - ETA: 68s - loss: 4.5795 - acc: 0.04 - ETA: 68s - loss: 4.5804 - acc: 0.04 - ETA: 67s - loss: 4.5800 - acc: 0.04 - ETA: 67s - loss: 4.5783 - acc: 0.04 - ETA: 67s - loss: 4.5753 - acc: 0.04 - ETA: 66s - loss: 4.5744 - acc: 0.04 - ETA: 66s - loss: 4.5746 - acc: 0.04 - ETA: 66s - loss: 4.5768 - acc: 0.04 - ETA: 66s - loss: 4.5790 - acc: 0.04 - ETA: 65s - loss: 4.5794 - acc: 0.04 - ETA: 65s - loss: 4.5809 - acc: 0.04 - ETA: 65s - loss: 4.5786 - acc: 0.04 - ETA: 64s - loss: 4.5779 - acc: 0.04 - ETA: 64s - loss: 4.5768 - acc: 0.04 - ETA: 64s - loss: 4.5783 - acc: 0.04 - ETA: 63s - loss: 4.5770 - acc: 0.04 - ETA: 63s - loss: 4.5768 - acc: 0.04 - ETA: 63s - loss: 4.5766 - acc: 0.04 - ETA: 62s - loss: 4.5769 - acc: 0.04 - ETA: 62s - loss: 4.5771 - acc: 0.04 - ETA: 62s - loss: 4.5737 - acc: 0.04 - ETA: 62s - loss: 4.5751 - acc: 0.04 - ETA: 61s - loss: 4.5764 - acc: 0.04 - ETA: 61s - loss: 4.5766 - acc: 0.04 - ETA: 61s - loss: 4.5792 - acc: 0.04 - ETA: 60s - loss: 4.5790 - acc: 0.04 - ETA: 60s - loss: 4.5790 - acc: 0.04 - ETA: 60s - loss: 4.5776 - acc: 0.04 - ETA: 59s - loss: 4.5768 - acc: 0.04 - ETA: 59s - loss: 4.5758 - acc: 0.04 - ETA: 59s - loss: 4.5747 - acc: 0.04 - ETA: 59s - loss: 4.5746 - acc: 0.04 - ETA: 58s - loss: 4.5749 - acc: 0.04 - ETA: 58s - loss: 4.5754 - acc: 0.04 - ETA: 58s - loss: 4.5754 - acc: 0.04 - ETA: 57s - loss: 4.5785 - acc: 0.04 - ETA: 57s - loss: 4.5773 - acc: 0.04 - ETA: 57s - loss: 4.5775 - acc: 0.04 - ETA: 56s - loss: 4.5770 - acc: 0.04 - ETA: 56s - loss: 4.5773 - acc: 0.04 - ETA: 56s - loss: 4.5765 - acc: 0.04 - ETA: 55s - loss: 4.5744 - acc: 0.04 - ETA: 55s - loss: 4.5720 - acc: 0.04 - ETA: 55s - loss: 4.5734 - acc: 0.04 - ETA: 55s - loss: 4.5736 - acc: 0.04 - ETA: 54s - loss: 4.5738 - acc: 0.04 - ETA: 54s - loss: 4.5722 - acc: 0.04 - ETA: 54s - loss: 4.5718 - acc: 0.04 - ETA: 53s - loss: 4.5716 - acc: 0.04 - ETA: 53s - loss: 4.5715 - acc: 0.04 - ETA: 53s - loss: 4.5724 - acc: 0.04 - ETA: 52s - loss: 4.5718 - acc: 0.04 - ETA: 52s - loss: 4.5705 - acc: 0.04 - ETA: 52s - loss: 4.5708 - acc: 0.04 - ETA: 52s - loss: 4.5695 - acc: 0.04 - ETA: 51s - loss: 4.5681 - acc: 0.04 - ETA: 51s - loss: 4.5660 - acc: 0.04 - ETA: 51s - loss: 4.5655 - acc: 0.04 - ETA: 50s - loss: 4.5652 - acc: 0.04 - ETA: 50s - loss: 4.5649 - acc: 0.04 - ETA: 50s - loss: 4.5665 - acc: 0.04 - ETA: 50s - loss: 4.5681 - acc: 0.04 - ETA: 49s - loss: 4.5684 - acc: 0.04 - ETA: 49s - loss: 4.5661 - acc: 0.04 - ETA: 49s - loss: 4.5686 - acc: 0.04 - ETA: 48s - loss: 4.5697 - acc: 0.04 - ETA: 48s - loss: 4.5716 - acc: 0.04 - ETA: 48s - loss: 4.5719 - acc: 0.04 - ETA: 47s - loss: 4.5742 - acc: 0.04 - ETA: 47s - loss: 4.5749 - acc: 0.04 - ETA: 47s - loss: 4.5737 - acc: 0.04 - ETA: 47s - loss: 4.5726 - acc: 0.04 - ETA: 46s - loss: 4.5716 - acc: 0.04 - ETA: 46s - loss: 4.5706 - acc: 0.04 - ETA: 46s - loss: 4.5700 - acc: 0.04 - ETA: 45s - loss: 4.5711 - acc: 0.04 - ETA: 45s - loss: 4.5698 - acc: 0.04 - ETA: 45s - loss: 4.5703 - acc: 0.04 - ETA: 45s - loss: 4.5707 - acc: 0.04 - ETA: 44s - loss: 4.5679 - acc: 0.04 - ETA: 44s - loss: 4.5682 - acc: 0.04 - ETA: 44s - loss: 4.5670 - acc: 0.04 - ETA: 43s - loss: 4.5687 - acc: 0.04 - ETA: 43s - loss: 4.5686 - acc: 0.04 - ETA: 43s - loss: 4.5667 - acc: 0.04 - ETA: 42s - loss: 4.5670 - acc: 0.04 - ETA: 42s - loss: 4.5663 - acc: 0.04 - ETA: 42s - loss: 4.5651 - acc: 0.04 - ETA: 42s - loss: 4.5649 - acc: 0.04 - ETA: 41s - loss: 4.5638 - acc: 0.04 - ETA: 41s - loss: 4.5630 - acc: 0.04 - ETA: 41s - loss: 4.5607 - acc: 0.04 - ETA: 40s - loss: 4.5621 - acc: 0.04 - ETA: 40s - loss: 4.5630 - acc: 0.04 - ETA: 40s - loss: 4.5633 - acc: 0.04 - ETA: 39s - loss: 4.5631 - acc: 0.04 - ETA: 39s - loss: 4.5637 - acc: 0.04 - ETA: 39s - loss: 4.5623 - acc: 0.04 - ETA: 39s - loss: 4.5625 - acc: 0.04 - ETA: 38s - loss: 4.5638 - acc: 0.04 - ETA: 38s - loss: 4.5643 - acc: 0.04 - ETA: 38s - loss: 4.5650 - acc: 0.04 - ETA: 37s - loss: 4.5650 - acc: 0.04 - ETA: 37s - loss: 4.5640 - acc: 0.04 - ETA: 37s - loss: 4.5639 - acc: 0.04 - ETA: 37s - loss: 4.5628 - acc: 0.04 - ETA: 36s - loss: 4.5621 - acc: 0.04 - ETA: 36s - loss: 4.5612 - acc: 0.04 - ETA: 36s - loss: 4.5621 - acc: 0.04 - ETA: 35s - loss: 4.5632 - acc: 0.04 - ETA: 35s - loss: 4.5628 - acc: 0.04446660/6680 [============================>.] - ETA: 35s - loss: 4.5626 - acc: 0.04 - ETA: 34s - loss: 4.5635 - acc: 0.04 - ETA: 34s - loss: 4.5636 - acc: 0.04 - ETA: 34s - loss: 4.5634 - acc: 0.04 - ETA: 34s - loss: 4.5632 - acc: 0.04 - ETA: 33s - loss: 4.5639 - acc: 0.04 - ETA: 33s - loss: 4.5637 - acc: 0.04 - ETA: 33s - loss: 4.5641 - acc: 0.04 - ETA: 32s - loss: 4.5639 - acc: 0.04 - ETA: 32s - loss: 4.5634 - acc: 0.04 - ETA: 32s - loss: 4.5619 - acc: 0.04 - ETA: 31s - loss: 4.5600 - acc: 0.04 - ETA: 31s - loss: 4.5594 - acc: 0.04 - ETA: 31s - loss: 4.5599 - acc: 0.04 - ETA: 31s - loss: 4.5615 - acc: 0.04 - ETA: 30s - loss: 4.5608 - acc: 0.04 - ETA: 30s - loss: 4.5605 - acc: 0.04 - ETA: 30s - loss: 4.5594 - acc: 0.04 - ETA: 29s - loss: 4.5603 - acc: 0.04 - ETA: 29s - loss: 4.5616 - acc: 0.04 - ETA: 29s - loss: 4.5619 - acc: 0.04 - ETA: 28s - loss: 4.5622 - acc: 0.04 - ETA: 28s - loss: 4.5619 - acc: 0.04 - ETA: 28s - loss: 4.5622 - acc: 0.04 - ETA: 28s - loss: 4.5614 - acc: 0.04 - ETA: 27s - loss: 4.5622 - acc: 0.04 - ETA: 27s - loss: 4.5638 - acc: 0.04 - ETA: 27s - loss: 4.5638 - acc: 0.04 - ETA: 26s - loss: 4.5637 - acc: 0.04 - ETA: 26s - loss: 4.5642 - acc: 0.04 - ETA: 26s - loss: 4.5654 - acc: 0.04 - ETA: 26s - loss: 4.5656 - acc: 0.04 - ETA: 25s - loss: 4.5649 - acc: 0.04 - ETA: 25s - loss: 4.5646 - acc: 0.04 - ETA: 25s - loss: 4.5648 - acc: 0.04 - ETA: 24s - loss: 4.5645 - acc: 0.04 - ETA: 24s - loss: 4.5643 - acc: 0.04 - ETA: 24s - loss: 4.5638 - acc: 0.04 - ETA: 23s - loss: 4.5631 - acc: 0.04 - ETA: 23s - loss: 4.5630 - acc: 0.04 - ETA: 23s - loss: 4.5634 - acc: 0.04 - ETA: 23s - loss: 4.5642 - acc: 0.04 - ETA: 22s - loss: 4.5631 - acc: 0.04 - ETA: 22s - loss: 4.5623 - acc: 0.04 - ETA: 22s - loss: 4.5624 - acc: 0.04 - ETA: 21s - loss: 4.5624 - acc: 0.04 - ETA: 21s - loss: 4.5618 - acc: 0.04 - ETA: 21s - loss: 4.5617 - acc: 0.04 - ETA: 20s - loss: 4.5620 - acc: 0.04 - ETA: 20s - loss: 4.5614 - acc: 0.04 - ETA: 20s - loss: 4.5631 - acc: 0.04 - ETA: 20s - loss: 4.5633 - acc: 0.04 - ETA: 19s - loss: 4.5624 - acc: 0.04 - ETA: 19s - loss: 4.5626 - acc: 0.04 - ETA: 19s - loss: 4.5628 - acc: 0.04 - ETA: 18s - loss: 4.5638 - acc: 0.04 - ETA: 18s - loss: 4.5626 - acc: 0.04 - ETA: 18s - loss: 4.5625 - acc: 0.04 - ETA: 17s - loss: 4.5629 - acc: 0.04 - ETA: 17s - loss: 4.5628 - acc: 0.04 - ETA: 17s - loss: 4.5630 - acc: 0.04 - ETA: 17s - loss: 4.5627 - acc: 0.04 - ETA: 16s - loss: 4.5628 - acc: 0.04 - ETA: 16s - loss: 4.5628 - acc: 0.04 - ETA: 16s - loss: 4.5639 - acc: 0.04 - ETA: 15s - loss: 4.5632 - acc: 0.04 - ETA: 15s - loss: 4.5630 - acc: 0.04 - ETA: 15s - loss: 4.5630 - acc: 0.04 - ETA: 14s - loss: 4.5624 - acc: 0.04 - ETA: 14s - loss: 4.5630 - acc: 0.04 - ETA: 14s - loss: 4.5631 - acc: 0.04 - ETA: 14s - loss: 4.5629 - acc: 0.04 - ETA: 13s - loss: 4.5634 - acc: 0.04 - ETA: 13s - loss: 4.5631 - acc: 0.04 - ETA: 13s - loss: 4.5623 - acc: 0.04 - ETA: 12s - loss: 4.5625 - acc: 0.04 - ETA: 12s - loss: 4.5622 - acc: 0.04 - ETA: 12s - loss: 4.5617 - acc: 0.04 - ETA: 11s - loss: 4.5611 - acc: 0.04 - ETA: 11s - loss: 4.5612 - acc: 0.04 - ETA: 11s - loss: 4.5615 - acc: 0.04 - ETA: 11s - loss: 4.5616 - acc: 0.04 - ETA: 10s - loss: 4.5622 - acc: 0.04 - ETA: 10s - loss: 4.5613 - acc: 0.04 - ETA: 10s - loss: 4.5618 - acc: 0.04 - ETA: 9s - loss: 4.5624 - acc: 0.0430 - ETA: 9s - loss: 4.5625 - acc: 0.042 - ETA: 9s - loss: 4.5620 - acc: 0.042 - ETA: 8s - loss: 4.5622 - acc: 0.042 - ETA: 8s - loss: 4.5628 - acc: 0.042 - ETA: 8s - loss: 4.5630 - acc: 0.042 - ETA: 8s - loss: 4.5626 - acc: 0.043 - ETA: 7s - loss: 4.5626 - acc: 0.042 - ETA: 7s - loss: 4.5627 - acc: 0.042 - ETA: 7s - loss: 4.5626 - acc: 0.042 - ETA: 6s - loss: 4.5613 - acc: 0.042 - ETA: 6s - loss: 4.5607 - acc: 0.042 - ETA: 6s - loss: 4.5607 - acc: 0.042 - ETA: 5s - loss: 4.5610 - acc: 0.042 - ETA: 5s - loss: 4.5608 - acc: 0.042 - ETA: 5s - loss: 4.5611 - acc: 0.042 - ETA: 5s - loss: 4.5613 - acc: 0.042 - ETA: 4s - loss: 4.5612 - acc: 0.042 - ETA: 4s - loss: 4.5601 - acc: 0.042 - ETA: 4s - loss: 4.5612 - acc: 0.042 - ETA: 3s - loss: 4.5614 - acc: 0.042 - ETA: 3s - loss: 4.5623 - acc: 0.042 - ETA: 3s - loss: 4.5622 - acc: 0.042 - ETA: 2s - loss: 4.5628 - acc: 0.042 - ETA: 2s - loss: 4.5628 - acc: 0.042 - ETA: 2s - loss: 4.5630 - acc: 0.042 - ETA: 2s - loss: 4.5632 - acc: 0.042 - ETA: 1s - loss: 4.5636 - acc: 0.042 - ETA: 1s - loss: 4.5635 - acc: 0.042 - ETA: 1s - loss: 4.5637 - acc: 0.042 - ETA: 0s - loss: 4.5629 - acc: 0.042 - ETA: 0s - loss: 4.5636 - acc: 0.042 - ETA: 0s - loss: 4.5637 - acc: 0.0431Epoch 00012: val_loss improved from 4.64153 to 4.62841, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 102s - loss: 4.5636 - acc: 0.0431 - val_loss: 4.6284 - val_acc: 0.0419\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 98s - loss: 4.9445 - acc: 0.05 - ETA: 98s - loss: 4.7313 - acc: 0.05 - ETA: 98s - loss: 4.7147 - acc: 0.03 - ETA: 98s - loss: 4.5351 - acc: 0.03 - ETA: 98s - loss: 4.5218 - acc: 0.06 - ETA: 98s - loss: 4.5012 - acc: 0.07 - ETA: 97s - loss: 4.4997 - acc: 0.07 - ETA: 97s - loss: 4.4655 - acc: 0.07 - ETA: 97s - loss: 4.4505 - acc: 0.06 - ETA: 97s - loss: 4.4403 - acc: 0.06 - ETA: 96s - loss: 4.4688 - acc: 0.06 - ETA: 96s - loss: 4.4489 - acc: 0.06 - ETA: 96s - loss: 4.4639 - acc: 0.05 - ETA: 96s - loss: 4.4999 - acc: 0.05 - ETA: 95s - loss: 4.4951 - acc: 0.05 - ETA: 95s - loss: 4.4956 - acc: 0.05 - ETA: 95s - loss: 4.5148 - acc: 0.05 - ETA: 94s - loss: 4.5105 - acc: 0.05 - ETA: 94s - loss: 4.5208 - acc: 0.05 - ETA: 94s - loss: 4.5313 - acc: 0.04 - ETA: 93s - loss: 4.5369 - acc: 0.04 - ETA: 93s - loss: 4.5440 - acc: 0.04 - ETA: 93s - loss: 4.5487 - acc: 0.04 - ETA: 93s - loss: 4.5509 - acc: 0.04 - ETA: 92s - loss: 4.5627 - acc: 0.04 - ETA: 92s - loss: 4.5462 - acc: 0.04 - ETA: 92s - loss: 4.5430 - acc: 0.04 - ETA: 91s - loss: 4.5395 - acc: 0.04 - ETA: 91s - loss: 4.5384 - acc: 0.04 - ETA: 91s - loss: 4.5364 - acc: 0.04 - ETA: 90s - loss: 4.5287 - acc: 0.04 - ETA: 90s - loss: 4.5232 - acc: 0.04 - ETA: 90s - loss: 4.5293 - acc: 0.04 - ETA: 90s - loss: 4.5272 - acc: 0.04 - ETA: 89s - loss: 4.5264 - acc: 0.04 - ETA: 89s - loss: 4.5296 - acc: 0.04 - ETA: 89s - loss: 4.5348 - acc: 0.04 - ETA: 88s - loss: 4.5314 - acc: 0.04 - ETA: 88s - loss: 4.5303 - acc: 0.04 - ETA: 88s - loss: 4.5385 - acc: 0.04 - ETA: 88s - loss: 4.5433 - acc: 0.04 - ETA: 87s - loss: 4.5466 - acc: 0.04 - ETA: 87s - loss: 4.5457 - acc: 0.04 - ETA: 87s - loss: 4.5509 - acc: 0.03 - ETA: 86s - loss: 4.5517 - acc: 0.04 - ETA: 86s - loss: 4.5543 - acc: 0.03 - ETA: 86s - loss: 4.5544 - acc: 0.03 - ETA: 85s - loss: 4.5522 - acc: 0.04 - ETA: 85s - loss: 4.5451 - acc: 0.04 - ETA: 85s - loss: 4.5500 - acc: 0.04 - ETA: 84s - loss: 4.5534 - acc: 0.04 - ETA: 84s - loss: 4.5501 - acc: 0.04 - ETA: 84s - loss: 4.5511 - acc: 0.04 - ETA: 84s - loss: 4.5502 - acc: 0.04 - ETA: 83s - loss: 4.5522 - acc: 0.04 - ETA: 83s - loss: 4.5495 - acc: 0.04 - ETA: 83s - loss: 4.5511 - acc: 0.04 - ETA: 82s - loss: 4.5529 - acc: 0.04 - ETA: 82s - loss: 4.5519 - acc: 0.04 - ETA: 82s - loss: 4.5524 - acc: 0.04 - ETA: 81s - loss: 4.5477 - acc: 0.04 - ETA: 81s - loss: 4.5505 - acc: 0.04 - ETA: 81s - loss: 4.5471 - acc: 0.04 - ETA: 81s - loss: 4.5518 - acc: 0.04 - ETA: 80s - loss: 4.5517 - acc: 0.04 - ETA: 80s - loss: 4.5478 - acc: 0.04 - ETA: 80s - loss: 4.5448 - acc: 0.04 - ETA: 79s - loss: 4.5439 - acc: 0.04 - ETA: 79s - loss: 4.5460 - acc: 0.04 - ETA: 79s - loss: 4.5501 - acc: 0.04 - ETA: 78s - loss: 4.5487 - acc: 0.04 - ETA: 78s - loss: 4.5450 - acc: 0.04 - ETA: 78s - loss: 4.5414 - acc: 0.04 - ETA: 78s - loss: 4.5390 - acc: 0.04 - ETA: 77s - loss: 4.5425 - acc: 0.04 - ETA: 77s - loss: 4.5412 - acc: 0.04 - ETA: 77s - loss: 4.5394 - acc: 0.04 - ETA: 76s - loss: 4.5363 - acc: 0.04 - ETA: 76s - loss: 4.5360 - acc: 0.04 - ETA: 76s - loss: 4.5390 - acc: 0.04 - ETA: 75s - loss: 4.5407 - acc: 0.04 - ETA: 75s - loss: 4.5396 - acc: 0.04 - ETA: 75s - loss: 4.5355 - acc: 0.04 - ETA: 75s - loss: 4.5340 - acc: 0.04 - ETA: 74s - loss: 4.5346 - acc: 0.04 - ETA: 74s - loss: 4.5369 - acc: 0.04 - ETA: 74s - loss: 4.5337 - acc: 0.04 - ETA: 73s - loss: 4.5345 - acc: 0.04 - ETA: 73s - loss: 4.5373 - acc: 0.04 - ETA: 73s - loss: 4.5392 - acc: 0.04 - ETA: 73s - loss: 4.5426 - acc: 0.04 - ETA: 72s - loss: 4.5403 - acc: 0.04 - ETA: 72s - loss: 4.5427 - acc: 0.04 - ETA: 72s - loss: 4.5443 - acc: 0.04 - ETA: 71s - loss: 4.5440 - acc: 0.04 - ETA: 71s - loss: 4.5415 - acc: 0.04 - ETA: 71s - loss: 4.5435 - acc: 0.04 - ETA: 70s - loss: 4.5442 - acc: 0.04 - ETA: 70s - loss: 4.5469 - acc: 0.04 - ETA: 70s - loss: 4.5450 - acc: 0.04 - ETA: 70s - loss: 4.5446 - acc: 0.04 - ETA: 69s - loss: 4.5460 - acc: 0.04 - ETA: 69s - loss: 4.5466 - acc: 0.04 - ETA: 69s - loss: 4.5474 - acc: 0.04 - ETA: 68s - loss: 4.5445 - acc: 0.04 - ETA: 68s - loss: 4.5433 - acc: 0.04 - ETA: 68s - loss: 4.5440 - acc: 0.04 - ETA: 67s - loss: 4.5452 - acc: 0.04 - ETA: 67s - loss: 4.5463 - acc: 0.04 - ETA: 67s - loss: 4.5454 - acc: 0.04 - ETA: 67s - loss: 4.5465 - acc: 0.04 - ETA: 66s - loss: 4.5455 - acc: 0.04 - ETA: 66s - loss: 4.5443 - acc: 0.04 - ETA: 66s - loss: 4.5457 - acc: 0.04 - ETA: 65s - loss: 4.5433 - acc: 0.04 - ETA: 65s - loss: 4.5422 - acc: 0.04 - ETA: 65s - loss: 4.5414 - acc: 0.04 - ETA: 64s - loss: 4.5446 - acc: 0.04 - ETA: 64s - loss: 4.5467 - acc: 0.04 - ETA: 64s - loss: 4.5470 - acc: 0.04 - ETA: 64s - loss: 4.5474 - acc: 0.04 - ETA: 63s - loss: 4.5467 - acc: 0.04 - ETA: 63s - loss: 4.5465 - acc: 0.04 - ETA: 63s - loss: 4.5439 - acc: 0.04 - ETA: 62s - loss: 4.5464 - acc: 0.04 - ETA: 62s - loss: 4.5470 - acc: 0.04 - ETA: 62s - loss: 4.5492 - acc: 0.04 - ETA: 61s - loss: 4.5493 - acc: 0.04 - ETA: 61s - loss: 4.5469 - acc: 0.04 - ETA: 61s - loss: 4.5464 - acc: 0.04 - ETA: 61s - loss: 4.5481 - acc: 0.04 - ETA: 60s - loss: 4.5486 - acc: 0.04 - ETA: 60s - loss: 4.5468 - acc: 0.04 - ETA: 60s - loss: 4.5454 - acc: 0.04 - ETA: 59s - loss: 4.5459 - acc: 0.04 - ETA: 59s - loss: 4.5456 - acc: 0.04 - ETA: 59s - loss: 4.5471 - acc: 0.04 - ETA: 58s - loss: 4.5467 - acc: 0.04 - ETA: 58s - loss: 4.5512 - acc: 0.04 - ETA: 58s - loss: 4.5502 - acc: 0.04 - ETA: 58s - loss: 4.5512 - acc: 0.04 - ETA: 57s - loss: 4.5534 - acc: 0.04 - ETA: 57s - loss: 4.5562 - acc: 0.04 - ETA: 57s - loss: 4.5560 - acc: 0.04 - ETA: 56s - loss: 4.5559 - acc: 0.04 - ETA: 56s - loss: 4.5555 - acc: 0.04 - ETA: 56s - loss: 4.5578 - acc: 0.04 - ETA: 55s - loss: 4.5578 - acc: 0.04 - ETA: 55s - loss: 4.5599 - acc: 0.04 - ETA: 55s - loss: 4.5602 - acc: 0.04 - ETA: 55s - loss: 4.5603 - acc: 0.04 - ETA: 54s - loss: 4.5609 - acc: 0.04 - ETA: 54s - loss: 4.5605 - acc: 0.04 - ETA: 54s - loss: 4.5596 - acc: 0.04 - ETA: 53s - loss: 4.5605 - acc: 0.04 - ETA: 53s - loss: 4.5591 - acc: 0.04 - ETA: 53s - loss: 4.5601 - acc: 0.04 - ETA: 52s - loss: 4.5596 - acc: 0.04 - ETA: 52s - loss: 4.5595 - acc: 0.04 - ETA: 52s - loss: 4.5586 - acc: 0.04 - ETA: 51s - loss: 4.5595 - acc: 0.04 - ETA: 51s - loss: 4.5597 - acc: 0.04 - ETA: 51s - loss: 4.5603 - acc: 0.04 - ETA: 51s - loss: 4.5604 - acc: 0.04 - ETA: 50s - loss: 4.5614 - acc: 0.04 - ETA: 50s - loss: 4.5603 - acc: 0.04 - ETA: 50s - loss: 4.5600 - acc: 0.04 - ETA: 49s - loss: 4.5606 - acc: 0.04 - ETA: 49s - loss: 4.5604 - acc: 0.04 - ETA: 49s - loss: 4.5600 - acc: 0.04 - ETA: 48s - loss: 4.5602 - acc: 0.04 - ETA: 48s - loss: 4.5604 - acc: 0.04 - ETA: 48s - loss: 4.5607 - acc: 0.04 - ETA: 48s - loss: 4.5619 - acc: 0.04 - ETA: 47s - loss: 4.5629 - acc: 0.04 - ETA: 47s - loss: 4.5625 - acc: 0.04 - ETA: 47s - loss: 4.5628 - acc: 0.04 - ETA: 46s - loss: 4.5636 - acc: 0.04 - ETA: 46s - loss: 4.5632 - acc: 0.04 - ETA: 46s - loss: 4.5619 - acc: 0.04 - ETA: 45s - loss: 4.5618 - acc: 0.04 - ETA: 45s - loss: 4.5619 - acc: 0.04 - ETA: 45s - loss: 4.5629 - acc: 0.04 - ETA: 45s - loss: 4.5624 - acc: 0.04 - ETA: 44s - loss: 4.5634 - acc: 0.04 - ETA: 44s - loss: 4.5635 - acc: 0.03 - ETA: 44s - loss: 4.5620 - acc: 0.03 - ETA: 43s - loss: 4.5623 - acc: 0.03 - ETA: 43s - loss: 4.5632 - acc: 0.03 - ETA: 43s - loss: 4.5638 - acc: 0.03 - ETA: 42s - loss: 4.5629 - acc: 0.03 - ETA: 42s - loss: 4.5628 - acc: 0.03 - ETA: 42s - loss: 4.5627 - acc: 0.03 - ETA: 42s - loss: 4.5626 - acc: 0.03 - ETA: 41s - loss: 4.5616 - acc: 0.03 - ETA: 41s - loss: 4.5601 - acc: 0.03 - ETA: 41s - loss: 4.5601 - acc: 0.03 - ETA: 40s - loss: 4.5600 - acc: 0.03 - ETA: 40s - loss: 4.5589 - acc: 0.03 - ETA: 40s - loss: 4.5588 - acc: 0.03 - ETA: 39s - loss: 4.5588 - acc: 0.03 - ETA: 39s - loss: 4.5579 - acc: 0.03 - ETA: 39s - loss: 4.5587 - acc: 0.03 - ETA: 39s - loss: 4.5608 - acc: 0.03 - ETA: 38s - loss: 4.5609 - acc: 0.03 - ETA: 38s - loss: 4.5625 - acc: 0.03 - ETA: 38s - loss: 4.5616 - acc: 0.03 - ETA: 37s - loss: 4.5608 - acc: 0.03 - ETA: 37s - loss: 4.5600 - acc: 0.03 - ETA: 37s - loss: 4.5595 - acc: 0.03 - ETA: 36s - loss: 4.5602 - acc: 0.03 - ETA: 36s - loss: 4.5602 - acc: 0.03 - ETA: 36s - loss: 4.5600 - acc: 0.03 - ETA: 36s - loss: 4.5594 - acc: 0.03 - ETA: 35s - loss: 4.5585 - acc: 0.0388"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 35s - loss: 4.5582 - acc: 0.03 - ETA: 35s - loss: 4.5582 - acc: 0.03 - ETA: 34s - loss: 4.5564 - acc: 0.03 - ETA: 34s - loss: 4.5566 - acc: 0.03 - ETA: 34s - loss: 4.5563 - acc: 0.03 - ETA: 33s - loss: 4.5558 - acc: 0.03 - ETA: 33s - loss: 4.5556 - acc: 0.03 - ETA: 33s - loss: 4.5543 - acc: 0.03 - ETA: 33s - loss: 4.5537 - acc: 0.03 - ETA: 32s - loss: 4.5547 - acc: 0.03 - ETA: 32s - loss: 4.5541 - acc: 0.03 - ETA: 32s - loss: 4.5544 - acc: 0.03 - ETA: 31s - loss: 4.5547 - acc: 0.03 - ETA: 31s - loss: 4.5552 - acc: 0.03 - ETA: 31s - loss: 4.5543 - acc: 0.03 - ETA: 30s - loss: 4.5532 - acc: 0.03 - ETA: 30s - loss: 4.5540 - acc: 0.03 - ETA: 30s - loss: 4.5533 - acc: 0.03 - ETA: 30s - loss: 4.5524 - acc: 0.03 - ETA: 29s - loss: 4.5526 - acc: 0.03 - ETA: 29s - loss: 4.5532 - acc: 0.03 - ETA: 29s - loss: 4.5519 - acc: 0.03 - ETA: 28s - loss: 4.5512 - acc: 0.03 - ETA: 28s - loss: 4.5498 - acc: 0.03 - ETA: 28s - loss: 4.5496 - acc: 0.03 - ETA: 27s - loss: 4.5486 - acc: 0.03 - ETA: 27s - loss: 4.5487 - acc: 0.03 - ETA: 27s - loss: 4.5482 - acc: 0.03 - ETA: 27s - loss: 4.5471 - acc: 0.03 - ETA: 26s - loss: 4.5465 - acc: 0.03 - ETA: 26s - loss: 4.5470 - acc: 0.03 - ETA: 26s - loss: 4.5473 - acc: 0.03 - ETA: 25s - loss: 4.5465 - acc: 0.03 - ETA: 25s - loss: 4.5463 - acc: 0.03 - ETA: 25s - loss: 4.5459 - acc: 0.03 - ETA: 24s - loss: 4.5474 - acc: 0.03 - ETA: 24s - loss: 4.5474 - acc: 0.03 - ETA: 24s - loss: 4.5473 - acc: 0.03 - ETA: 24s - loss: 4.5474 - acc: 0.03 - ETA: 23s - loss: 4.5469 - acc: 0.03 - ETA: 23s - loss: 4.5463 - acc: 0.04 - ETA: 23s - loss: 4.5459 - acc: 0.03 - ETA: 22s - loss: 4.5455 - acc: 0.04 - ETA: 22s - loss: 4.5447 - acc: 0.04 - ETA: 22s - loss: 4.5437 - acc: 0.04 - ETA: 21s - loss: 4.5435 - acc: 0.04 - ETA: 21s - loss: 4.5433 - acc: 0.04 - ETA: 21s - loss: 4.5434 - acc: 0.04 - ETA: 21s - loss: 4.5433 - acc: 0.04 - ETA: 20s - loss: 4.5431 - acc: 0.04 - ETA: 20s - loss: 4.5421 - acc: 0.04 - ETA: 20s - loss: 4.5441 - acc: 0.04 - ETA: 19s - loss: 4.5438 - acc: 0.04 - ETA: 19s - loss: 4.5448 - acc: 0.04 - ETA: 19s - loss: 4.5441 - acc: 0.04 - ETA: 18s - loss: 4.5444 - acc: 0.04 - ETA: 18s - loss: 4.5437 - acc: 0.04 - ETA: 18s - loss: 4.5431 - acc: 0.04 - ETA: 18s - loss: 4.5430 - acc: 0.04 - ETA: 17s - loss: 4.5429 - acc: 0.04 - ETA: 17s - loss: 4.5429 - acc: 0.04 - ETA: 17s - loss: 4.5423 - acc: 0.04 - ETA: 16s - loss: 4.5418 - acc: 0.04 - ETA: 16s - loss: 4.5423 - acc: 0.04 - ETA: 16s - loss: 4.5438 - acc: 0.04 - ETA: 15s - loss: 4.5438 - acc: 0.04 - ETA: 15s - loss: 4.5444 - acc: 0.04 - ETA: 15s - loss: 4.5446 - acc: 0.04 - ETA: 15s - loss: 4.5440 - acc: 0.04 - ETA: 14s - loss: 4.5449 - acc: 0.04 - ETA: 14s - loss: 4.5447 - acc: 0.04 - ETA: 14s - loss: 4.5446 - acc: 0.04 - ETA: 13s - loss: 4.5448 - acc: 0.04 - ETA: 13s - loss: 4.5463 - acc: 0.04 - ETA: 13s - loss: 4.5476 - acc: 0.04 - ETA: 12s - loss: 4.5476 - acc: 0.04 - ETA: 12s - loss: 4.5472 - acc: 0.04 - ETA: 12s - loss: 4.5473 - acc: 0.04 - ETA: 12s - loss: 4.5477 - acc: 0.04 - ETA: 11s - loss: 4.5472 - acc: 0.04 - ETA: 11s - loss: 4.5469 - acc: 0.04 - ETA: 11s - loss: 4.5465 - acc: 0.04 - ETA: 10s - loss: 4.5471 - acc: 0.04 - ETA: 10s - loss: 4.5466 - acc: 0.04 - ETA: 10s - loss: 4.5478 - acc: 0.04 - ETA: 9s - loss: 4.5489 - acc: 0.0410 - ETA: 9s - loss: 4.5494 - acc: 0.041 - ETA: 9s - loss: 4.5492 - acc: 0.041 - ETA: 9s - loss: 4.5487 - acc: 0.041 - ETA: 8s - loss: 4.5476 - acc: 0.041 - ETA: 8s - loss: 4.5482 - acc: 0.041 - ETA: 8s - loss: 4.5480 - acc: 0.041 - ETA: 7s - loss: 4.5481 - acc: 0.041 - ETA: 7s - loss: 4.5475 - acc: 0.041 - ETA: 7s - loss: 4.5467 - acc: 0.041 - ETA: 6s - loss: 4.5468 - acc: 0.041 - ETA: 6s - loss: 4.5457 - acc: 0.041 - ETA: 6s - loss: 4.5463 - acc: 0.041 - ETA: 6s - loss: 4.5453 - acc: 0.041 - ETA: 5s - loss: 4.5456 - acc: 0.041 - ETA: 5s - loss: 4.5457 - acc: 0.041 - ETA: 5s - loss: 4.5476 - acc: 0.041 - ETA: 4s - loss: 4.5476 - acc: 0.041 - ETA: 4s - loss: 4.5477 - acc: 0.041 - ETA: 4s - loss: 4.5467 - acc: 0.041 - ETA: 3s - loss: 4.5456 - acc: 0.041 - ETA: 3s - loss: 4.5454 - acc: 0.041 - ETA: 3s - loss: 4.5452 - acc: 0.041 - ETA: 3s - loss: 4.5451 - acc: 0.041 - ETA: 2s - loss: 4.5450 - acc: 0.041 - ETA: 2s - loss: 4.5449 - acc: 0.041 - ETA: 2s - loss: 4.5447 - acc: 0.041 - ETA: 1s - loss: 4.5442 - acc: 0.041 - ETA: 1s - loss: 4.5439 - acc: 0.041 - ETA: 1s - loss: 4.5439 - acc: 0.041 - ETA: 0s - loss: 4.5438 - acc: 0.041 - ETA: 0s - loss: 4.5447 - acc: 0.041 - ETA: 0s - loss: 4.5451 - acc: 0.0416Epoch 00013: val_loss improved from 4.62841 to 4.62776, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 102s - loss: 4.5450 - acc: 0.0418 - val_loss: 4.6278 - val_acc: 0.0419\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 100s - loss: 4.4021 - acc: 0.050 - ETA: 99s - loss: 4.4496 - acc: 0.050 - ETA: 99s - loss: 4.3995 - acc: 0.05 - ETA: 99s - loss: 4.4072 - acc: 0.05 - ETA: 98s - loss: 4.3664 - acc: 0.06 - ETA: 98s - loss: 4.4074 - acc: 0.05 - ETA: 98s - loss: 4.3563 - acc: 0.06 - ETA: 97s - loss: 4.3459 - acc: 0.05 - ETA: 97s - loss: 4.3539 - acc: 0.05 - ETA: 97s - loss: 4.3702 - acc: 0.04 - ETA: 96s - loss: 4.3710 - acc: 0.04 - ETA: 96s - loss: 4.3538 - acc: 0.04 - ETA: 96s - loss: 4.3278 - acc: 0.05 - ETA: 96s - loss: 4.3507 - acc: 0.05 - ETA: 95s - loss: 4.3747 - acc: 0.05 - ETA: 95s - loss: 4.3844 - acc: 0.04 - ETA: 95s - loss: 4.3906 - acc: 0.05 - ETA: 94s - loss: 4.4000 - acc: 0.05 - ETA: 94s - loss: 4.3944 - acc: 0.05 - ETA: 94s - loss: 4.4077 - acc: 0.05 - ETA: 94s - loss: 4.4026 - acc: 0.05 - ETA: 93s - loss: 4.4260 - acc: 0.05 - ETA: 93s - loss: 4.4248 - acc: 0.05 - ETA: 93s - loss: 4.4347 - acc: 0.05 - ETA: 92s - loss: 4.4441 - acc: 0.05 - ETA: 92s - loss: 4.4414 - acc: 0.05 - ETA: 92s - loss: 4.4445 - acc: 0.05 - ETA: 91s - loss: 4.4434 - acc: 0.05 - ETA: 91s - loss: 4.4439 - acc: 0.05 - ETA: 91s - loss: 4.4399 - acc: 0.05 - ETA: 90s - loss: 4.4483 - acc: 0.05 - ETA: 90s - loss: 4.4527 - acc: 0.05 - ETA: 90s - loss: 4.4409 - acc: 0.05 - ETA: 90s - loss: 4.4461 - acc: 0.05 - ETA: 89s - loss: 4.4552 - acc: 0.05 - ETA: 89s - loss: 4.4527 - acc: 0.05 - ETA: 89s - loss: 4.4470 - acc: 0.05 - ETA: 88s - loss: 4.4558 - acc: 0.05 - ETA: 88s - loss: 4.4541 - acc: 0.05 - ETA: 88s - loss: 4.4581 - acc: 0.05 - ETA: 87s - loss: 4.4633 - acc: 0.05 - ETA: 87s - loss: 4.4701 - acc: 0.05 - ETA: 87s - loss: 4.4655 - acc: 0.05 - ETA: 87s - loss: 4.4696 - acc: 0.05 - ETA: 86s - loss: 4.4677 - acc: 0.05 - ETA: 86s - loss: 4.4602 - acc: 0.05 - ETA: 86s - loss: 4.4613 - acc: 0.05 - ETA: 85s - loss: 4.4662 - acc: 0.05 - ETA: 85s - loss: 4.4649 - acc: 0.05 - ETA: 85s - loss: 4.4745 - acc: 0.05 - ETA: 84s - loss: 4.4765 - acc: 0.05 - ETA: 84s - loss: 4.4767 - acc: 0.05 - ETA: 84s - loss: 4.4813 - acc: 0.04 - ETA: 84s - loss: 4.4855 - acc: 0.05 - ETA: 83s - loss: 4.4904 - acc: 0.04 - ETA: 83s - loss: 4.4908 - acc: 0.04 - ETA: 83s - loss: 4.4904 - acc: 0.04 - ETA: 82s - loss: 4.4943 - acc: 0.04 - ETA: 82s - loss: 4.4939 - acc: 0.04 - ETA: 82s - loss: 4.4933 - acc: 0.04 - ETA: 81s - loss: 4.4940 - acc: 0.04 - ETA: 81s - loss: 4.4926 - acc: 0.04 - ETA: 81s - loss: 4.4882 - acc: 0.04 - ETA: 81s - loss: 4.4831 - acc: 0.05 - ETA: 80s - loss: 4.4817 - acc: 0.05 - ETA: 80s - loss: 4.4823 - acc: 0.05 - ETA: 80s - loss: 4.4879 - acc: 0.04 - ETA: 79s - loss: 4.4921 - acc: 0.04 - ETA: 79s - loss: 4.4897 - acc: 0.04 - ETA: 79s - loss: 4.4916 - acc: 0.04 - ETA: 78s - loss: 4.4902 - acc: 0.04 - ETA: 78s - loss: 4.4882 - acc: 0.04 - ETA: 78s - loss: 4.4882 - acc: 0.04 - ETA: 78s - loss: 4.4870 - acc: 0.04 - ETA: 77s - loss: 4.4886 - acc: 0.04 - ETA: 77s - loss: 4.4881 - acc: 0.04 - ETA: 77s - loss: 4.4882 - acc: 0.04 - ETA: 76s - loss: 4.4947 - acc: 0.04 - ETA: 76s - loss: 4.4972 - acc: 0.04 - ETA: 76s - loss: 4.4995 - acc: 0.04 - ETA: 75s - loss: 4.5015 - acc: 0.04 - ETA: 75s - loss: 4.5001 - acc: 0.04 - ETA: 75s - loss: 4.5053 - acc: 0.04 - ETA: 75s - loss: 4.5022 - acc: 0.04 - ETA: 74s - loss: 4.5000 - acc: 0.04 - ETA: 74s - loss: 4.5015 - acc: 0.04 - ETA: 74s - loss: 4.5011 - acc: 0.04 - ETA: 73s - loss: 4.4996 - acc: 0.04 - ETA: 73s - loss: 4.4970 - acc: 0.04 - ETA: 73s - loss: 4.4991 - acc: 0.04 - ETA: 72s - loss: 4.4991 - acc: 0.04 - ETA: 72s - loss: 4.4969 - acc: 0.04 - ETA: 72s - loss: 4.4939 - acc: 0.04 - ETA: 72s - loss: 4.4924 - acc: 0.04 - ETA: 71s - loss: 4.4907 - acc: 0.04 - ETA: 71s - loss: 4.4877 - acc: 0.04 - ETA: 71s - loss: 4.4882 - acc: 0.04 - ETA: 70s - loss: 4.4876 - acc: 0.04 - ETA: 70s - loss: 4.4884 - acc: 0.04 - ETA: 70s - loss: 4.4890 - acc: 0.04 - ETA: 69s - loss: 4.4937 - acc: 0.04 - ETA: 69s - loss: 4.4925 - acc: 0.04 - ETA: 69s - loss: 4.4947 - acc: 0.04 - ETA: 69s - loss: 4.4948 - acc: 0.04 - ETA: 68s - loss: 4.4938 - acc: 0.04 - ETA: 68s - loss: 4.4970 - acc: 0.04 - ETA: 68s - loss: 4.4960 - acc: 0.04 - ETA: 67s - loss: 4.4951 - acc: 0.04 - ETA: 67s - loss: 4.4951 - acc: 0.04 - ETA: 67s - loss: 4.4960 - acc: 0.04 - ETA: 66s - loss: 4.4978 - acc: 0.04 - ETA: 66s - loss: 4.5056 - acc: 0.04 - ETA: 66s - loss: 4.5047 - acc: 0.04 - ETA: 66s - loss: 4.5052 - acc: 0.04 - ETA: 65s - loss: 4.5057 - acc: 0.04 - ETA: 65s - loss: 4.5056 - acc: 0.04 - ETA: 65s - loss: 4.5048 - acc: 0.04 - ETA: 64s - loss: 4.5058 - acc: 0.04 - ETA: 64s - loss: 4.5089 - acc: 0.04 - ETA: 64s - loss: 4.5131 - acc: 0.04 - ETA: 63s - loss: 4.5121 - acc: 0.04 - ETA: 63s - loss: 4.5140 - acc: 0.04 - ETA: 63s - loss: 4.5166 - acc: 0.04 - ETA: 63s - loss: 4.5166 - acc: 0.04 - ETA: 62s - loss: 4.5173 - acc: 0.04 - ETA: 62s - loss: 4.5196 - acc: 0.04 - ETA: 62s - loss: 4.5205 - acc: 0.04 - ETA: 61s - loss: 4.5189 - acc: 0.04 - ETA: 61s - loss: 4.5199 - acc: 0.04 - ETA: 61s - loss: 4.5213 - acc: 0.04 - ETA: 60s - loss: 4.5215 - acc: 0.04 - ETA: 60s - loss: 4.5225 - acc: 0.04 - ETA: 60s - loss: 4.5234 - acc: 0.04 - ETA: 60s - loss: 4.5239 - acc: 0.04 - ETA: 59s - loss: 4.5252 - acc: 0.04 - ETA: 59s - loss: 4.5248 - acc: 0.04 - ETA: 59s - loss: 4.5261 - acc: 0.04 - ETA: 58s - loss: 4.5260 - acc: 0.04 - ETA: 58s - loss: 4.5276 - acc: 0.04 - ETA: 58s - loss: 4.5263 - acc: 0.04 - ETA: 57s - loss: 4.5259 - acc: 0.04 - ETA: 57s - loss: 4.5269 - acc: 0.04 - ETA: 57s - loss: 4.5250 - acc: 0.04 - ETA: 57s - loss: 4.5243 - acc: 0.04 - ETA: 56s - loss: 4.5261 - acc: 0.04 - ETA: 56s - loss: 4.5244 - acc: 0.04 - ETA: 56s - loss: 4.5258 - acc: 0.04 - ETA: 55s - loss: 4.5258 - acc: 0.04 - ETA: 55s - loss: 4.5271 - acc: 0.04 - ETA: 55s - loss: 4.5261 - acc: 0.04 - ETA: 54s - loss: 4.5275 - acc: 0.04 - ETA: 54s - loss: 4.5267 - acc: 0.04 - ETA: 54s - loss: 4.5275 - acc: 0.04 - ETA: 54s - loss: 4.5268 - acc: 0.04 - ETA: 53s - loss: 4.5264 - acc: 0.04 - ETA: 53s - loss: 4.5251 - acc: 0.04 - ETA: 53s - loss: 4.5256 - acc: 0.04 - ETA: 52s - loss: 4.5283 - acc: 0.04 - ETA: 52s - loss: 4.5309 - acc: 0.04 - ETA: 52s - loss: 4.5311 - acc: 0.04 - ETA: 51s - loss: 4.5309 - acc: 0.04 - ETA: 51s - loss: 4.5321 - acc: 0.04 - ETA: 51s - loss: 4.5331 - acc: 0.04 - ETA: 51s - loss: 4.5316 - acc: 0.04 - ETA: 50s - loss: 4.5291 - acc: 0.04 - ETA: 50s - loss: 4.5296 - acc: 0.04 - ETA: 50s - loss: 4.5288 - acc: 0.04 - ETA: 49s - loss: 4.5286 - acc: 0.04 - ETA: 49s - loss: 4.5278 - acc: 0.04 - ETA: 49s - loss: 4.5272 - acc: 0.04 - ETA: 48s - loss: 4.5280 - acc: 0.04 - ETA: 48s - loss: 4.5280 - acc: 0.04 - ETA: 48s - loss: 4.5271 - acc: 0.04 - ETA: 48s - loss: 4.5293 - acc: 0.04 - ETA: 47s - loss: 4.5287 - acc: 0.04 - ETA: 47s - loss: 4.5285 - acc: 0.04 - ETA: 47s - loss: 4.5288 - acc: 0.04 - ETA: 46s - loss: 4.5293 - acc: 0.04 - ETA: 46s - loss: 4.5307 - acc: 0.04 - ETA: 46s - loss: 4.5304 - acc: 0.04 - ETA: 45s - loss: 4.5318 - acc: 0.04 - ETA: 45s - loss: 4.5310 - acc: 0.04 - ETA: 45s - loss: 4.5295 - acc: 0.04 - ETA: 45s - loss: 4.5305 - acc: 0.04 - ETA: 44s - loss: 4.5307 - acc: 0.04 - ETA: 44s - loss: 4.5301 - acc: 0.04 - ETA: 44s - loss: 4.5304 - acc: 0.04 - ETA: 43s - loss: 4.5300 - acc: 0.04 - ETA: 43s - loss: 4.5297 - acc: 0.04 - ETA: 43s - loss: 4.5302 - acc: 0.04 - ETA: 42s - loss: 4.5309 - acc: 0.04 - ETA: 42s - loss: 4.5307 - acc: 0.04 - ETA: 42s - loss: 4.5300 - acc: 0.04 - ETA: 42s - loss: 4.5299 - acc: 0.04 - ETA: 41s - loss: 4.5283 - acc: 0.04 - ETA: 41s - loss: 4.5267 - acc: 0.04 - ETA: 41s - loss: 4.5264 - acc: 0.04 - ETA: 40s - loss: 4.5268 - acc: 0.04 - ETA: 40s - loss: 4.5245 - acc: 0.04 - ETA: 40s - loss: 4.5270 - acc: 0.04 - ETA: 39s - loss: 4.5278 - acc: 0.04 - ETA: 39s - loss: 4.5261 - acc: 0.04 - ETA: 39s - loss: 4.5264 - acc: 0.04 - ETA: 39s - loss: 4.5267 - acc: 0.04 - ETA: 38s - loss: 4.5267 - acc: 0.04 - ETA: 38s - loss: 4.5264 - acc: 0.04 - ETA: 38s - loss: 4.5267 - acc: 0.04 - ETA: 37s - loss: 4.5264 - acc: 0.04 - ETA: 37s - loss: 4.5279 - acc: 0.04 - ETA: 37s - loss: 4.5289 - acc: 0.04 - ETA: 36s - loss: 4.5296 - acc: 0.04 - ETA: 36s - loss: 4.5303 - acc: 0.04 - ETA: 36s - loss: 4.5326 - acc: 0.04 - ETA: 36s - loss: 4.5321 - acc: 0.04 - ETA: 35s - loss: 4.5326 - acc: 0.04476660/6680 [============================>.] - ETA: 35s - loss: 4.5329 - acc: 0.04 - ETA: 35s - loss: 4.5322 - acc: 0.04 - ETA: 34s - loss: 4.5312 - acc: 0.04 - ETA: 34s - loss: 4.5300 - acc: 0.04 - ETA: 34s - loss: 4.5300 - acc: 0.04 - ETA: 33s - loss: 4.5308 - acc: 0.04 - ETA: 33s - loss: 4.5314 - acc: 0.04 - ETA: 33s - loss: 4.5322 - acc: 0.04 - ETA: 33s - loss: 4.5320 - acc: 0.04 - ETA: 32s - loss: 4.5321 - acc: 0.04 - ETA: 32s - loss: 4.5317 - acc: 0.04 - ETA: 32s - loss: 4.5309 - acc: 0.04 - ETA: 31s - loss: 4.5302 - acc: 0.04 - ETA: 31s - loss: 4.5298 - acc: 0.04 - ETA: 31s - loss: 4.5289 - acc: 0.04 - ETA: 30s - loss: 4.5286 - acc: 0.04 - ETA: 30s - loss: 4.5286 - acc: 0.04 - ETA: 30s - loss: 4.5288 - acc: 0.04 - ETA: 30s - loss: 4.5292 - acc: 0.04 - ETA: 29s - loss: 4.5291 - acc: 0.04 - ETA: 29s - loss: 4.5291 - acc: 0.04 - ETA: 29s - loss: 4.5291 - acc: 0.04 - ETA: 28s - loss: 4.5286 - acc: 0.04 - ETA: 28s - loss: 4.5307 - acc: 0.04 - ETA: 28s - loss: 4.5310 - acc: 0.04 - ETA: 27s - loss: 4.5311 - acc: 0.04 - ETA: 27s - loss: 4.5307 - acc: 0.04 - ETA: 27s - loss: 4.5306 - acc: 0.04 - ETA: 27s - loss: 4.5307 - acc: 0.04 - ETA: 26s - loss: 4.5298 - acc: 0.04 - ETA: 26s - loss: 4.5313 - acc: 0.04 - ETA: 26s - loss: 4.5309 - acc: 0.04 - ETA: 25s - loss: 4.5320 - acc: 0.04 - ETA: 25s - loss: 4.5314 - acc: 0.04 - ETA: 25s - loss: 4.5325 - acc: 0.04 - ETA: 24s - loss: 4.5318 - acc: 0.04 - ETA: 24s - loss: 4.5314 - acc: 0.04 - ETA: 24s - loss: 4.5318 - acc: 0.04 - ETA: 24s - loss: 4.5317 - acc: 0.04 - ETA: 23s - loss: 4.5317 - acc: 0.04 - ETA: 23s - loss: 4.5311 - acc: 0.04 - ETA: 23s - loss: 4.5304 - acc: 0.04 - ETA: 22s - loss: 4.5326 - acc: 0.04 - ETA: 22s - loss: 4.5341 - acc: 0.04 - ETA: 22s - loss: 4.5342 - acc: 0.04 - ETA: 21s - loss: 4.5339 - acc: 0.04 - ETA: 21s - loss: 4.5316 - acc: 0.04 - ETA: 21s - loss: 4.5319 - acc: 0.04 - ETA: 21s - loss: 4.5314 - acc: 0.04 - ETA: 20s - loss: 4.5307 - acc: 0.04 - ETA: 20s - loss: 4.5308 - acc: 0.04 - ETA: 20s - loss: 4.5307 - acc: 0.04 - ETA: 19s - loss: 4.5305 - acc: 0.04 - ETA: 19s - loss: 4.5305 - acc: 0.04 - ETA: 19s - loss: 4.5299 - acc: 0.04 - ETA: 18s - loss: 4.5294 - acc: 0.04 - ETA: 18s - loss: 4.5303 - acc: 0.04 - ETA: 18s - loss: 4.5294 - acc: 0.04 - ETA: 18s - loss: 4.5298 - acc: 0.04 - ETA: 17s - loss: 4.5297 - acc: 0.04 - ETA: 17s - loss: 4.5292 - acc: 0.04 - ETA: 17s - loss: 4.5289 - acc: 0.04 - ETA: 16s - loss: 4.5286 - acc: 0.04 - ETA: 16s - loss: 4.5280 - acc: 0.04 - ETA: 16s - loss: 4.5281 - acc: 0.04 - ETA: 15s - loss: 4.5282 - acc: 0.04 - ETA: 15s - loss: 4.5282 - acc: 0.04 - ETA: 15s - loss: 4.5278 - acc: 0.04 - ETA: 15s - loss: 4.5269 - acc: 0.04 - ETA: 14s - loss: 4.5264 - acc: 0.04 - ETA: 14s - loss: 4.5269 - acc: 0.04 - ETA: 14s - loss: 4.5268 - acc: 0.04 - ETA: 13s - loss: 4.5269 - acc: 0.04 - ETA: 13s - loss: 4.5279 - acc: 0.04 - ETA: 13s - loss: 4.5274 - acc: 0.04 - ETA: 12s - loss: 4.5269 - acc: 0.04 - ETA: 12s - loss: 4.5281 - acc: 0.04 - ETA: 12s - loss: 4.5285 - acc: 0.04 - ETA: 12s - loss: 4.5286 - acc: 0.04 - ETA: 11s - loss: 4.5291 - acc: 0.04 - ETA: 11s - loss: 4.5287 - acc: 0.04 - ETA: 11s - loss: 4.5289 - acc: 0.04 - ETA: 10s - loss: 4.5285 - acc: 0.04 - ETA: 10s - loss: 4.5281 - acc: 0.04 - ETA: 10s - loss: 4.5282 - acc: 0.04 - ETA: 9s - loss: 4.5284 - acc: 0.0449 - ETA: 9s - loss: 4.5284 - acc: 0.044 - ETA: 9s - loss: 4.5291 - acc: 0.044 - ETA: 9s - loss: 4.5283 - acc: 0.044 - ETA: 8s - loss: 4.5285 - acc: 0.044 - ETA: 8s - loss: 4.5287 - acc: 0.044 - ETA: 8s - loss: 4.5286 - acc: 0.045 - ETA: 7s - loss: 4.5284 - acc: 0.045 - ETA: 7s - loss: 4.5288 - acc: 0.045 - ETA: 7s - loss: 4.5295 - acc: 0.045 - ETA: 6s - loss: 4.5295 - acc: 0.045 - ETA: 6s - loss: 4.5289 - acc: 0.045 - ETA: 6s - loss: 4.5289 - acc: 0.045 - ETA: 6s - loss: 4.5286 - acc: 0.044 - ETA: 5s - loss: 4.5289 - acc: 0.044 - ETA: 5s - loss: 4.5293 - acc: 0.044 - ETA: 5s - loss: 4.5287 - acc: 0.044 - ETA: 4s - loss: 4.5283 - acc: 0.044 - ETA: 4s - loss: 4.5280 - acc: 0.044 - ETA: 4s - loss: 4.5273 - acc: 0.044 - ETA: 3s - loss: 4.5266 - acc: 0.044 - ETA: 3s - loss: 4.5265 - acc: 0.045 - ETA: 3s - loss: 4.5266 - acc: 0.045 - ETA: 3s - loss: 4.5262 - acc: 0.045 - ETA: 2s - loss: 4.5255 - acc: 0.045 - ETA: 2s - loss: 4.5255 - acc: 0.045 - ETA: 2s - loss: 4.5252 - acc: 0.045 - ETA: 1s - loss: 4.5244 - acc: 0.045 - ETA: 1s - loss: 4.5246 - acc: 0.045 - ETA: 1s - loss: 4.5242 - acc: 0.045 - ETA: 0s - loss: 4.5236 - acc: 0.046 - ETA: 0s - loss: 4.5235 - acc: 0.046 - ETA: 0s - loss: 4.5235 - acc: 0.0461Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 102s - loss: 4.5230 - acc: 0.0461 - val_loss: 4.6315 - val_acc: 0.0323\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 98s - loss: 4.6746 - acc: 0.0000e+ - ETA: 99s - loss: 4.5726 - acc: 0.0500   - ETA: 99s - loss: 4.5793 - acc: 0.05 - ETA: 99s - loss: 4.5834 - acc: 0.03 - ETA: 99s - loss: 4.5593 - acc: 0.05 - ETA: 99s - loss: 4.5452 - acc: 0.04 - ETA: 99s - loss: 4.5225 - acc: 0.04 - ETA: 98s - loss: 4.5347 - acc: 0.04 - ETA: 98s - loss: 4.5514 - acc: 0.03 - ETA: 98s - loss: 4.5446 - acc: 0.03 - ETA: 97s - loss: 4.5467 - acc: 0.03 - ETA: 97s - loss: 4.5362 - acc: 0.03 - ETA: 97s - loss: 4.5206 - acc: 0.03 - ETA: 96s - loss: 4.5076 - acc: 0.04 - ETA: 96s - loss: 4.4873 - acc: 0.05 - ETA: 95s - loss: 4.4921 - acc: 0.05 - ETA: 95s - loss: 4.4834 - acc: 0.05 - ETA: 94s - loss: 4.4735 - acc: 0.05 - ETA: 94s - loss: 4.4709 - acc: 0.05 - ETA: 94s - loss: 4.4697 - acc: 0.05 - ETA: 93s - loss: 4.4788 - acc: 0.04 - ETA: 93s - loss: 4.4784 - acc: 0.04 - ETA: 93s - loss: 4.4757 - acc: 0.04 - ETA: 92s - loss: 4.4804 - acc: 0.04 - ETA: 92s - loss: 4.4854 - acc: 0.04 - ETA: 92s - loss: 4.4858 - acc: 0.04 - ETA: 91s - loss: 4.4944 - acc: 0.04 - ETA: 91s - loss: 4.5024 - acc: 0.04 - ETA: 91s - loss: 4.4953 - acc: 0.04 - ETA: 90s - loss: 4.4884 - acc: 0.04 - ETA: 90s - loss: 4.4858 - acc: 0.04 - ETA: 90s - loss: 4.4799 - acc: 0.04 - ETA: 89s - loss: 4.4807 - acc: 0.04 - ETA: 89s - loss: 4.4745 - acc: 0.04 - ETA: 89s - loss: 4.4719 - acc: 0.04 - ETA: 88s - loss: 4.4862 - acc: 0.04 - ETA: 88s - loss: 4.4834 - acc: 0.04 - ETA: 88s - loss: 4.4765 - acc: 0.04 - ETA: 87s - loss: 4.4739 - acc: 0.04 - ETA: 87s - loss: 4.4709 - acc: 0.05 - ETA: 87s - loss: 4.4835 - acc: 0.05 - ETA: 86s - loss: 4.4865 - acc: 0.05 - ETA: 86s - loss: 4.4896 - acc: 0.05 - ETA: 86s - loss: 4.4896 - acc: 0.05 - ETA: 86s - loss: 4.4898 - acc: 0.05 - ETA: 85s - loss: 4.4980 - acc: 0.05 - ETA: 85s - loss: 4.4940 - acc: 0.05 - ETA: 85s - loss: 4.4900 - acc: 0.05 - ETA: 84s - loss: 4.4932 - acc: 0.05 - ETA: 84s - loss: 4.4949 - acc: 0.05 - ETA: 84s - loss: 4.4922 - acc: 0.05 - ETA: 84s - loss: 4.4907 - acc: 0.05 - ETA: 83s - loss: 4.4979 - acc: 0.05 - ETA: 83s - loss: 4.4945 - acc: 0.05 - ETA: 83s - loss: 4.4928 - acc: 0.05 - ETA: 82s - loss: 4.4926 - acc: 0.05 - ETA: 82s - loss: 4.4911 - acc: 0.05 - ETA: 82s - loss: 4.4888 - acc: 0.05 - ETA: 82s - loss: 4.4824 - acc: 0.05 - ETA: 81s - loss: 4.4836 - acc: 0.05 - ETA: 81s - loss: 4.4802 - acc: 0.05 - ETA: 81s - loss: 4.4795 - acc: 0.05 - ETA: 80s - loss: 4.4779 - acc: 0.05 - ETA: 80s - loss: 4.4809 - acc: 0.05 - ETA: 80s - loss: 4.4847 - acc: 0.05 - ETA: 80s - loss: 4.4877 - acc: 0.05 - ETA: 79s - loss: 4.4987 - acc: 0.05 - ETA: 79s - loss: 4.5001 - acc: 0.05 - ETA: 79s - loss: 4.4992 - acc: 0.05 - ETA: 78s - loss: 4.5023 - acc: 0.05 - ETA: 78s - loss: 4.5055 - acc: 0.05 - ETA: 78s - loss: 4.5060 - acc: 0.05 - ETA: 78s - loss: 4.5077 - acc: 0.05 - ETA: 77s - loss: 4.5130 - acc: 0.05 - ETA: 77s - loss: 4.5117 - acc: 0.05 - ETA: 77s - loss: 4.5101 - acc: 0.05 - ETA: 76s - loss: 4.5118 - acc: 0.05 - ETA: 76s - loss: 4.5090 - acc: 0.05 - ETA: 76s - loss: 4.5089 - acc: 0.05 - ETA: 75s - loss: 4.5100 - acc: 0.05 - ETA: 75s - loss: 4.5097 - acc: 0.05 - ETA: 75s - loss: 4.5069 - acc: 0.05 - ETA: 75s - loss: 4.5102 - acc: 0.05 - ETA: 74s - loss: 4.5133 - acc: 0.05 - ETA: 74s - loss: 4.5153 - acc: 0.05 - ETA: 74s - loss: 4.5180 - acc: 0.05 - ETA: 73s - loss: 4.5191 - acc: 0.05 - ETA: 73s - loss: 4.5185 - acc: 0.05 - ETA: 73s - loss: 4.5174 - acc: 0.05 - ETA: 72s - loss: 4.5157 - acc: 0.05 - ETA: 72s - loss: 4.5142 - acc: 0.05 - ETA: 72s - loss: 4.5139 - acc: 0.05 - ETA: 72s - loss: 4.5162 - acc: 0.05 - ETA: 71s - loss: 4.5185 - acc: 0.04 - ETA: 71s - loss: 4.5163 - acc: 0.04 - ETA: 71s - loss: 4.5125 - acc: 0.05 - ETA: 70s - loss: 4.5118 - acc: 0.04 - ETA: 70s - loss: 4.5091 - acc: 0.05 - ETA: 70s - loss: 4.5083 - acc: 0.04 - ETA: 69s - loss: 4.5092 - acc: 0.04 - ETA: 69s - loss: 4.5114 - acc: 0.04 - ETA: 69s - loss: 4.5099 - acc: 0.04 - ETA: 69s - loss: 4.5103 - acc: 0.04 - ETA: 68s - loss: 4.5119 - acc: 0.04 - ETA: 68s - loss: 4.5132 - acc: 0.04 - ETA: 68s - loss: 4.5133 - acc: 0.04 - ETA: 67s - loss: 4.5102 - acc: 0.04 - ETA: 67s - loss: 4.5103 - acc: 0.04 - ETA: 67s - loss: 4.5111 - acc: 0.04 - ETA: 67s - loss: 4.5093 - acc: 0.04 - ETA: 66s - loss: 4.5111 - acc: 0.04 - ETA: 66s - loss: 4.5117 - acc: 0.04 - ETA: 66s - loss: 4.5155 - acc: 0.04 - ETA: 65s - loss: 4.5159 - acc: 0.04 - ETA: 65s - loss: 4.5197 - acc: 0.04 - ETA: 65s - loss: 4.5201 - acc: 0.04 - ETA: 64s - loss: 4.5199 - acc: 0.04 - ETA: 64s - loss: 4.5199 - acc: 0.04 - ETA: 64s - loss: 4.5213 - acc: 0.04 - ETA: 64s - loss: 4.5196 - acc: 0.04 - ETA: 63s - loss: 4.5207 - acc: 0.04 - ETA: 63s - loss: 4.5196 - acc: 0.04 - ETA: 63s - loss: 4.5198 - acc: 0.04 - ETA: 62s - loss: 4.5206 - acc: 0.04 - ETA: 62s - loss: 4.5217 - acc: 0.04 - ETA: 62s - loss: 4.5224 - acc: 0.04 - ETA: 61s - loss: 4.5206 - acc: 0.04 - ETA: 61s - loss: 4.5215 - acc: 0.04 - ETA: 61s - loss: 4.5214 - acc: 0.04 - ETA: 61s - loss: 4.5201 - acc: 0.04 - ETA: 60s - loss: 4.5216 - acc: 0.04 - ETA: 60s - loss: 4.5222 - acc: 0.04 - ETA: 60s - loss: 4.5221 - acc: 0.04 - ETA: 59s - loss: 4.5210 - acc: 0.04 - ETA: 59s - loss: 4.5230 - acc: 0.04 - ETA: 59s - loss: 4.5241 - acc: 0.04 - ETA: 58s - loss: 4.5252 - acc: 0.04 - ETA: 58s - loss: 4.5268 - acc: 0.04 - ETA: 58s - loss: 4.5247 - acc: 0.04 - ETA: 58s - loss: 4.5241 - acc: 0.04 - ETA: 57s - loss: 4.5239 - acc: 0.04 - ETA: 57s - loss: 4.5223 - acc: 0.04 - ETA: 57s - loss: 4.5221 - acc: 0.04 - ETA: 56s - loss: 4.5225 - acc: 0.04 - ETA: 56s - loss: 4.5219 - acc: 0.04 - ETA: 56s - loss: 4.5208 - acc: 0.04 - ETA: 55s - loss: 4.5210 - acc: 0.04 - ETA: 55s - loss: 4.5210 - acc: 0.04 - ETA: 55s - loss: 4.5205 - acc: 0.04 - ETA: 55s - loss: 4.5184 - acc: 0.04 - ETA: 54s - loss: 4.5175 - acc: 0.04 - ETA: 54s - loss: 4.5197 - acc: 0.04 - ETA: 54s - loss: 4.5199 - acc: 0.04 - ETA: 53s - loss: 4.5191 - acc: 0.04 - ETA: 53s - loss: 4.5208 - acc: 0.04 - ETA: 53s - loss: 4.5195 - acc: 0.04 - ETA: 53s - loss: 4.5189 - acc: 0.04 - ETA: 52s - loss: 4.5199 - acc: 0.04 - ETA: 52s - loss: 4.5203 - acc: 0.04 - ETA: 52s - loss: 4.5217 - acc: 0.04 - ETA: 51s - loss: 4.5221 - acc: 0.04 - ETA: 51s - loss: 4.5216 - acc: 0.04 - ETA: 51s - loss: 4.5207 - acc: 0.04 - ETA: 50s - loss: 4.5203 - acc: 0.04 - ETA: 50s - loss: 4.5208 - acc: 0.04 - ETA: 50s - loss: 4.5187 - acc: 0.04 - ETA: 50s - loss: 4.5178 - acc: 0.04 - ETA: 49s - loss: 4.5189 - acc: 0.04 - ETA: 49s - loss: 4.5193 - acc: 0.04 - ETA: 49s - loss: 4.5197 - acc: 0.04 - ETA: 48s - loss: 4.5200 - acc: 0.04 - ETA: 48s - loss: 4.5190 - acc: 0.04 - ETA: 48s - loss: 4.5192 - acc: 0.04 - ETA: 47s - loss: 4.5189 - acc: 0.04 - ETA: 47s - loss: 4.5178 - acc: 0.04 - ETA: 47s - loss: 4.5181 - acc: 0.04 - ETA: 47s - loss: 4.5183 - acc: 0.04 - ETA: 46s - loss: 4.5190 - acc: 0.04 - ETA: 46s - loss: 4.5176 - acc: 0.04 - ETA: 46s - loss: 4.5170 - acc: 0.04 - ETA: 45s - loss: 4.5168 - acc: 0.04 - ETA: 45s - loss: 4.5167 - acc: 0.04 - ETA: 45s - loss: 4.5163 - acc: 0.04 - ETA: 44s - loss: 4.5161 - acc: 0.04 - ETA: 44s - loss: 4.5145 - acc: 0.04 - ETA: 44s - loss: 4.5126 - acc: 0.04 - ETA: 44s - loss: 4.5140 - acc: 0.04 - ETA: 43s - loss: 4.5119 - acc: 0.04 - ETA: 43s - loss: 4.5133 - acc: 0.04 - ETA: 43s - loss: 4.5126 - acc: 0.04 - ETA: 42s - loss: 4.5115 - acc: 0.04 - ETA: 42s - loss: 4.5119 - acc: 0.04 - ETA: 42s - loss: 4.5118 - acc: 0.04 - ETA: 41s - loss: 4.5097 - acc: 0.04 - ETA: 41s - loss: 4.5098 - acc: 0.04 - ETA: 41s - loss: 4.5107 - acc: 0.04 - ETA: 41s - loss: 4.5107 - acc: 0.04 - ETA: 40s - loss: 4.5110 - acc: 0.04 - ETA: 40s - loss: 4.5106 - acc: 0.04 - ETA: 40s - loss: 4.5115 - acc: 0.04 - ETA: 39s - loss: 4.5116 - acc: 0.04 - ETA: 39s - loss: 4.5109 - acc: 0.04 - ETA: 39s - loss: 4.5142 - acc: 0.04 - ETA: 38s - loss: 4.5139 - acc: 0.04 - ETA: 38s - loss: 4.5135 - acc: 0.04 - ETA: 38s - loss: 4.5126 - acc: 0.04 - ETA: 38s - loss: 4.5113 - acc: 0.04 - ETA: 37s - loss: 4.5102 - acc: 0.04 - ETA: 37s - loss: 4.5096 - acc: 0.04 - ETA: 37s - loss: 4.5099 - acc: 0.04 - ETA: 36s - loss: 4.5096 - acc: 0.04 - ETA: 36s - loss: 4.5087 - acc: 0.04 - ETA: 36s - loss: 4.5071 - acc: 0.04 - ETA: 35s - loss: 4.5084 - acc: 0.04 - ETA: 35s - loss: 4.5094 - acc: 0.0470"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 35s - loss: 4.5106 - acc: 0.04 - ETA: 35s - loss: 4.5100 - acc: 0.04 - ETA: 34s - loss: 4.5095 - acc: 0.04 - ETA: 34s - loss: 4.5113 - acc: 0.04 - ETA: 34s - loss: 4.5115 - acc: 0.04 - ETA: 33s - loss: 4.5113 - acc: 0.04 - ETA: 33s - loss: 4.5107 - acc: 0.04 - ETA: 33s - loss: 4.5090 - acc: 0.04 - ETA: 32s - loss: 4.5092 - acc: 0.04 - ETA: 32s - loss: 4.5090 - acc: 0.04 - ETA: 32s - loss: 4.5094 - acc: 0.04 - ETA: 32s - loss: 4.5097 - acc: 0.04 - ETA: 31s - loss: 4.5101 - acc: 0.04 - ETA: 31s - loss: 4.5098 - acc: 0.04 - ETA: 31s - loss: 4.5112 - acc: 0.04 - ETA: 30s - loss: 4.5109 - acc: 0.04 - ETA: 30s - loss: 4.5119 - acc: 0.04 - ETA: 30s - loss: 4.5116 - acc: 0.04 - ETA: 29s - loss: 4.5120 - acc: 0.04 - ETA: 29s - loss: 4.5122 - acc: 0.04 - ETA: 29s - loss: 4.5114 - acc: 0.04 - ETA: 29s - loss: 4.5111 - acc: 0.04 - ETA: 28s - loss: 4.5128 - acc: 0.04 - ETA: 28s - loss: 4.5130 - acc: 0.04 - ETA: 28s - loss: 4.5132 - acc: 0.04 - ETA: 27s - loss: 4.5131 - acc: 0.04 - ETA: 27s - loss: 4.5130 - acc: 0.04 - ETA: 27s - loss: 4.5135 - acc: 0.04 - ETA: 26s - loss: 4.5123 - acc: 0.04 - ETA: 26s - loss: 4.5133 - acc: 0.04 - ETA: 26s - loss: 4.5125 - acc: 0.04 - ETA: 26s - loss: 4.5125 - acc: 0.04 - ETA: 25s - loss: 4.5133 - acc: 0.04 - ETA: 25s - loss: 4.5129 - acc: 0.04 - ETA: 25s - loss: 4.5127 - acc: 0.04 - ETA: 24s - loss: 4.5126 - acc: 0.04 - ETA: 24s - loss: 4.5112 - acc: 0.04 - ETA: 24s - loss: 4.5110 - acc: 0.04 - ETA: 23s - loss: 4.5109 - acc: 0.04 - ETA: 23s - loss: 4.5109 - acc: 0.04 - ETA: 23s - loss: 4.5098 - acc: 0.04 - ETA: 23s - loss: 4.5089 - acc: 0.04 - ETA: 22s - loss: 4.5092 - acc: 0.04 - ETA: 22s - loss: 4.5093 - acc: 0.04 - ETA: 22s - loss: 4.5094 - acc: 0.04 - ETA: 21s - loss: 4.5081 - acc: 0.04 - ETA: 21s - loss: 4.5065 - acc: 0.04 - ETA: 21s - loss: 4.5071 - acc: 0.04 - ETA: 21s - loss: 4.5067 - acc: 0.04 - ETA: 20s - loss: 4.5059 - acc: 0.04 - ETA: 20s - loss: 4.5058 - acc: 0.04 - ETA: 20s - loss: 4.5062 - acc: 0.04 - ETA: 19s - loss: 4.5063 - acc: 0.04 - ETA: 19s - loss: 4.5062 - acc: 0.04 - ETA: 19s - loss: 4.5071 - acc: 0.04 - ETA: 18s - loss: 4.5077 - acc: 0.04 - ETA: 18s - loss: 4.5082 - acc: 0.04 - ETA: 18s - loss: 4.5085 - acc: 0.04 - ETA: 17s - loss: 4.5084 - acc: 0.04 - ETA: 17s - loss: 4.5088 - acc: 0.04 - ETA: 17s - loss: 4.5077 - acc: 0.04 - ETA: 17s - loss: 4.5076 - acc: 0.04 - ETA: 16s - loss: 4.5068 - acc: 0.04 - ETA: 16s - loss: 4.5075 - acc: 0.04 - ETA: 16s - loss: 4.5065 - acc: 0.04 - ETA: 15s - loss: 4.5067 - acc: 0.04 - ETA: 15s - loss: 4.5068 - acc: 0.04 - ETA: 15s - loss: 4.5069 - acc: 0.04 - ETA: 15s - loss: 4.5062 - acc: 0.04 - ETA: 14s - loss: 4.5065 - acc: 0.04 - ETA: 14s - loss: 4.5075 - acc: 0.04 - ETA: 14s - loss: 4.5089 - acc: 0.04 - ETA: 13s - loss: 4.5084 - acc: 0.04 - ETA: 13s - loss: 4.5085 - acc: 0.04 - ETA: 13s - loss: 4.5098 - acc: 0.04 - ETA: 12s - loss: 4.5105 - acc: 0.04 - ETA: 12s - loss: 4.5096 - acc: 0.04 - ETA: 12s - loss: 4.5105 - acc: 0.04 - ETA: 12s - loss: 4.5107 - acc: 0.04 - ETA: 11s - loss: 4.5105 - acc: 0.04 - ETA: 11s - loss: 4.5101 - acc: 0.04 - ETA: 11s - loss: 4.5103 - acc: 0.04 - ETA: 10s - loss: 4.5109 - acc: 0.04 - ETA: 10s - loss: 4.5103 - acc: 0.04 - ETA: 10s - loss: 4.5096 - acc: 0.04 - ETA: 9s - loss: 4.5097 - acc: 0.0467 - ETA: 9s - loss: 4.5101 - acc: 0.046 - ETA: 9s - loss: 4.5102 - acc: 0.046 - ETA: 9s - loss: 4.5100 - acc: 0.046 - ETA: 8s - loss: 4.5094 - acc: 0.046 - ETA: 8s - loss: 4.5098 - acc: 0.046 - ETA: 8s - loss: 4.5083 - acc: 0.046 - ETA: 7s - loss: 4.5093 - acc: 0.046 - ETA: 7s - loss: 4.5098 - acc: 0.046 - ETA: 7s - loss: 4.5105 - acc: 0.046 - ETA: 6s - loss: 4.5099 - acc: 0.046 - ETA: 6s - loss: 4.5087 - acc: 0.047 - ETA: 6s - loss: 4.5090 - acc: 0.047 - ETA: 6s - loss: 4.5094 - acc: 0.047 - ETA: 5s - loss: 4.5092 - acc: 0.047 - ETA: 5s - loss: 4.5103 - acc: 0.047 - ETA: 5s - loss: 4.5094 - acc: 0.047 - ETA: 4s - loss: 4.5105 - acc: 0.047 - ETA: 4s - loss: 4.5106 - acc: 0.047 - ETA: 4s - loss: 4.5108 - acc: 0.047 - ETA: 3s - loss: 4.5108 - acc: 0.047 - ETA: 3s - loss: 4.5108 - acc: 0.047 - ETA: 3s - loss: 4.5101 - acc: 0.047 - ETA: 3s - loss: 4.5094 - acc: 0.047 - ETA: 2s - loss: 4.5091 - acc: 0.047 - ETA: 2s - loss: 4.5098 - acc: 0.047 - ETA: 2s - loss: 4.5096 - acc: 0.047 - ETA: 1s - loss: 4.5095 - acc: 0.047 - ETA: 1s - loss: 4.5093 - acc: 0.047 - ETA: 1s - loss: 4.5084 - acc: 0.047 - ETA: 0s - loss: 4.5082 - acc: 0.047 - ETA: 0s - loss: 4.5075 - acc: 0.047 - ETA: 0s - loss: 4.5081 - acc: 0.0476Epoch 00015: val_loss improved from 4.62776 to 4.61766, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 102s - loss: 4.5085 - acc: 0.0475 - val_loss: 4.6177 - val_acc: 0.0323\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 100s - loss: 4.4488 - acc: 0.0000e+0 - ETA: 99s - loss: 4.3821 - acc: 0.0000e+0 - ETA: 99s - loss: 4.3278 - acc: 0.0333   - ETA: 99s - loss: 4.3319 - acc: 0.03 - ETA: 99s - loss: 4.3134 - acc: 0.04 - ETA: 98s - loss: 4.3190 - acc: 0.06 - ETA: 98s - loss: 4.3509 - acc: 0.05 - ETA: 98s - loss: 4.3644 - acc: 0.05 - ETA: 97s - loss: 4.3615 - acc: 0.05 - ETA: 97s - loss: 4.3583 - acc: 0.05 - ETA: 97s - loss: 4.3543 - acc: 0.05 - ETA: 96s - loss: 4.3527 - acc: 0.05 - ETA: 96s - loss: 4.3650 - acc: 0.06 - ETA: 96s - loss: 4.3631 - acc: 0.06 - ETA: 95s - loss: 4.3718 - acc: 0.06 - ETA: 95s - loss: 4.3973 - acc: 0.05 - ETA: 95s - loss: 4.3993 - acc: 0.05 - ETA: 94s - loss: 4.4021 - acc: 0.05 - ETA: 94s - loss: 4.3964 - acc: 0.05 - ETA: 94s - loss: 4.4003 - acc: 0.05 - ETA: 93s - loss: 4.4178 - acc: 0.05 - ETA: 93s - loss: 4.4218 - acc: 0.04 - ETA: 93s - loss: 4.4276 - acc: 0.04 - ETA: 93s - loss: 4.4329 - acc: 0.04 - ETA: 92s - loss: 4.4392 - acc: 0.04 - ETA: 92s - loss: 4.4407 - acc: 0.04 - ETA: 92s - loss: 4.4300 - acc: 0.04 - ETA: 91s - loss: 4.4423 - acc: 0.04 - ETA: 91s - loss: 4.4442 - acc: 0.04 - ETA: 91s - loss: 4.4457 - acc: 0.05 - ETA: 90s - loss: 4.4395 - acc: 0.05 - ETA: 90s - loss: 4.4424 - acc: 0.05 - ETA: 90s - loss: 4.4471 - acc: 0.05 - ETA: 90s - loss: 4.4499 - acc: 0.04 - ETA: 89s - loss: 4.4405 - acc: 0.05 - ETA: 89s - loss: 4.4374 - acc: 0.05 - ETA: 89s - loss: 4.4411 - acc: 0.05 - ETA: 88s - loss: 4.4421 - acc: 0.05 - ETA: 88s - loss: 4.4438 - acc: 0.05 - ETA: 88s - loss: 4.4528 - acc: 0.05 - ETA: 88s - loss: 4.4511 - acc: 0.05 - ETA: 87s - loss: 4.4540 - acc: 0.05 - ETA: 87s - loss: 4.4506 - acc: 0.05 - ETA: 87s - loss: 4.4534 - acc: 0.05 - ETA: 86s - loss: 4.4571 - acc: 0.05 - ETA: 86s - loss: 4.4629 - acc: 0.05 - ETA: 86s - loss: 4.4656 - acc: 0.05 - ETA: 85s - loss: 4.4617 - acc: 0.05 - ETA: 85s - loss: 4.4591 - acc: 0.05 - ETA: 85s - loss: 4.4602 - acc: 0.05 - ETA: 85s - loss: 4.4669 - acc: 0.05 - ETA: 84s - loss: 4.4637 - acc: 0.05 - ETA: 84s - loss: 4.4583 - acc: 0.05 - ETA: 84s - loss: 4.4584 - acc: 0.05 - ETA: 83s - loss: 4.4552 - acc: 0.05 - ETA: 83s - loss: 4.4592 - acc: 0.05 - ETA: 83s - loss: 4.4576 - acc: 0.05 - ETA: 82s - loss: 4.4562 - acc: 0.05 - ETA: 82s - loss: 4.4534 - acc: 0.05 - ETA: 82s - loss: 4.4542 - acc: 0.05 - ETA: 82s - loss: 4.4520 - acc: 0.05 - ETA: 81s - loss: 4.4506 - acc: 0.05 - ETA: 81s - loss: 4.4528 - acc: 0.05 - ETA: 81s - loss: 4.4501 - acc: 0.05 - ETA: 80s - loss: 4.4515 - acc: 0.05 - ETA: 80s - loss: 4.4511 - acc: 0.05 - ETA: 80s - loss: 4.4511 - acc: 0.05 - ETA: 80s - loss: 4.4483 - acc: 0.05 - ETA: 79s - loss: 4.4484 - acc: 0.05 - ETA: 79s - loss: 4.4512 - acc: 0.05 - ETA: 79s - loss: 4.4477 - acc: 0.05 - ETA: 78s - loss: 4.4481 - acc: 0.05 - ETA: 78s - loss: 4.4460 - acc: 0.05 - ETA: 78s - loss: 4.4400 - acc: 0.05 - ETA: 77s - loss: 4.4404 - acc: 0.05 - ETA: 77s - loss: 4.4391 - acc: 0.05 - ETA: 77s - loss: 4.4444 - acc: 0.05 - ETA: 77s - loss: 4.4421 - acc: 0.05 - ETA: 76s - loss: 4.4415 - acc: 0.05 - ETA: 76s - loss: 4.4386 - acc: 0.05 - ETA: 76s - loss: 4.4419 - acc: 0.05 - ETA: 75s - loss: 4.4403 - acc: 0.05 - ETA: 75s - loss: 4.4424 - acc: 0.05 - ETA: 75s - loss: 4.4442 - acc: 0.04 - ETA: 74s - loss: 4.4463 - acc: 0.04 - ETA: 74s - loss: 4.4419 - acc: 0.05 - ETA: 74s - loss: 4.4381 - acc: 0.05 - ETA: 74s - loss: 4.4462 - acc: 0.05 - ETA: 73s - loss: 4.4488 - acc: 0.05 - ETA: 73s - loss: 4.4493 - acc: 0.05 - ETA: 73s - loss: 4.4492 - acc: 0.04 - ETA: 72s - loss: 4.4519 - acc: 0.04 - ETA: 72s - loss: 4.4514 - acc: 0.04 - ETA: 72s - loss: 4.4527 - acc: 0.04 - ETA: 71s - loss: 4.4493 - acc: 0.05 - ETA: 71s - loss: 4.4453 - acc: 0.05 - ETA: 71s - loss: 4.4443 - acc: 0.04 - ETA: 70s - loss: 4.4439 - acc: 0.04 - ETA: 70s - loss: 4.4492 - acc: 0.04 - ETA: 70s - loss: 4.4443 - acc: 0.04 - ETA: 70s - loss: 4.4408 - acc: 0.04 - ETA: 69s - loss: 4.4436 - acc: 0.04 - ETA: 69s - loss: 4.4465 - acc: 0.04 - ETA: 69s - loss: 4.4482 - acc: 0.04 - ETA: 68s - loss: 4.4501 - acc: 0.04 - ETA: 68s - loss: 4.4516 - acc: 0.04 - ETA: 68s - loss: 4.4522 - acc: 0.04 - ETA: 67s - loss: 4.4569 - acc: 0.04 - ETA: 67s - loss: 4.4568 - acc: 0.04 - ETA: 67s - loss: 4.4569 - acc: 0.04 - ETA: 67s - loss: 4.4554 - acc: 0.04 - ETA: 66s - loss: 4.4580 - acc: 0.04 - ETA: 66s - loss: 4.4583 - acc: 0.05 - ETA: 66s - loss: 4.4623 - acc: 0.04 - ETA: 65s - loss: 4.4605 - acc: 0.05 - ETA: 65s - loss: 4.4636 - acc: 0.05 - ETA: 65s - loss: 4.4635 - acc: 0.05 - ETA: 64s - loss: 4.4620 - acc: 0.05 - ETA: 64s - loss: 4.4636 - acc: 0.05 - ETA: 64s - loss: 4.4622 - acc: 0.05 - ETA: 64s - loss: 4.4632 - acc: 0.05 - ETA: 63s - loss: 4.4621 - acc: 0.05 - ETA: 63s - loss: 4.4636 - acc: 0.05 - ETA: 63s - loss: 4.4654 - acc: 0.05 - ETA: 62s - loss: 4.4648 - acc: 0.05 - ETA: 62s - loss: 4.4665 - acc: 0.05 - ETA: 62s - loss: 4.4681 - acc: 0.05 - ETA: 61s - loss: 4.4693 - acc: 0.05 - ETA: 61s - loss: 4.4698 - acc: 0.05 - ETA: 61s - loss: 4.4717 - acc: 0.05 - ETA: 61s - loss: 4.4743 - acc: 0.05 - ETA: 60s - loss: 4.4744 - acc: 0.05 - ETA: 60s - loss: 4.4732 - acc: 0.05 - ETA: 60s - loss: 4.4728 - acc: 0.05 - ETA: 59s - loss: 4.4742 - acc: 0.05 - ETA: 59s - loss: 4.4728 - acc: 0.05 - ETA: 59s - loss: 4.4734 - acc: 0.05 - ETA: 58s - loss: 4.4741 - acc: 0.05 - ETA: 58s - loss: 4.4749 - acc: 0.05 - ETA: 58s - loss: 4.4748 - acc: 0.05 - ETA: 58s - loss: 4.4763 - acc: 0.05 - ETA: 57s - loss: 4.4775 - acc: 0.05 - ETA: 57s - loss: 4.4761 - acc: 0.05 - ETA: 57s - loss: 4.4780 - acc: 0.05 - ETA: 56s - loss: 4.4786 - acc: 0.05 - ETA: 56s - loss: 4.4786 - acc: 0.05 - ETA: 56s - loss: 4.4794 - acc: 0.05 - ETA: 55s - loss: 4.4804 - acc: 0.05 - ETA: 55s - loss: 4.4810 - acc: 0.05 - ETA: 55s - loss: 4.4810 - acc: 0.05 - ETA: 55s - loss: 4.4809 - acc: 0.05 - ETA: 54s - loss: 4.4845 - acc: 0.05 - ETA: 54s - loss: 4.4854 - acc: 0.05 - ETA: 54s - loss: 4.4837 - acc: 0.05 - ETA: 53s - loss: 4.4829 - acc: 0.05 - ETA: 53s - loss: 4.4825 - acc: 0.05 - ETA: 53s - loss: 4.4833 - acc: 0.04 - ETA: 52s - loss: 4.4836 - acc: 0.04 - ETA: 52s - loss: 4.4827 - acc: 0.04 - ETA: 52s - loss: 4.4830 - acc: 0.04 - ETA: 51s - loss: 4.4827 - acc: 0.04 - ETA: 51s - loss: 4.4842 - acc: 0.04 - ETA: 51s - loss: 4.4850 - acc: 0.04 - ETA: 51s - loss: 4.4865 - acc: 0.04 - ETA: 50s - loss: 4.4873 - acc: 0.04 - ETA: 50s - loss: 4.4881 - acc: 0.04 - ETA: 50s - loss: 4.4870 - acc: 0.04 - ETA: 49s - loss: 4.4870 - acc: 0.04 - ETA: 49s - loss: 4.4897 - acc: 0.04 - ETA: 49s - loss: 4.4901 - acc: 0.04 - ETA: 48s - loss: 4.4915 - acc: 0.04 - ETA: 48s - loss: 4.4921 - acc: 0.04 - ETA: 48s - loss: 4.4928 - acc: 0.04 - ETA: 48s - loss: 4.4924 - acc: 0.04 - ETA: 47s - loss: 4.4949 - acc: 0.04 - ETA: 47s - loss: 4.4950 - acc: 0.04 - ETA: 47s - loss: 4.4932 - acc: 0.04 - ETA: 46s - loss: 4.4916 - acc: 0.04 - ETA: 46s - loss: 4.4942 - acc: 0.04 - ETA: 46s - loss: 4.4936 - acc: 0.04 - ETA: 45s - loss: 4.4934 - acc: 0.04 - ETA: 45s - loss: 4.4929 - acc: 0.04 - ETA: 45s - loss: 4.4942 - acc: 0.04 - ETA: 45s - loss: 4.4948 - acc: 0.04 - ETA: 44s - loss: 4.4940 - acc: 0.04 - ETA: 44s - loss: 4.4941 - acc: 0.04 - ETA: 44s - loss: 4.4938 - acc: 0.04 - ETA: 43s - loss: 4.4953 - acc: 0.04 - ETA: 43s - loss: 4.4959 - acc: 0.04 - ETA: 43s - loss: 4.4947 - acc: 0.04 - ETA: 42s - loss: 4.4952 - acc: 0.04 - ETA: 42s - loss: 4.4932 - acc: 0.04 - ETA: 42s - loss: 4.4919 - acc: 0.04 - ETA: 42s - loss: 4.4918 - acc: 0.04 - ETA: 41s - loss: 4.4919 - acc: 0.04 - ETA: 41s - loss: 4.4921 - acc: 0.04 - ETA: 41s - loss: 4.4933 - acc: 0.04 - ETA: 40s - loss: 4.4940 - acc: 0.04 - ETA: 40s - loss: 4.4935 - acc: 0.04 - ETA: 40s - loss: 4.4929 - acc: 0.04 - ETA: 39s - loss: 4.4918 - acc: 0.04 - ETA: 39s - loss: 4.4923 - acc: 0.04 - ETA: 39s - loss: 4.4921 - acc: 0.04 - ETA: 39s - loss: 4.4933 - acc: 0.04 - ETA: 38s - loss: 4.4933 - acc: 0.04 - ETA: 38s - loss: 4.4935 - acc: 0.04 - ETA: 38s - loss: 4.4922 - acc: 0.04 - ETA: 37s - loss: 4.4939 - acc: 0.04 - ETA: 37s - loss: 4.4937 - acc: 0.04 - ETA: 37s - loss: 4.4933 - acc: 0.04 - ETA: 36s - loss: 4.4931 - acc: 0.04 - ETA: 36s - loss: 4.4934 - acc: 0.04 - ETA: 36s - loss: 4.4927 - acc: 0.04 - ETA: 36s - loss: 4.4921 - acc: 0.04 - ETA: 35s - loss: 4.4934 - acc: 0.04676660/6680 [============================>.] - ETA: 35s - loss: 4.4947 - acc: 0.04 - ETA: 35s - loss: 4.4953 - acc: 0.04 - ETA: 34s - loss: 4.4945 - acc: 0.04 - ETA: 34s - loss: 4.4956 - acc: 0.04 - ETA: 34s - loss: 4.4958 - acc: 0.04 - ETA: 33s - loss: 4.4959 - acc: 0.04 - ETA: 33s - loss: 4.4950 - acc: 0.04 - ETA: 33s - loss: 4.4957 - acc: 0.04 - ETA: 33s - loss: 4.4970 - acc: 0.04 - ETA: 32s - loss: 4.4958 - acc: 0.04 - ETA: 32s - loss: 4.4949 - acc: 0.04 - ETA: 32s - loss: 4.4953 - acc: 0.04 - ETA: 31s - loss: 4.4941 - acc: 0.04 - ETA: 31s - loss: 4.4919 - acc: 0.04 - ETA: 31s - loss: 4.4925 - acc: 0.04 - ETA: 30s - loss: 4.4922 - acc: 0.04 - ETA: 30s - loss: 4.4920 - acc: 0.04 - ETA: 30s - loss: 4.4944 - acc: 0.04 - ETA: 30s - loss: 4.4947 - acc: 0.04 - ETA: 29s - loss: 4.4949 - acc: 0.04 - ETA: 29s - loss: 4.4968 - acc: 0.04 - ETA: 29s - loss: 4.4973 - acc: 0.04 - ETA: 28s - loss: 4.4972 - acc: 0.04 - ETA: 28s - loss: 4.4980 - acc: 0.04 - ETA: 28s - loss: 4.4974 - acc: 0.04 - ETA: 27s - loss: 4.4979 - acc: 0.04 - ETA: 27s - loss: 4.4981 - acc: 0.04 - ETA: 27s - loss: 4.4987 - acc: 0.04 - ETA: 27s - loss: 4.4982 - acc: 0.04 - ETA: 26s - loss: 4.4973 - acc: 0.04 - ETA: 26s - loss: 4.4978 - acc: 0.04 - ETA: 26s - loss: 4.4975 - acc: 0.04 - ETA: 25s - loss: 4.4969 - acc: 0.04 - ETA: 25s - loss: 4.4971 - acc: 0.04 - ETA: 25s - loss: 4.4971 - acc: 0.04 - ETA: 24s - loss: 4.4959 - acc: 0.04 - ETA: 24s - loss: 4.4958 - acc: 0.04 - ETA: 24s - loss: 4.4954 - acc: 0.04 - ETA: 24s - loss: 4.4957 - acc: 0.04 - ETA: 23s - loss: 4.4964 - acc: 0.04 - ETA: 23s - loss: 4.4955 - acc: 0.04 - ETA: 23s - loss: 4.4967 - acc: 0.04 - ETA: 22s - loss: 4.4974 - acc: 0.04 - ETA: 22s - loss: 4.4970 - acc: 0.04 - ETA: 22s - loss: 4.4963 - acc: 0.04 - ETA: 21s - loss: 4.4967 - acc: 0.04 - ETA: 21s - loss: 4.4964 - acc: 0.04 - ETA: 21s - loss: 4.4962 - acc: 0.04 - ETA: 21s - loss: 4.4972 - acc: 0.04 - ETA: 20s - loss: 4.4971 - acc: 0.04 - ETA: 20s - loss: 4.4976 - acc: 0.04 - ETA: 20s - loss: 4.4981 - acc: 0.04 - ETA: 19s - loss: 4.4972 - acc: 0.04 - ETA: 19s - loss: 4.4963 - acc: 0.04 - ETA: 19s - loss: 4.4958 - acc: 0.04 - ETA: 18s - loss: 4.4964 - acc: 0.04 - ETA: 18s - loss: 4.4960 - acc: 0.04 - ETA: 18s - loss: 4.4960 - acc: 0.04 - ETA: 18s - loss: 4.4960 - acc: 0.04 - ETA: 17s - loss: 4.4944 - acc: 0.04 - ETA: 17s - loss: 4.4939 - acc: 0.04 - ETA: 17s - loss: 4.4945 - acc: 0.04 - ETA: 16s - loss: 4.4939 - acc: 0.04 - ETA: 16s - loss: 4.4930 - acc: 0.04 - ETA: 16s - loss: 4.4925 - acc: 0.04 - ETA: 15s - loss: 4.4927 - acc: 0.04 - ETA: 15s - loss: 4.4919 - acc: 0.04 - ETA: 15s - loss: 4.4914 - acc: 0.04 - ETA: 15s - loss: 4.4894 - acc: 0.04 - ETA: 14s - loss: 4.4897 - acc: 0.04 - ETA: 14s - loss: 4.4895 - acc: 0.04 - ETA: 14s - loss: 4.4898 - acc: 0.04 - ETA: 13s - loss: 4.4898 - acc: 0.04 - ETA: 13s - loss: 4.4890 - acc: 0.04 - ETA: 13s - loss: 4.4894 - acc: 0.04 - ETA: 12s - loss: 4.4905 - acc: 0.04 - ETA: 12s - loss: 4.4909 - acc: 0.04 - ETA: 12s - loss: 4.4923 - acc: 0.04 - ETA: 12s - loss: 4.4923 - acc: 0.04 - ETA: 11s - loss: 4.4923 - acc: 0.04 - ETA: 11s - loss: 4.4932 - acc: 0.04 - ETA: 11s - loss: 4.4929 - acc: 0.04 - ETA: 10s - loss: 4.4916 - acc: 0.04 - ETA: 10s - loss: 4.4915 - acc: 0.04 - ETA: 10s - loss: 4.4909 - acc: 0.04 - ETA: 9s - loss: 4.4902 - acc: 0.0468 - ETA: 9s - loss: 4.4896 - acc: 0.047 - ETA: 9s - loss: 4.4899 - acc: 0.047 - ETA: 9s - loss: 4.4899 - acc: 0.046 - ETA: 8s - loss: 4.4898 - acc: 0.046 - ETA: 8s - loss: 4.4892 - acc: 0.046 - ETA: 8s - loss: 4.4892 - acc: 0.046 - ETA: 7s - loss: 4.4896 - acc: 0.046 - ETA: 7s - loss: 4.4892 - acc: 0.046 - ETA: 7s - loss: 4.4892 - acc: 0.046 - ETA: 6s - loss: 4.4889 - acc: 0.046 - ETA: 6s - loss: 4.4896 - acc: 0.046 - ETA: 6s - loss: 4.4896 - acc: 0.046 - ETA: 6s - loss: 4.4897 - acc: 0.047 - ETA: 5s - loss: 4.4906 - acc: 0.047 - ETA: 5s - loss: 4.4915 - acc: 0.047 - ETA: 5s - loss: 4.4915 - acc: 0.047 - ETA: 4s - loss: 4.4910 - acc: 0.047 - ETA: 4s - loss: 4.4909 - acc: 0.047 - ETA: 4s - loss: 4.4908 - acc: 0.047 - ETA: 3s - loss: 4.4906 - acc: 0.047 - ETA: 3s - loss: 4.4905 - acc: 0.047 - ETA: 3s - loss: 4.4903 - acc: 0.047 - ETA: 3s - loss: 4.4894 - acc: 0.047 - ETA: 2s - loss: 4.4892 - acc: 0.048 - ETA: 2s - loss: 4.4887 - acc: 0.048 - ETA: 2s - loss: 4.4900 - acc: 0.047 - ETA: 1s - loss: 4.4908 - acc: 0.047 - ETA: 1s - loss: 4.4916 - acc: 0.047 - ETA: 1s - loss: 4.4908 - acc: 0.047 - ETA: 0s - loss: 4.4906 - acc: 0.047 - ETA: 0s - loss: 4.4910 - acc: 0.047 - ETA: 0s - loss: 4.4913 - acc: 0.0476Epoch 00016: val_loss improved from 4.61766 to 4.57852, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 102s - loss: 4.4917 - acc: 0.0475 - val_loss: 4.5785 - val_acc: 0.0371\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 98s - loss: 4.7251 - acc: 0.0000e+ - ETA: 98s - loss: 4.5297 - acc: 0.0500   - ETA: 98s - loss: 4.4238 - acc: 0.08 - ETA: 98s - loss: 4.3937 - acc: 0.07 - ETA: 98s - loss: 4.3265 - acc: 0.08 - ETA: 98s - loss: 4.3941 - acc: 0.06 - ETA: 98s - loss: 4.3686 - acc: 0.06 - ETA: 97s - loss: 4.4014 - acc: 0.06 - ETA: 97s - loss: 4.4627 - acc: 0.05 - ETA: 97s - loss: 4.4671 - acc: 0.06 - ETA: 96s - loss: 4.4588 - acc: 0.05 - ETA: 96s - loss: 4.4615 - acc: 0.05 - ETA: 96s - loss: 4.4874 - acc: 0.05 - ETA: 96s - loss: 4.4843 - acc: 0.05 - ETA: 95s - loss: 4.4893 - acc: 0.05 - ETA: 95s - loss: 4.4761 - acc: 0.05 - ETA: 95s - loss: 4.4759 - acc: 0.05 - ETA: 94s - loss: 4.4809 - acc: 0.05 - ETA: 94s - loss: 4.4739 - acc: 0.05 - ETA: 94s - loss: 4.4651 - acc: 0.05 - ETA: 93s - loss: 4.4583 - acc: 0.05 - ETA: 93s - loss: 4.4685 - acc: 0.05 - ETA: 93s - loss: 4.4628 - acc: 0.04 - ETA: 93s - loss: 4.4576 - acc: 0.04 - ETA: 92s - loss: 4.4611 - acc: 0.05 - ETA: 92s - loss: 4.4544 - acc: 0.05 - ETA: 92s - loss: 4.4572 - acc: 0.05 - ETA: 91s - loss: 4.4671 - acc: 0.04 - ETA: 91s - loss: 4.4632 - acc: 0.05 - ETA: 91s - loss: 4.4634 - acc: 0.05 - ETA: 90s - loss: 4.4729 - acc: 0.05 - ETA: 90s - loss: 4.4660 - acc: 0.05 - ETA: 90s - loss: 4.4607 - acc: 0.05 - ETA: 90s - loss: 4.4550 - acc: 0.05 - ETA: 89s - loss: 4.4627 - acc: 0.05 - ETA: 89s - loss: 4.4658 - acc: 0.05 - ETA: 89s - loss: 4.4584 - acc: 0.05 - ETA: 88s - loss: 4.4490 - acc: 0.05 - ETA: 88s - loss: 4.4501 - acc: 0.05 - ETA: 88s - loss: 4.4521 - acc: 0.05 - ETA: 87s - loss: 4.4505 - acc: 0.05 - ETA: 87s - loss: 4.4449 - acc: 0.05 - ETA: 87s - loss: 4.4450 - acc: 0.05 - ETA: 87s - loss: 4.4450 - acc: 0.05 - ETA: 86s - loss: 4.4476 - acc: 0.05 - ETA: 86s - loss: 4.4497 - acc: 0.05 - ETA: 86s - loss: 4.4519 - acc: 0.05 - ETA: 85s - loss: 4.4479 - acc: 0.04 - ETA: 85s - loss: 4.4507 - acc: 0.05 - ETA: 85s - loss: 4.4491 - acc: 0.04 - ETA: 84s - loss: 4.4474 - acc: 0.04 - ETA: 84s - loss: 4.4554 - acc: 0.04 - ETA: 84s - loss: 4.4554 - acc: 0.04 - ETA: 83s - loss: 4.4545 - acc: 0.04 - ETA: 83s - loss: 4.4549 - acc: 0.04 - ETA: 83s - loss: 4.4561 - acc: 0.04 - ETA: 83s - loss: 4.4575 - acc: 0.04 - ETA: 82s - loss: 4.4598 - acc: 0.04 - ETA: 82s - loss: 4.4570 - acc: 0.04 - ETA: 82s - loss: 4.4609 - acc: 0.04 - ETA: 81s - loss: 4.4606 - acc: 0.04 - ETA: 81s - loss: 4.4614 - acc: 0.04 - ETA: 81s - loss: 4.4632 - acc: 0.04 - ETA: 80s - loss: 4.4608 - acc: 0.04 - ETA: 80s - loss: 4.4578 - acc: 0.04 - ETA: 80s - loss: 4.4599 - acc: 0.04 - ETA: 80s - loss: 4.4608 - acc: 0.04 - ETA: 79s - loss: 4.4586 - acc: 0.04 - ETA: 79s - loss: 4.4594 - acc: 0.04 - ETA: 79s - loss: 4.4603 - acc: 0.04 - ETA: 78s - loss: 4.4544 - acc: 0.04 - ETA: 78s - loss: 4.4514 - acc: 0.04 - ETA: 78s - loss: 4.4526 - acc: 0.04 - ETA: 78s - loss: 4.4509 - acc: 0.04 - ETA: 77s - loss: 4.4478 - acc: 0.04 - ETA: 77s - loss: 4.4490 - acc: 0.04 - ETA: 77s - loss: 4.4497 - acc: 0.04 - ETA: 76s - loss: 4.4513 - acc: 0.04 - ETA: 76s - loss: 4.4507 - acc: 0.04 - ETA: 76s - loss: 4.4495 - acc: 0.04 - ETA: 75s - loss: 4.4519 - acc: 0.04 - ETA: 75s - loss: 4.4519 - acc: 0.04 - ETA: 75s - loss: 4.4483 - acc: 0.04 - ETA: 75s - loss: 4.4516 - acc: 0.04 - ETA: 74s - loss: 4.4502 - acc: 0.04 - ETA: 74s - loss: 4.4532 - acc: 0.04 - ETA: 74s - loss: 4.4513 - acc: 0.04 - ETA: 73s - loss: 4.4569 - acc: 0.04 - ETA: 73s - loss: 4.4571 - acc: 0.04 - ETA: 73s - loss: 4.4558 - acc: 0.04 - ETA: 72s - loss: 4.4590 - acc: 0.04 - ETA: 72s - loss: 4.4584 - acc: 0.04 - ETA: 72s - loss: 4.4584 - acc: 0.04 - ETA: 72s - loss: 4.4557 - acc: 0.04 - ETA: 71s - loss: 4.4565 - acc: 0.04 - ETA: 71s - loss: 4.4547 - acc: 0.04 - ETA: 71s - loss: 4.4533 - acc: 0.05 - ETA: 70s - loss: 4.4500 - acc: 0.04 - ETA: 70s - loss: 4.4533 - acc: 0.04 - ETA: 70s - loss: 4.4530 - acc: 0.04 - ETA: 69s - loss: 4.4527 - acc: 0.04 - ETA: 69s - loss: 4.4548 - acc: 0.04 - ETA: 69s - loss: 4.4590 - acc: 0.04 - ETA: 69s - loss: 4.4547 - acc: 0.04 - ETA: 68s - loss: 4.4577 - acc: 0.04 - ETA: 68s - loss: 4.4572 - acc: 0.04 - ETA: 68s - loss: 4.4594 - acc: 0.04 - ETA: 67s - loss: 4.4604 - acc: 0.04 - ETA: 67s - loss: 4.4600 - acc: 0.04 - ETA: 67s - loss: 4.4570 - acc: 0.04 - ETA: 66s - loss: 4.4548 - acc: 0.04 - ETA: 66s - loss: 4.4536 - acc: 0.04 - ETA: 66s - loss: 4.4546 - acc: 0.04 - ETA: 66s - loss: 4.4570 - acc: 0.04 - ETA: 65s - loss: 4.4562 - acc: 0.04 - ETA: 65s - loss: 4.4553 - acc: 0.04 - ETA: 65s - loss: 4.4565 - acc: 0.04 - ETA: 64s - loss: 4.4595 - acc: 0.04 - ETA: 64s - loss: 4.4593 - acc: 0.04 - ETA: 64s - loss: 4.4597 - acc: 0.04 - ETA: 63s - loss: 4.4580 - acc: 0.04 - ETA: 63s - loss: 4.4590 - acc: 0.04 - ETA: 63s - loss: 4.4603 - acc: 0.04 - ETA: 63s - loss: 4.4612 - acc: 0.04 - ETA: 62s - loss: 4.4605 - acc: 0.04 - ETA: 62s - loss: 4.4582 - acc: 0.04 - ETA: 62s - loss: 4.4599 - acc: 0.04 - ETA: 61s - loss: 4.4624 - acc: 0.04 - ETA: 61s - loss: 4.4629 - acc: 0.04 - ETA: 61s - loss: 4.4634 - acc: 0.04 - ETA: 60s - loss: 4.4626 - acc: 0.04 - ETA: 60s - loss: 4.4635 - acc: 0.04 - ETA: 60s - loss: 4.4625 - acc: 0.04 - ETA: 60s - loss: 4.4617 - acc: 0.04 - ETA: 59s - loss: 4.4632 - acc: 0.04 - ETA: 59s - loss: 4.4638 - acc: 0.04 - ETA: 59s - loss: 4.4630 - acc: 0.04 - ETA: 58s - loss: 4.4629 - acc: 0.04 - ETA: 58s - loss: 4.4611 - acc: 0.04 - ETA: 58s - loss: 4.4637 - acc: 0.04 - ETA: 57s - loss: 4.4649 - acc: 0.04 - ETA: 57s - loss: 4.4672 - acc: 0.04 - ETA: 57s - loss: 4.4645 - acc: 0.04 - ETA: 57s - loss: 4.4643 - acc: 0.04 - ETA: 56s - loss: 4.4637 - acc: 0.04 - ETA: 56s - loss: 4.4660 - acc: 0.04 - ETA: 56s - loss: 4.4656 - acc: 0.04 - ETA: 55s - loss: 4.4652 - acc: 0.04 - ETA: 55s - loss: 4.4680 - acc: 0.04 - ETA: 55s - loss: 4.4688 - acc: 0.04 - ETA: 54s - loss: 4.4683 - acc: 0.04 - ETA: 54s - loss: 4.4692 - acc: 0.04 - ETA: 54s - loss: 4.4699 - acc: 0.04 - ETA: 54s - loss: 4.4707 - acc: 0.04 - ETA: 53s - loss: 4.4725 - acc: 0.04 - ETA: 53s - loss: 4.4720 - acc: 0.04 - ETA: 53s - loss: 4.4707 - acc: 0.04 - ETA: 52s - loss: 4.4707 - acc: 0.04 - ETA: 52s - loss: 4.4702 - acc: 0.04 - ETA: 52s - loss: 4.4711 - acc: 0.04 - ETA: 51s - loss: 4.4722 - acc: 0.04 - ETA: 51s - loss: 4.4725 - acc: 0.04 - ETA: 51s - loss: 4.4716 - acc: 0.04 - ETA: 51s - loss: 4.4713 - acc: 0.04 - ETA: 50s - loss: 4.4701 - acc: 0.04 - ETA: 50s - loss: 4.4676 - acc: 0.04 - ETA: 50s - loss: 4.4669 - acc: 0.04 - ETA: 49s - loss: 4.4677 - acc: 0.04 - ETA: 49s - loss: 4.4676 - acc: 0.04 - ETA: 49s - loss: 4.4670 - acc: 0.04 - ETA: 48s - loss: 4.4669 - acc: 0.04 - ETA: 48s - loss: 4.4673 - acc: 0.04 - ETA: 48s - loss: 4.4673 - acc: 0.04 - ETA: 48s - loss: 4.4670 - acc: 0.04 - ETA: 47s - loss: 4.4661 - acc: 0.04 - ETA: 47s - loss: 4.4666 - acc: 0.04 - ETA: 47s - loss: 4.4700 - acc: 0.04 - ETA: 46s - loss: 4.4704 - acc: 0.04 - ETA: 46s - loss: 4.4704 - acc: 0.04 - ETA: 46s - loss: 4.4697 - acc: 0.04 - ETA: 45s - loss: 4.4690 - acc: 0.04 - ETA: 45s - loss: 4.4678 - acc: 0.04 - ETA: 45s - loss: 4.4664 - acc: 0.04 - ETA: 45s - loss: 4.4666 - acc: 0.04 - ETA: 44s - loss: 4.4674 - acc: 0.04 - ETA: 44s - loss: 4.4689 - acc: 0.04 - ETA: 44s - loss: 4.4702 - acc: 0.04 - ETA: 43s - loss: 4.4730 - acc: 0.04 - ETA: 43s - loss: 4.4719 - acc: 0.04 - ETA: 43s - loss: 4.4722 - acc: 0.04 - ETA: 42s - loss: 4.4704 - acc: 0.04 - ETA: 42s - loss: 4.4707 - acc: 0.04 - ETA: 42s - loss: 4.4695 - acc: 0.04 - ETA: 42s - loss: 4.4702 - acc: 0.04 - ETA: 41s - loss: 4.4709 - acc: 0.04 - ETA: 41s - loss: 4.4727 - acc: 0.04 - ETA: 41s - loss: 4.4729 - acc: 0.04 - ETA: 40s - loss: 4.4733 - acc: 0.04 - ETA: 40s - loss: 4.4737 - acc: 0.04 - ETA: 40s - loss: 4.4731 - acc: 0.04 - ETA: 39s - loss: 4.4741 - acc: 0.04 - ETA: 39s - loss: 4.4746 - acc: 0.04 - ETA: 39s - loss: 4.4753 - acc: 0.04 - ETA: 39s - loss: 4.4747 - acc: 0.04 - ETA: 38s - loss: 4.4745 - acc: 0.04 - ETA: 38s - loss: 4.4753 - acc: 0.04 - ETA: 38s - loss: 4.4730 - acc: 0.04 - ETA: 37s - loss: 4.4720 - acc: 0.04 - ETA: 37s - loss: 4.4713 - acc: 0.04 - ETA: 37s - loss: 4.4738 - acc: 0.04 - ETA: 36s - loss: 4.4752 - acc: 0.04 - ETA: 36s - loss: 4.4742 - acc: 0.04 - ETA: 36s - loss: 4.4747 - acc: 0.04 - ETA: 36s - loss: 4.4746 - acc: 0.04 - ETA: 35s - loss: 4.4748 - acc: 0.0481"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 35s - loss: 4.4750 - acc: 0.04 - ETA: 35s - loss: 4.4756 - acc: 0.04 - ETA: 34s - loss: 4.4745 - acc: 0.04 - ETA: 34s - loss: 4.4741 - acc: 0.04 - ETA: 34s - loss: 4.4743 - acc: 0.04 - ETA: 33s - loss: 4.4748 - acc: 0.04 - ETA: 33s - loss: 4.4752 - acc: 0.04 - ETA: 33s - loss: 4.4756 - acc: 0.04 - ETA: 33s - loss: 4.4766 - acc: 0.04 - ETA: 32s - loss: 4.4754 - acc: 0.04 - ETA: 32s - loss: 4.4750 - acc: 0.04 - ETA: 32s - loss: 4.4747 - acc: 0.04 - ETA: 31s - loss: 4.4757 - acc: 0.04 - ETA: 31s - loss: 4.4776 - acc: 0.04 - ETA: 31s - loss: 4.4780 - acc: 0.04 - ETA: 30s - loss: 4.4772 - acc: 0.04 - ETA: 30s - loss: 4.4760 - acc: 0.04 - ETA: 30s - loss: 4.4762 - acc: 0.04 - ETA: 30s - loss: 4.4764 - acc: 0.04 - ETA: 29s - loss: 4.4750 - acc: 0.04 - ETA: 29s - loss: 4.4739 - acc: 0.04 - ETA: 29s - loss: 4.4747 - acc: 0.04 - ETA: 28s - loss: 4.4742 - acc: 0.04 - ETA: 28s - loss: 4.4740 - acc: 0.04 - ETA: 28s - loss: 4.4743 - acc: 0.04 - ETA: 27s - loss: 4.4747 - acc: 0.04 - ETA: 27s - loss: 4.4749 - acc: 0.04 - ETA: 27s - loss: 4.4752 - acc: 0.04 - ETA: 27s - loss: 4.4752 - acc: 0.04 - ETA: 26s - loss: 4.4751 - acc: 0.04 - ETA: 26s - loss: 4.4754 - acc: 0.04 - ETA: 26s - loss: 4.4748 - acc: 0.04 - ETA: 25s - loss: 4.4752 - acc: 0.04 - ETA: 25s - loss: 4.4750 - acc: 0.04 - ETA: 25s - loss: 4.4745 - acc: 0.04 - ETA: 24s - loss: 4.4750 - acc: 0.04 - ETA: 24s - loss: 4.4741 - acc: 0.04 - ETA: 24s - loss: 4.4744 - acc: 0.04 - ETA: 24s - loss: 4.4758 - acc: 0.04 - ETA: 23s - loss: 4.4761 - acc: 0.04 - ETA: 23s - loss: 4.4764 - acc: 0.04 - ETA: 23s - loss: 4.4774 - acc: 0.04 - ETA: 22s - loss: 4.4784 - acc: 0.04 - ETA: 22s - loss: 4.4791 - acc: 0.04 - ETA: 22s - loss: 4.4784 - acc: 0.04 - ETA: 21s - loss: 4.4778 - acc: 0.04 - ETA: 21s - loss: 4.4763 - acc: 0.04 - ETA: 21s - loss: 4.4771 - acc: 0.04 - ETA: 21s - loss: 4.4755 - acc: 0.04 - ETA: 20s - loss: 4.4754 - acc: 0.04 - ETA: 20s - loss: 4.4769 - acc: 0.04 - ETA: 20s - loss: 4.4770 - acc: 0.04 - ETA: 19s - loss: 4.4771 - acc: 0.04 - ETA: 19s - loss: 4.4769 - acc: 0.04 - ETA: 19s - loss: 4.4765 - acc: 0.04 - ETA: 18s - loss: 4.4758 - acc: 0.04 - ETA: 18s - loss: 4.4760 - acc: 0.04 - ETA: 18s - loss: 4.4768 - acc: 0.04 - ETA: 18s - loss: 4.4762 - acc: 0.04 - ETA: 17s - loss: 4.4771 - acc: 0.04 - ETA: 17s - loss: 4.4766 - acc: 0.04 - ETA: 17s - loss: 4.4788 - acc: 0.04 - ETA: 16s - loss: 4.4785 - acc: 0.04 - ETA: 16s - loss: 4.4797 - acc: 0.04 - ETA: 16s - loss: 4.4796 - acc: 0.04 - ETA: 15s - loss: 4.4793 - acc: 0.04 - ETA: 15s - loss: 4.4803 - acc: 0.04 - ETA: 15s - loss: 4.4806 - acc: 0.04 - ETA: 15s - loss: 4.4815 - acc: 0.04 - ETA: 14s - loss: 4.4806 - acc: 0.04 - ETA: 14s - loss: 4.4802 - acc: 0.04 - ETA: 14s - loss: 4.4797 - acc: 0.04 - ETA: 13s - loss: 4.4800 - acc: 0.04 - ETA: 13s - loss: 4.4799 - acc: 0.04 - ETA: 13s - loss: 4.4802 - acc: 0.04 - ETA: 12s - loss: 4.4791 - acc: 0.04 - ETA: 12s - loss: 4.4785 - acc: 0.04 - ETA: 12s - loss: 4.4792 - acc: 0.04 - ETA: 12s - loss: 4.4804 - acc: 0.04 - ETA: 11s - loss: 4.4807 - acc: 0.04 - ETA: 11s - loss: 4.4805 - acc: 0.04 - ETA: 11s - loss: 4.4798 - acc: 0.04 - ETA: 10s - loss: 4.4794 - acc: 0.04 - ETA: 10s - loss: 4.4793 - acc: 0.04 - ETA: 10s - loss: 4.4784 - acc: 0.04 - ETA: 9s - loss: 4.4771 - acc: 0.0500 - ETA: 9s - loss: 4.4770 - acc: 0.049 - ETA: 9s - loss: 4.4758 - acc: 0.049 - ETA: 9s - loss: 4.4765 - acc: 0.049 - ETA: 8s - loss: 4.4762 - acc: 0.049 - ETA: 8s - loss: 4.4770 - acc: 0.050 - ETA: 8s - loss: 4.4767 - acc: 0.050 - ETA: 7s - loss: 4.4766 - acc: 0.050 - ETA: 7s - loss: 4.4769 - acc: 0.049 - ETA: 7s - loss: 4.4762 - acc: 0.050 - ETA: 6s - loss: 4.4759 - acc: 0.050 - ETA: 6s - loss: 4.4755 - acc: 0.050 - ETA: 6s - loss: 4.4749 - acc: 0.050 - ETA: 6s - loss: 4.4750 - acc: 0.050 - ETA: 5s - loss: 4.4763 - acc: 0.050 - ETA: 5s - loss: 4.4768 - acc: 0.050 - ETA: 5s - loss: 4.4780 - acc: 0.050 - ETA: 4s - loss: 4.4775 - acc: 0.050 - ETA: 4s - loss: 4.4776 - acc: 0.050 - ETA: 4s - loss: 4.4770 - acc: 0.050 - ETA: 3s - loss: 4.4777 - acc: 0.050 - ETA: 3s - loss: 4.4780 - acc: 0.050 - ETA: 3s - loss: 4.4776 - acc: 0.050 - ETA: 3s - loss: 4.4778 - acc: 0.050 - ETA: 2s - loss: 4.4776 - acc: 0.050 - ETA: 2s - loss: 4.4786 - acc: 0.050 - ETA: 2s - loss: 4.4782 - acc: 0.050 - ETA: 1s - loss: 4.4782 - acc: 0.050 - ETA: 1s - loss: 4.4777 - acc: 0.050 - ETA: 1s - loss: 4.4772 - acc: 0.050 - ETA: 0s - loss: 4.4771 - acc: 0.050 - ETA: 0s - loss: 4.4766 - acc: 0.050 - ETA: 0s - loss: 4.4760 - acc: 0.0500Epoch 00017: val_loss improved from 4.57852 to 4.57717, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 102s - loss: 4.4751 - acc: 0.0501 - val_loss: 4.5772 - val_acc: 0.0383\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 100s - loss: 4.8229 - acc: 0.0000e+0 - ETA: 99s - loss: 4.7905 - acc: 0.0000e+0 - ETA: 98s - loss: 4.6672 - acc: 0.0167   - ETA: 98s - loss: 4.7232 - acc: 0.01 - ETA: 98s - loss: 4.6994 - acc: 0.01 - ETA: 97s - loss: 4.6617 - acc: 0.03 - ETA: 97s - loss: 4.6533 - acc: 0.02 - ETA: 96s - loss: 4.6159 - acc: 0.02 - ETA: 96s - loss: 4.5907 - acc: 0.03 - ETA: 96s - loss: 4.5595 - acc: 0.04 - ETA: 95s - loss: 4.5934 - acc: 0.03 - ETA: 95s - loss: 4.5818 - acc: 0.05 - ETA: 95s - loss: 4.5735 - acc: 0.04 - ETA: 94s - loss: 4.5574 - acc: 0.04 - ETA: 94s - loss: 4.5217 - acc: 0.06 - ETA: 93s - loss: 4.5131 - acc: 0.05 - ETA: 93s - loss: 4.5132 - acc: 0.05 - ETA: 93s - loss: 4.5000 - acc: 0.05 - ETA: 92s - loss: 4.4825 - acc: 0.05 - ETA: 92s - loss: 4.4764 - acc: 0.05 - ETA: 92s - loss: 4.4807 - acc: 0.05 - ETA: 92s - loss: 4.4888 - acc: 0.05 - ETA: 91s - loss: 4.4832 - acc: 0.05 - ETA: 91s - loss: 4.4893 - acc: 0.05 - ETA: 91s - loss: 4.4836 - acc: 0.05 - ETA: 91s - loss: 4.4717 - acc: 0.05 - ETA: 90s - loss: 4.4919 - acc: 0.05 - ETA: 90s - loss: 4.4884 - acc: 0.05 - ETA: 90s - loss: 4.4977 - acc: 0.05 - ETA: 90s - loss: 4.4898 - acc: 0.05 - ETA: 89s - loss: 4.5001 - acc: 0.05 - ETA: 89s - loss: 4.4977 - acc: 0.05 - ETA: 89s - loss: 4.4928 - acc: 0.05 - ETA: 88s - loss: 4.4963 - acc: 0.05 - ETA: 88s - loss: 4.4846 - acc: 0.05 - ETA: 88s - loss: 4.4844 - acc: 0.05 - ETA: 88s - loss: 4.4810 - acc: 0.05 - ETA: 87s - loss: 4.4765 - acc: 0.05 - ETA: 87s - loss: 4.4814 - acc: 0.05 - ETA: 87s - loss: 4.4733 - acc: 0.05 - ETA: 86s - loss: 4.4710 - acc: 0.05 - ETA: 86s - loss: 4.4769 - acc: 0.05 - ETA: 86s - loss: 4.4856 - acc: 0.05 - ETA: 86s - loss: 4.4841 - acc: 0.05 - ETA: 85s - loss: 4.4816 - acc: 0.05 - ETA: 85s - loss: 4.4780 - acc: 0.05 - ETA: 85s - loss: 4.4698 - acc: 0.05 - ETA: 84s - loss: 4.4617 - acc: 0.05 - ETA: 84s - loss: 4.4567 - acc: 0.05 - ETA: 84s - loss: 4.4555 - acc: 0.05 - ETA: 83s - loss: 4.4531 - acc: 0.05 - ETA: 83s - loss: 4.4465 - acc: 0.05 - ETA: 83s - loss: 4.4482 - acc: 0.05 - ETA: 82s - loss: 4.4500 - acc: 0.05 - ETA: 82s - loss: 4.4504 - acc: 0.05 - ETA: 82s - loss: 4.4527 - acc: 0.05 - ETA: 82s - loss: 4.4524 - acc: 0.05 - ETA: 81s - loss: 4.4498 - acc: 0.05 - ETA: 81s - loss: 4.4548 - acc: 0.05 - ETA: 81s - loss: 4.4561 - acc: 0.05 - ETA: 80s - loss: 4.4574 - acc: 0.05 - ETA: 80s - loss: 4.4541 - acc: 0.05 - ETA: 80s - loss: 4.4578 - acc: 0.05 - ETA: 79s - loss: 4.4595 - acc: 0.05 - ETA: 79s - loss: 4.4578 - acc: 0.05 - ETA: 79s - loss: 4.4547 - acc: 0.05 - ETA: 78s - loss: 4.4512 - acc: 0.05 - ETA: 78s - loss: 4.4516 - acc: 0.05 - ETA: 78s - loss: 4.4539 - acc: 0.05 - ETA: 78s - loss: 4.4537 - acc: 0.05 - ETA: 77s - loss: 4.4610 - acc: 0.05 - ETA: 77s - loss: 4.4616 - acc: 0.05 - ETA: 77s - loss: 4.4587 - acc: 0.05 - ETA: 76s - loss: 4.4594 - acc: 0.05 - ETA: 76s - loss: 4.4555 - acc: 0.05 - ETA: 76s - loss: 4.4583 - acc: 0.05 - ETA: 76s - loss: 4.4599 - acc: 0.05 - ETA: 75s - loss: 4.4602 - acc: 0.05 - ETA: 75s - loss: 4.4571 - acc: 0.05 - ETA: 75s - loss: 4.4603 - acc: 0.05 - ETA: 74s - loss: 4.4603 - acc: 0.05 - ETA: 74s - loss: 4.4620 - acc: 0.05 - ETA: 74s - loss: 4.4643 - acc: 0.05 - ETA: 73s - loss: 4.4627 - acc: 0.05 - ETA: 73s - loss: 4.4657 - acc: 0.05 - ETA: 73s - loss: 4.4699 - acc: 0.05 - ETA: 73s - loss: 4.4705 - acc: 0.05 - ETA: 72s - loss: 4.4674 - acc: 0.05 - ETA: 72s - loss: 4.4670 - acc: 0.05 - ETA: 72s - loss: 4.4683 - acc: 0.05 - ETA: 71s - loss: 4.4691 - acc: 0.05 - ETA: 71s - loss: 4.4695 - acc: 0.05 - ETA: 71s - loss: 4.4715 - acc: 0.05 - ETA: 70s - loss: 4.4725 - acc: 0.05 - ETA: 70s - loss: 4.4713 - acc: 0.05 - ETA: 70s - loss: 4.4742 - acc: 0.05 - ETA: 69s - loss: 4.4758 - acc: 0.05 - ETA: 69s - loss: 4.4761 - acc: 0.05 - ETA: 69s - loss: 4.4748 - acc: 0.05 - ETA: 69s - loss: 4.4763 - acc: 0.05 - ETA: 68s - loss: 4.4752 - acc: 0.05 - ETA: 68s - loss: 4.4745 - acc: 0.05 - ETA: 68s - loss: 4.4758 - acc: 0.05 - ETA: 67s - loss: 4.4757 - acc: 0.05 - ETA: 67s - loss: 4.4735 - acc: 0.05 - ETA: 67s - loss: 4.4712 - acc: 0.05 - ETA: 67s - loss: 4.4692 - acc: 0.05 - ETA: 66s - loss: 4.4693 - acc: 0.05 - ETA: 66s - loss: 4.4707 - acc: 0.05 - ETA: 66s - loss: 4.4701 - acc: 0.05 - ETA: 65s - loss: 4.4690 - acc: 0.05 - ETA: 65s - loss: 4.4670 - acc: 0.05 - ETA: 65s - loss: 4.4667 - acc: 0.05 - ETA: 64s - loss: 4.4682 - acc: 0.05 - ETA: 64s - loss: 4.4672 - acc: 0.05 - ETA: 64s - loss: 4.4662 - acc: 0.05 - ETA: 64s - loss: 4.4691 - acc: 0.05 - ETA: 63s - loss: 4.4686 - acc: 0.05 - ETA: 63s - loss: 4.4680 - acc: 0.05 - ETA: 63s - loss: 4.4679 - acc: 0.05 - ETA: 62s - loss: 4.4672 - acc: 0.05 - ETA: 62s - loss: 4.4642 - acc: 0.05 - ETA: 62s - loss: 4.4654 - acc: 0.05 - ETA: 61s - loss: 4.4643 - acc: 0.05 - ETA: 61s - loss: 4.4669 - acc: 0.05 - ETA: 61s - loss: 4.4634 - acc: 0.05 - ETA: 61s - loss: 4.4615 - acc: 0.05 - ETA: 60s - loss: 4.4644 - acc: 0.05 - ETA: 60s - loss: 4.4647 - acc: 0.05 - ETA: 60s - loss: 4.4661 - acc: 0.05 - ETA: 59s - loss: 4.4665 - acc: 0.05 - ETA: 59s - loss: 4.4682 - acc: 0.05 - ETA: 59s - loss: 4.4697 - acc: 0.05 - ETA: 59s - loss: 4.4685 - acc: 0.05 - ETA: 58s - loss: 4.4713 - acc: 0.05 - ETA: 58s - loss: 4.4691 - acc: 0.05 - ETA: 58s - loss: 4.4715 - acc: 0.05 - ETA: 57s - loss: 4.4711 - acc: 0.05 - ETA: 57s - loss: 4.4678 - acc: 0.05 - ETA: 57s - loss: 4.4686 - acc: 0.05 - ETA: 56s - loss: 4.4682 - acc: 0.05 - ETA: 56s - loss: 4.4665 - acc: 0.05 - ETA: 56s - loss: 4.4679 - acc: 0.05 - ETA: 56s - loss: 4.4656 - acc: 0.05 - ETA: 55s - loss: 4.4658 - acc: 0.05 - ETA: 55s - loss: 4.4655 - acc: 0.05 - ETA: 55s - loss: 4.4637 - acc: 0.05 - ETA: 54s - loss: 4.4660 - acc: 0.05 - ETA: 54s - loss: 4.4673 - acc: 0.05 - ETA: 54s - loss: 4.4658 - acc: 0.05 - ETA: 54s - loss: 4.4629 - acc: 0.05 - ETA: 53s - loss: 4.4654 - acc: 0.05 - ETA: 53s - loss: 4.4641 - acc: 0.05 - ETA: 53s - loss: 4.4636 - acc: 0.05 - ETA: 52s - loss: 4.4658 - acc: 0.05 - ETA: 52s - loss: 4.4653 - acc: 0.05 - ETA: 52s - loss: 4.4654 - acc: 0.05 - ETA: 51s - loss: 4.4634 - acc: 0.05 - ETA: 51s - loss: 4.4633 - acc: 0.05 - ETA: 51s - loss: 4.4618 - acc: 0.05 - ETA: 51s - loss: 4.4611 - acc: 0.05 - ETA: 50s - loss: 4.4577 - acc: 0.05 - ETA: 50s - loss: 4.4565 - acc: 0.05 - ETA: 50s - loss: 4.4557 - acc: 0.05 - ETA: 49s - loss: 4.4548 - acc: 0.05 - ETA: 49s - loss: 4.4526 - acc: 0.05 - ETA: 49s - loss: 4.4536 - acc: 0.05 - ETA: 49s - loss: 4.4531 - acc: 0.05 - ETA: 48s - loss: 4.4535 - acc: 0.05 - ETA: 48s - loss: 4.4542 - acc: 0.05 - ETA: 48s - loss: 4.4554 - acc: 0.05 - ETA: 47s - loss: 4.4554 - acc: 0.05 - ETA: 47s - loss: 4.4533 - acc: 0.05 - ETA: 47s - loss: 4.4539 - acc: 0.05 - ETA: 47s - loss: 4.4552 - acc: 0.05 - ETA: 46s - loss: 4.4558 - acc: 0.05 - ETA: 46s - loss: 4.4557 - acc: 0.05 - ETA: 46s - loss: 4.4555 - acc: 0.05 - ETA: 45s - loss: 4.4549 - acc: 0.05 - ETA: 45s - loss: 4.4557 - acc: 0.05 - ETA: 45s - loss: 4.4549 - acc: 0.05 - ETA: 44s - loss: 4.4571 - acc: 0.05 - ETA: 44s - loss: 4.4569 - acc: 0.05 - ETA: 44s - loss: 4.4574 - acc: 0.05 - ETA: 44s - loss: 4.4578 - acc: 0.05 - ETA: 43s - loss: 4.4566 - acc: 0.05 - ETA: 43s - loss: 4.4562 - acc: 0.05 - ETA: 43s - loss: 4.4563 - acc: 0.05 - ETA: 42s - loss: 4.4557 - acc: 0.05 - ETA: 42s - loss: 4.4558 - acc: 0.05 - ETA: 42s - loss: 4.4547 - acc: 0.05 - ETA: 42s - loss: 4.4544 - acc: 0.05 - ETA: 41s - loss: 4.4539 - acc: 0.05 - ETA: 41s - loss: 4.4557 - acc: 0.05 - ETA: 41s - loss: 4.4562 - acc: 0.05 - ETA: 40s - loss: 4.4569 - acc: 0.05 - ETA: 40s - loss: 4.4571 - acc: 0.05 - ETA: 40s - loss: 4.4565 - acc: 0.05 - ETA: 39s - loss: 4.4567 - acc: 0.05 - ETA: 39s - loss: 4.4568 - acc: 0.05 - ETA: 39s - loss: 4.4577 - acc: 0.05 - ETA: 39s - loss: 4.4575 - acc: 0.05 - ETA: 38s - loss: 4.4557 - acc: 0.05 - ETA: 38s - loss: 4.4543 - acc: 0.05 - ETA: 38s - loss: 4.4538 - acc: 0.05 - ETA: 37s - loss: 4.4549 - acc: 0.05 - ETA: 37s - loss: 4.4540 - acc: 0.05 - ETA: 37s - loss: 4.4542 - acc: 0.05 - ETA: 36s - loss: 4.4531 - acc: 0.05 - ETA: 36s - loss: 4.4533 - acc: 0.05 - ETA: 36s - loss: 4.4529 - acc: 0.05 - ETA: 36s - loss: 4.4510 - acc: 0.05 - ETA: 35s - loss: 4.4499 - acc: 0.05 - ETA: 35s - loss: 4.4494 - acc: 0.05 - ETA: 35s - loss: 4.4514 - acc: 0.05356660/6680 [============================>.] - ETA: 34s - loss: 4.4507 - acc: 0.05 - ETA: 34s - loss: 4.4509 - acc: 0.05 - ETA: 34s - loss: 4.4490 - acc: 0.05 - ETA: 34s - loss: 4.4499 - acc: 0.05 - ETA: 33s - loss: 4.4501 - acc: 0.05 - ETA: 33s - loss: 4.4490 - acc: 0.05 - ETA: 33s - loss: 4.4505 - acc: 0.05 - ETA: 32s - loss: 4.4494 - acc: 0.05 - ETA: 32s - loss: 4.4517 - acc: 0.05 - ETA: 32s - loss: 4.4516 - acc: 0.05 - ETA: 31s - loss: 4.4514 - acc: 0.05 - ETA: 31s - loss: 4.4497 - acc: 0.05 - ETA: 31s - loss: 4.4502 - acc: 0.05 - ETA: 31s - loss: 4.4529 - acc: 0.05 - ETA: 30s - loss: 4.4526 - acc: 0.05 - ETA: 30s - loss: 4.4525 - acc: 0.05 - ETA: 30s - loss: 4.4516 - acc: 0.05 - ETA: 29s - loss: 4.4528 - acc: 0.05 - ETA: 29s - loss: 4.4521 - acc: 0.05 - ETA: 29s - loss: 4.4528 - acc: 0.05 - ETA: 28s - loss: 4.4537 - acc: 0.05 - ETA: 28s - loss: 4.4545 - acc: 0.05 - ETA: 28s - loss: 4.4557 - acc: 0.05 - ETA: 28s - loss: 4.4556 - acc: 0.05 - ETA: 27s - loss: 4.4559 - acc: 0.05 - ETA: 27s - loss: 4.4567 - acc: 0.05 - ETA: 27s - loss: 4.4561 - acc: 0.05 - ETA: 26s - loss: 4.4577 - acc: 0.05 - ETA: 26s - loss: 4.4580 - acc: 0.05 - ETA: 26s - loss: 4.4579 - acc: 0.05 - ETA: 26s - loss: 4.4580 - acc: 0.05 - ETA: 25s - loss: 4.4583 - acc: 0.05 - ETA: 25s - loss: 4.4583 - acc: 0.05 - ETA: 25s - loss: 4.4583 - acc: 0.05 - ETA: 24s - loss: 4.4587 - acc: 0.05 - ETA: 24s - loss: 4.4590 - acc: 0.05 - ETA: 24s - loss: 4.4591 - acc: 0.05 - ETA: 23s - loss: 4.4594 - acc: 0.05 - ETA: 23s - loss: 4.4604 - acc: 0.05 - ETA: 23s - loss: 4.4603 - acc: 0.05 - ETA: 23s - loss: 4.4599 - acc: 0.05 - ETA: 22s - loss: 4.4596 - acc: 0.05 - ETA: 22s - loss: 4.4597 - acc: 0.05 - ETA: 22s - loss: 4.4603 - acc: 0.05 - ETA: 21s - loss: 4.4601 - acc: 0.05 - ETA: 21s - loss: 4.4603 - acc: 0.05 - ETA: 21s - loss: 4.4593 - acc: 0.05 - ETA: 20s - loss: 4.4593 - acc: 0.05 - ETA: 20s - loss: 4.4595 - acc: 0.05 - ETA: 20s - loss: 4.4593 - acc: 0.05 - ETA: 20s - loss: 4.4594 - acc: 0.05 - ETA: 19s - loss: 4.4588 - acc: 0.05 - ETA: 19s - loss: 4.4589 - acc: 0.05 - ETA: 19s - loss: 4.4605 - acc: 0.05 - ETA: 18s - loss: 4.4607 - acc: 0.05 - ETA: 18s - loss: 4.4609 - acc: 0.05 - ETA: 18s - loss: 4.4603 - acc: 0.05 - ETA: 18s - loss: 4.4609 - acc: 0.05 - ETA: 17s - loss: 4.4611 - acc: 0.05 - ETA: 17s - loss: 4.4607 - acc: 0.05 - ETA: 17s - loss: 4.4610 - acc: 0.05 - ETA: 16s - loss: 4.4602 - acc: 0.05 - ETA: 16s - loss: 4.4599 - acc: 0.05 - ETA: 16s - loss: 4.4614 - acc: 0.05 - ETA: 15s - loss: 4.4617 - acc: 0.05 - ETA: 15s - loss: 4.4629 - acc: 0.05 - ETA: 15s - loss: 4.4635 - acc: 0.05 - ETA: 15s - loss: 4.4621 - acc: 0.05 - ETA: 14s - loss: 4.4619 - acc: 0.05 - ETA: 14s - loss: 4.4616 - acc: 0.05 - ETA: 14s - loss: 4.4624 - acc: 0.05 - ETA: 13s - loss: 4.4620 - acc: 0.05 - ETA: 13s - loss: 4.4614 - acc: 0.05 - ETA: 13s - loss: 4.4610 - acc: 0.05 - ETA: 12s - loss: 4.4610 - acc: 0.05 - ETA: 12s - loss: 4.4603 - acc: 0.05 - ETA: 12s - loss: 4.4604 - acc: 0.05 - ETA: 12s - loss: 4.4605 - acc: 0.05 - ETA: 11s - loss: 4.4602 - acc: 0.05 - ETA: 11s - loss: 4.4593 - acc: 0.05 - ETA: 11s - loss: 4.4605 - acc: 0.05 - ETA: 10s - loss: 4.4613 - acc: 0.05 - ETA: 10s - loss: 4.4608 - acc: 0.05 - ETA: 10s - loss: 4.4602 - acc: 0.05 - ETA: 10s - loss: 4.4594 - acc: 0.05 - ETA: 9s - loss: 4.4587 - acc: 0.0538 - ETA: 9s - loss: 4.4590 - acc: 0.053 - ETA: 9s - loss: 4.4597 - acc: 0.054 - ETA: 8s - loss: 4.4604 - acc: 0.053 - ETA: 8s - loss: 4.4606 - acc: 0.054 - ETA: 8s - loss: 4.4604 - acc: 0.053 - ETA: 7s - loss: 4.4603 - acc: 0.053 - ETA: 7s - loss: 4.4597 - acc: 0.053 - ETA: 7s - loss: 4.4595 - acc: 0.053 - ETA: 7s - loss: 4.4594 - acc: 0.053 - ETA: 6s - loss: 4.4593 - acc: 0.054 - ETA: 6s - loss: 4.4587 - acc: 0.054 - ETA: 6s - loss: 4.4590 - acc: 0.054 - ETA: 5s - loss: 4.4591 - acc: 0.054 - ETA: 5s - loss: 4.4591 - acc: 0.054 - ETA: 5s - loss: 4.4589 - acc: 0.054 - ETA: 5s - loss: 4.4585 - acc: 0.054 - ETA: 4s - loss: 4.4580 - acc: 0.054 - ETA: 4s - loss: 4.4583 - acc: 0.054 - ETA: 4s - loss: 4.4593 - acc: 0.054 - ETA: 3s - loss: 4.4598 - acc: 0.054 - ETA: 3s - loss: 4.4594 - acc: 0.054 - ETA: 3s - loss: 4.4599 - acc: 0.054 - ETA: 2s - loss: 4.4601 - acc: 0.054 - ETA: 2s - loss: 4.4601 - acc: 0.054 - ETA: 2s - loss: 4.4597 - acc: 0.054 - ETA: 2s - loss: 4.4598 - acc: 0.054 - ETA: 1s - loss: 4.4600 - acc: 0.054 - ETA: 1s - loss: 4.4596 - acc: 0.054 - ETA: 1s - loss: 4.4598 - acc: 0.054 - ETA: 0s - loss: 4.4595 - acc: 0.054 - ETA: 0s - loss: 4.4592 - acc: 0.054 - ETA: 0s - loss: 4.4583 - acc: 0.0547Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 100s - loss: 4.4581 - acc: 0.0546 - val_loss: 4.6018 - val_acc: 0.0335\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 91s - loss: 4.5619 - acc: 0.10 - ETA: 95s - loss: 4.4893 - acc: 0.07 - ETA: 96s - loss: 4.4518 - acc: 0.06 - ETA: 95s - loss: 4.3935 - acc: 0.08 - ETA: 96s - loss: 4.4325 - acc: 0.08 - ETA: 96s - loss: 4.3583 - acc: 0.08 - ETA: 95s - loss: 4.3565 - acc: 0.09 - ETA: 95s - loss: 4.3284 - acc: 0.10 - ETA: 95s - loss: 4.3873 - acc: 0.09 - ETA: 95s - loss: 4.4096 - acc: 0.09 - ETA: 94s - loss: 4.4299 - acc: 0.09 - ETA: 94s - loss: 4.4365 - acc: 0.09 - ETA: 94s - loss: 4.4320 - acc: 0.08 - ETA: 93s - loss: 4.4201 - acc: 0.08 - ETA: 93s - loss: 4.4245 - acc: 0.08 - ETA: 93s - loss: 4.4058 - acc: 0.08 - ETA: 92s - loss: 4.4276 - acc: 0.08 - ETA: 92s - loss: 4.4339 - acc: 0.08 - ETA: 92s - loss: 4.4273 - acc: 0.08 - ETA: 92s - loss: 4.4245 - acc: 0.08 - ETA: 91s - loss: 4.4185 - acc: 0.08 - ETA: 91s - loss: 4.4152 - acc: 0.08 - ETA: 91s - loss: 4.4136 - acc: 0.08 - ETA: 91s - loss: 4.4126 - acc: 0.07 - ETA: 90s - loss: 4.4049 - acc: 0.07 - ETA: 90s - loss: 4.3945 - acc: 0.08 - ETA: 90s - loss: 4.3963 - acc: 0.07 - ETA: 89s - loss: 4.3818 - acc: 0.08 - ETA: 89s - loss: 4.3835 - acc: 0.07 - ETA: 89s - loss: 4.3836 - acc: 0.07 - ETA: 89s - loss: 4.3877 - acc: 0.07 - ETA: 88s - loss: 4.3999 - acc: 0.07 - ETA: 88s - loss: 4.4030 - acc: 0.07 - ETA: 88s - loss: 4.4059 - acc: 0.07 - ETA: 87s - loss: 4.4046 - acc: 0.07 - ETA: 87s - loss: 4.4075 - acc: 0.07 - ETA: 87s - loss: 4.4081 - acc: 0.07 - ETA: 87s - loss: 4.3980 - acc: 0.07 - ETA: 86s - loss: 4.4043 - acc: 0.07 - ETA: 86s - loss: 4.4136 - acc: 0.06 - ETA: 86s - loss: 4.4221 - acc: 0.06 - ETA: 86s - loss: 4.4204 - acc: 0.06 - ETA: 86s - loss: 4.4283 - acc: 0.06 - ETA: 85s - loss: 4.4274 - acc: 0.06 - ETA: 85s - loss: 4.4175 - acc: 0.06 - ETA: 85s - loss: 4.4108 - acc: 0.06 - ETA: 84s - loss: 4.4174 - acc: 0.06 - ETA: 84s - loss: 4.4202 - acc: 0.06 - ETA: 84s - loss: 4.4235 - acc: 0.06 - ETA: 84s - loss: 4.4232 - acc: 0.06 - ETA: 83s - loss: 4.4223 - acc: 0.06 - ETA: 83s - loss: 4.4232 - acc: 0.06 - ETA: 83s - loss: 4.4240 - acc: 0.06 - ETA: 82s - loss: 4.4205 - acc: 0.06 - ETA: 82s - loss: 4.4138 - acc: 0.06 - ETA: 82s - loss: 4.4047 - acc: 0.05 - ETA: 81s - loss: 4.4121 - acc: 0.05 - ETA: 81s - loss: 4.4092 - acc: 0.05 - ETA: 81s - loss: 4.4092 - acc: 0.05 - ETA: 80s - loss: 4.4035 - acc: 0.06 - ETA: 80s - loss: 4.4034 - acc: 0.06 - ETA: 80s - loss: 4.4018 - acc: 0.06 - ETA: 80s - loss: 4.3955 - acc: 0.06 - ETA: 79s - loss: 4.3925 - acc: 0.06 - ETA: 79s - loss: 4.3934 - acc: 0.06 - ETA: 79s - loss: 4.3951 - acc: 0.06 - ETA: 78s - loss: 4.4006 - acc: 0.06 - ETA: 78s - loss: 4.4007 - acc: 0.06 - ETA: 78s - loss: 4.3985 - acc: 0.06 - ETA: 77s - loss: 4.4002 - acc: 0.06 - ETA: 77s - loss: 4.4045 - acc: 0.05 - ETA: 77s - loss: 4.4090 - acc: 0.05 - ETA: 77s - loss: 4.4104 - acc: 0.05 - ETA: 76s - loss: 4.4142 - acc: 0.05 - ETA: 76s - loss: 4.4165 - acc: 0.05 - ETA: 76s - loss: 4.4142 - acc: 0.05 - ETA: 75s - loss: 4.4173 - acc: 0.05 - ETA: 75s - loss: 4.4176 - acc: 0.05 - ETA: 75s - loss: 4.4146 - acc: 0.05 - ETA: 75s - loss: 4.4137 - acc: 0.05 - ETA: 74s - loss: 4.4142 - acc: 0.05 - ETA: 74s - loss: 4.4176 - acc: 0.05 - ETA: 74s - loss: 4.4176 - acc: 0.05 - ETA: 73s - loss: 4.4202 - acc: 0.05 - ETA: 73s - loss: 4.4215 - acc: 0.05 - ETA: 73s - loss: 4.4180 - acc: 0.05 - ETA: 72s - loss: 4.4172 - acc: 0.05 - ETA: 72s - loss: 4.4217 - acc: 0.05 - ETA: 72s - loss: 4.4207 - acc: 0.05 - ETA: 72s - loss: 4.4228 - acc: 0.05 - ETA: 71s - loss: 4.4218 - acc: 0.05 - ETA: 71s - loss: 4.4249 - acc: 0.05 - ETA: 71s - loss: 4.4237 - acc: 0.05 - ETA: 70s - loss: 4.4239 - acc: 0.05 - ETA: 70s - loss: 4.4221 - acc: 0.05 - ETA: 70s - loss: 4.4238 - acc: 0.05 - ETA: 69s - loss: 4.4237 - acc: 0.05 - ETA: 69s - loss: 4.4238 - acc: 0.05 - ETA: 69s - loss: 4.4260 - acc: 0.05 - ETA: 69s - loss: 4.4224 - acc: 0.05 - ETA: 68s - loss: 4.4204 - acc: 0.05 - ETA: 68s - loss: 4.4229 - acc: 0.05 - ETA: 68s - loss: 4.4248 - acc: 0.05 - ETA: 67s - loss: 4.4225 - acc: 0.05 - ETA: 67s - loss: 4.4258 - acc: 0.05 - ETA: 67s - loss: 4.4228 - acc: 0.05 - ETA: 66s - loss: 4.4233 - acc: 0.05 - ETA: 66s - loss: 4.4246 - acc: 0.05 - ETA: 66s - loss: 4.4217 - acc: 0.05 - ETA: 66s - loss: 4.4227 - acc: 0.05 - ETA: 65s - loss: 4.4234 - acc: 0.05 - ETA: 65s - loss: 4.4211 - acc: 0.05 - ETA: 65s - loss: 4.4189 - acc: 0.05 - ETA: 64s - loss: 4.4165 - acc: 0.05 - ETA: 64s - loss: 4.4148 - acc: 0.06 - ETA: 64s - loss: 4.4131 - acc: 0.06 - ETA: 64s - loss: 4.4152 - acc: 0.06 - ETA: 63s - loss: 4.4135 - acc: 0.06 - ETA: 63s - loss: 4.4141 - acc: 0.06 - ETA: 63s - loss: 4.4161 - acc: 0.06 - ETA: 62s - loss: 4.4149 - acc: 0.05 - ETA: 62s - loss: 4.4168 - acc: 0.05 - ETA: 62s - loss: 4.4170 - acc: 0.05 - ETA: 61s - loss: 4.4154 - acc: 0.05 - ETA: 61s - loss: 4.4132 - acc: 0.05 - ETA: 61s - loss: 4.4139 - acc: 0.05 - ETA: 61s - loss: 4.4163 - acc: 0.05 - ETA: 60s - loss: 4.4159 - acc: 0.05 - ETA: 60s - loss: 4.4163 - acc: 0.05 - ETA: 60s - loss: 4.4164 - acc: 0.05 - ETA: 59s - loss: 4.4149 - acc: 0.05 - ETA: 59s - loss: 4.4148 - acc: 0.05 - ETA: 59s - loss: 4.4176 - acc: 0.05 - ETA: 58s - loss: 4.4172 - acc: 0.05 - ETA: 58s - loss: 4.4182 - acc: 0.05 - ETA: 58s - loss: 4.4176 - acc: 0.05 - ETA: 58s - loss: 4.4165 - acc: 0.05 - ETA: 57s - loss: 4.4181 - acc: 0.05 - ETA: 57s - loss: 4.4166 - acc: 0.05 - ETA: 57s - loss: 4.4168 - acc: 0.06 - ETA: 56s - loss: 4.4172 - acc: 0.06 - ETA: 56s - loss: 4.4191 - acc: 0.05 - ETA: 56s - loss: 4.4192 - acc: 0.05 - ETA: 55s - loss: 4.4201 - acc: 0.05 - ETA: 55s - loss: 4.4197 - acc: 0.05 - ETA: 55s - loss: 4.4218 - acc: 0.05 - ETA: 55s - loss: 4.4212 - acc: 0.05 - ETA: 54s - loss: 4.4231 - acc: 0.05 - ETA: 54s - loss: 4.4250 - acc: 0.05 - ETA: 54s - loss: 4.4262 - acc: 0.05 - ETA: 53s - loss: 4.4270 - acc: 0.05 - ETA: 53s - loss: 4.4262 - acc: 0.05 - ETA: 53s - loss: 4.4285 - acc: 0.05 - ETA: 53s - loss: 4.4268 - acc: 0.05 - ETA: 52s - loss: 4.4253 - acc: 0.05 - ETA: 52s - loss: 4.4229 - acc: 0.05 - ETA: 52s - loss: 4.4225 - acc: 0.05 - ETA: 51s - loss: 4.4252 - acc: 0.05 - ETA: 51s - loss: 4.4266 - acc: 0.05 - ETA: 51s - loss: 4.4275 - acc: 0.05 - ETA: 51s - loss: 4.4266 - acc: 0.05 - ETA: 50s - loss: 4.4259 - acc: 0.05 - ETA: 50s - loss: 4.4252 - acc: 0.05 - ETA: 50s - loss: 4.4252 - acc: 0.05 - ETA: 49s - loss: 4.4270 - acc: 0.05 - ETA: 49s - loss: 4.4254 - acc: 0.05 - ETA: 49s - loss: 4.4253 - acc: 0.05 - ETA: 49s - loss: 4.4268 - acc: 0.05 - ETA: 48s - loss: 4.4276 - acc: 0.05 - ETA: 48s - loss: 4.4281 - acc: 0.05 - ETA: 48s - loss: 4.4289 - acc: 0.05 - ETA: 47s - loss: 4.4306 - acc: 0.05 - ETA: 47s - loss: 4.4315 - acc: 0.05 - ETA: 47s - loss: 4.4319 - acc: 0.05 - ETA: 46s - loss: 4.4309 - acc: 0.05 - ETA: 46s - loss: 4.4304 - acc: 0.05 - ETA: 46s - loss: 4.4303 - acc: 0.05 - ETA: 46s - loss: 4.4299 - acc: 0.05 - ETA: 45s - loss: 4.4300 - acc: 0.05 - ETA: 45s - loss: 4.4291 - acc: 0.05 - ETA: 45s - loss: 4.4302 - acc: 0.05 - ETA: 44s - loss: 4.4296 - acc: 0.05 - ETA: 44s - loss: 4.4280 - acc: 0.05 - ETA: 44s - loss: 4.4267 - acc: 0.05 - ETA: 43s - loss: 4.4276 - acc: 0.05 - ETA: 43s - loss: 4.4272 - acc: 0.05 - ETA: 43s - loss: 4.4268 - acc: 0.05 - ETA: 43s - loss: 4.4299 - acc: 0.05 - ETA: 42s - loss: 4.4306 - acc: 0.05 - ETA: 42s - loss: 4.4314 - acc: 0.05 - ETA: 42s - loss: 4.4319 - acc: 0.05 - ETA: 41s - loss: 4.4326 - acc: 0.05 - ETA: 41s - loss: 4.4348 - acc: 0.05 - ETA: 41s - loss: 4.4346 - acc: 0.05 - ETA: 41s - loss: 4.4348 - acc: 0.05 - ETA: 40s - loss: 4.4346 - acc: 0.05 - ETA: 40s - loss: 4.4346 - acc: 0.05 - ETA: 40s - loss: 4.4351 - acc: 0.05 - ETA: 39s - loss: 4.4372 - acc: 0.05 - ETA: 39s - loss: 4.4368 - acc: 0.05 - ETA: 39s - loss: 4.4375 - acc: 0.05 - ETA: 38s - loss: 4.4373 - acc: 0.05 - ETA: 38s - loss: 4.4363 - acc: 0.05 - ETA: 38s - loss: 4.4381 - acc: 0.05 - ETA: 38s - loss: 4.4386 - acc: 0.05 - ETA: 37s - loss: 4.4385 - acc: 0.05 - ETA: 37s - loss: 4.4411 - acc: 0.05 - ETA: 37s - loss: 4.4410 - acc: 0.05 - ETA: 36s - loss: 4.4407 - acc: 0.05 - ETA: 36s - loss: 4.4411 - acc: 0.05 - ETA: 36s - loss: 4.4417 - acc: 0.05 - ETA: 36s - loss: 4.4427 - acc: 0.05 - ETA: 35s - loss: 4.4430 - acc: 0.05 - ETA: 35s - loss: 4.4437 - acc: 0.05 - ETA: 35s - loss: 4.4441 - acc: 0.0542"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 34s - loss: 4.4424 - acc: 0.05 - ETA: 34s - loss: 4.4440 - acc: 0.05 - ETA: 34s - loss: 4.4443 - acc: 0.05 - ETA: 33s - loss: 4.4449 - acc: 0.05 - ETA: 33s - loss: 4.4447 - acc: 0.05 - ETA: 33s - loss: 4.4432 - acc: 0.05 - ETA: 33s - loss: 4.4427 - acc: 0.05 - ETA: 32s - loss: 4.4426 - acc: 0.05 - ETA: 32s - loss: 4.4416 - acc: 0.05 - ETA: 32s - loss: 4.4410 - acc: 0.05 - ETA: 31s - loss: 4.4413 - acc: 0.05 - ETA: 31s - loss: 4.4414 - acc: 0.05 - ETA: 31s - loss: 4.4429 - acc: 0.05 - ETA: 30s - loss: 4.4413 - acc: 0.05 - ETA: 30s - loss: 4.4427 - acc: 0.05 - ETA: 30s - loss: 4.4411 - acc: 0.05 - ETA: 30s - loss: 4.4399 - acc: 0.05 - ETA: 29s - loss: 4.4393 - acc: 0.05 - ETA: 29s - loss: 4.4396 - acc: 0.05 - ETA: 29s - loss: 4.4395 - acc: 0.05 - ETA: 28s - loss: 4.4396 - acc: 0.05 - ETA: 28s - loss: 4.4392 - acc: 0.05 - ETA: 28s - loss: 4.4384 - acc: 0.05 - ETA: 28s - loss: 4.4383 - acc: 0.05 - ETA: 27s - loss: 4.4395 - acc: 0.05 - ETA: 27s - loss: 4.4401 - acc: 0.05 - ETA: 27s - loss: 4.4407 - acc: 0.05 - ETA: 26s - loss: 4.4411 - acc: 0.05 - ETA: 26s - loss: 4.4411 - acc: 0.05 - ETA: 26s - loss: 4.4407 - acc: 0.05 - ETA: 25s - loss: 4.4412 - acc: 0.05 - ETA: 25s - loss: 4.4401 - acc: 0.05 - ETA: 25s - loss: 4.4402 - acc: 0.05 - ETA: 25s - loss: 4.4416 - acc: 0.05 - ETA: 24s - loss: 4.4419 - acc: 0.05 - ETA: 24s - loss: 4.4424 - acc: 0.05 - ETA: 24s - loss: 4.4423 - acc: 0.05 - ETA: 23s - loss: 4.4434 - acc: 0.05 - ETA: 23s - loss: 4.4436 - acc: 0.05 - ETA: 23s - loss: 4.4434 - acc: 0.05 - ETA: 23s - loss: 4.4449 - acc: 0.05 - ETA: 22s - loss: 4.4445 - acc: 0.05 - ETA: 22s - loss: 4.4454 - acc: 0.05 - ETA: 22s - loss: 4.4452 - acc: 0.05 - ETA: 21s - loss: 4.4443 - acc: 0.05 - ETA: 21s - loss: 4.4434 - acc: 0.05 - ETA: 21s - loss: 4.4424 - acc: 0.05 - ETA: 20s - loss: 4.4426 - acc: 0.05 - ETA: 20s - loss: 4.4422 - acc: 0.05 - ETA: 20s - loss: 4.4413 - acc: 0.05 - ETA: 20s - loss: 4.4422 - acc: 0.05 - ETA: 19s - loss: 4.4416 - acc: 0.05 - ETA: 19s - loss: 4.4406 - acc: 0.05 - ETA: 19s - loss: 4.4401 - acc: 0.05 - ETA: 18s - loss: 4.4408 - acc: 0.05 - ETA: 18s - loss: 4.4410 - acc: 0.05 - ETA: 18s - loss: 4.4424 - acc: 0.05 - ETA: 17s - loss: 4.4440 - acc: 0.05 - ETA: 17s - loss: 4.4456 - acc: 0.05 - ETA: 17s - loss: 4.4444 - acc: 0.05 - ETA: 17s - loss: 4.4449 - acc: 0.05 - ETA: 16s - loss: 4.4452 - acc: 0.05 - ETA: 16s - loss: 4.4457 - acc: 0.05 - ETA: 16s - loss: 4.4454 - acc: 0.05 - ETA: 15s - loss: 4.4450 - acc: 0.05 - ETA: 15s - loss: 4.4434 - acc: 0.05 - ETA: 15s - loss: 4.4429 - acc: 0.05 - ETA: 15s - loss: 4.4437 - acc: 0.05 - ETA: 14s - loss: 4.4453 - acc: 0.05 - ETA: 14s - loss: 4.4455 - acc: 0.05 - ETA: 14s - loss: 4.4467 - acc: 0.05 - ETA: 13s - loss: 4.4471 - acc: 0.05 - ETA: 13s - loss: 4.4470 - acc: 0.05 - ETA: 13s - loss: 4.4473 - acc: 0.05 - ETA: 12s - loss: 4.4466 - acc: 0.05 - ETA: 12s - loss: 4.4465 - acc: 0.05 - ETA: 12s - loss: 4.4450 - acc: 0.05 - ETA: 12s - loss: 4.4463 - acc: 0.05 - ETA: 11s - loss: 4.4464 - acc: 0.05 - ETA: 11s - loss: 4.4472 - acc: 0.05 - ETA: 11s - loss: 4.4473 - acc: 0.05 - ETA: 10s - loss: 4.4479 - acc: 0.05 - ETA: 10s - loss: 4.4481 - acc: 0.05 - ETA: 10s - loss: 4.4490 - acc: 0.05 - ETA: 10s - loss: 4.4491 - acc: 0.05 - ETA: 9s - loss: 4.4491 - acc: 0.0537 - ETA: 9s - loss: 4.4493 - acc: 0.053 - ETA: 9s - loss: 4.4485 - acc: 0.053 - ETA: 8s - loss: 4.4489 - acc: 0.053 - ETA: 8s - loss: 4.4484 - acc: 0.053 - ETA: 8s - loss: 4.4480 - acc: 0.053 - ETA: 7s - loss: 4.4486 - acc: 0.053 - ETA: 7s - loss: 4.4481 - acc: 0.053 - ETA: 7s - loss: 4.4491 - acc: 0.053 - ETA: 7s - loss: 4.4486 - acc: 0.053 - ETA: 6s - loss: 4.4488 - acc: 0.053 - ETA: 6s - loss: 4.4508 - acc: 0.053 - ETA: 6s - loss: 4.4508 - acc: 0.053 - ETA: 5s - loss: 4.4509 - acc: 0.053 - ETA: 5s - loss: 4.4499 - acc: 0.053 - ETA: 5s - loss: 4.4500 - acc: 0.053 - ETA: 5s - loss: 4.4496 - acc: 0.053 - ETA: 4s - loss: 4.4484 - acc: 0.053 - ETA: 4s - loss: 4.4480 - acc: 0.054 - ETA: 4s - loss: 4.4483 - acc: 0.054 - ETA: 3s - loss: 4.4478 - acc: 0.054 - ETA: 3s - loss: 4.4485 - acc: 0.054 - ETA: 3s - loss: 4.4487 - acc: 0.054 - ETA: 2s - loss: 4.4473 - acc: 0.054 - ETA: 2s - loss: 4.4477 - acc: 0.055 - ETA: 2s - loss: 4.4471 - acc: 0.055 - ETA: 2s - loss: 4.4469 - acc: 0.055 - ETA: 1s - loss: 4.4460 - acc: 0.055 - ETA: 1s - loss: 4.4461 - acc: 0.055 - ETA: 1s - loss: 4.4449 - acc: 0.055 - ETA: 0s - loss: 4.4449 - acc: 0.055 - ETA: 0s - loss: 4.4442 - acc: 0.055 - ETA: 0s - loss: 4.4446 - acc: 0.0556Epoch 00019: val_loss improved from 4.57717 to 4.57388, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 100s - loss: 4.4444 - acc: 0.0555 - val_loss: 4.5739 - val_acc: 0.0311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c17b4cd668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 3.5885%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6600/6680 [============================>.] - ETA: 145s - loss: 14.6329 - acc: 0.05 - ETA: 32s - loss: 15.0186 - acc: 0.0200 - ETA: 17s - loss: 15.1171 - acc: 0.030 - ETA: 11s - loss: 14.9192 - acc: 0.028 - ETA: 9s - loss: 14.9060 - acc: 0.022 - ETA: 8s - loss: 14.8440 - acc: 0.02 - ETA: 6s - loss: 14.7603 - acc: 0.02 - ETA: 6s - loss: 14.7336 - acc: 0.02 - ETA: 5s - loss: 14.6357 - acc: 0.02 - ETA: 5s - loss: 14.6227 - acc: 0.02 - ETA: 4s - loss: 14.5540 - acc: 0.03 - ETA: 4s - loss: 14.5268 - acc: 0.03 - ETA: 4s - loss: 14.4859 - acc: 0.03 - ETA: 3s - loss: 14.4128 - acc: 0.03 - ETA: 3s - loss: 14.3914 - acc: 0.03 - ETA: 3s - loss: 14.3578 - acc: 0.03 - ETA: 3s - loss: 14.2653 - acc: 0.04 - ETA: 3s - loss: 14.2341 - acc: 0.04 - ETA: 3s - loss: 14.1645 - acc: 0.04 - ETA: 2s - loss: 14.1084 - acc: 0.05 - ETA: 2s - loss: 14.0505 - acc: 0.05 - ETA: 2s - loss: 13.9855 - acc: 0.05 - ETA: 2s - loss: 13.9860 - acc: 0.05 - ETA: 2s - loss: 13.9496 - acc: 0.05 - ETA: 2s - loss: 13.9152 - acc: 0.05 - ETA: 2s - loss: 13.8752 - acc: 0.06 - ETA: 2s - loss: 13.7883 - acc: 0.06 - ETA: 2s - loss: 13.7510 - acc: 0.06 - ETA: 2s - loss: 13.7163 - acc: 0.06 - ETA: 1s - loss: 13.6552 - acc: 0.06 - ETA: 1s - loss: 13.6194 - acc: 0.07 - ETA: 1s - loss: 13.5654 - acc: 0.07 - ETA: 1s - loss: 13.5608 - acc: 0.07 - ETA: 1s - loss: 13.5012 - acc: 0.07 - ETA: 1s - loss: 13.4634 - acc: 0.07 - ETA: 1s - loss: 13.4323 - acc: 0.07 - ETA: 1s - loss: 13.4079 - acc: 0.07 - ETA: 1s - loss: 13.3677 - acc: 0.07 - ETA: 1s - loss: 13.3091 - acc: 0.08 - ETA: 1s - loss: 13.2486 - acc: 0.08 - ETA: 1s - loss: 13.1872 - acc: 0.08 - ETA: 1s - loss: 13.1536 - acc: 0.09 - ETA: 0s - loss: 13.1218 - acc: 0.09 - ETA: 0s - loss: 13.0718 - acc: 0.09 - ETA: 0s - loss: 13.0359 - acc: 0.09 - ETA: 0s - loss: 13.0003 - acc: 0.09 - ETA: 0s - loss: 12.9731 - acc: 0.10 - ETA: 0s - loss: 12.9475 - acc: 0.10 - ETA: 0s - loss: 12.9256 - acc: 0.10 - ETA: 0s - loss: 12.9051 - acc: 0.10 - ETA: 0s - loss: 12.8771 - acc: 0.10 - ETA: 0s - loss: 12.8655 - acc: 0.10 - ETA: 0s - loss: 12.8373 - acc: 0.10 - ETA: 0s - loss: 12.8249 - acc: 0.10 - ETA: 0s - loss: 12.8142 - acc: 0.10 - ETA: 0s - loss: 12.7795 - acc: 0.10 - ETA: 0s - loss: 12.7669 - acc: 0.1102Epoch 00000: val_loss improved from inf to 11.11253, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 12.7441 - acc: 0.1111 - val_loss: 11.1125 - val_acc: 0.2012\n",
      "Epoch 2/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 12.3323 - acc: 0.20 - ETA: 3s - loss: 10.6311 - acc: 0.29 - ETA: 3s - loss: 11.0863 - acc: 0.24 - ETA: 3s - loss: 10.9559 - acc: 0.23 - ETA: 2s - loss: 11.1386 - acc: 0.21 - ETA: 2s - loss: 11.0956 - acc: 0.22 - ETA: 2s - loss: 10.8747 - acc: 0.23 - ETA: 2s - loss: 10.9806 - acc: 0.22 - ETA: 2s - loss: 10.9895 - acc: 0.22 - ETA: 2s - loss: 10.9844 - acc: 0.23 - ETA: 2s - loss: 10.9257 - acc: 0.23 - ETA: 2s - loss: 10.8603 - acc: 0.23 - ETA: 2s - loss: 10.8417 - acc: 0.23 - ETA: 2s - loss: 10.8411 - acc: 0.23 - ETA: 2s - loss: 10.8152 - acc: 0.23 - ETA: 2s - loss: 10.7539 - acc: 0.24 - ETA: 2s - loss: 10.8080 - acc: 0.24 - ETA: 2s - loss: 10.7616 - acc: 0.24 - ETA: 2s - loss: 10.7576 - acc: 0.24 - ETA: 2s - loss: 10.7429 - acc: 0.24 - ETA: 2s - loss: 10.7326 - acc: 0.24 - ETA: 2s - loss: 10.7310 - acc: 0.24 - ETA: 1s - loss: 10.7245 - acc: 0.24 - ETA: 1s - loss: 10.6746 - acc: 0.25 - ETA: 1s - loss: 10.6772 - acc: 0.25 - ETA: 1s - loss: 10.6659 - acc: 0.25 - ETA: 1s - loss: 10.6388 - acc: 0.25 - ETA: 1s - loss: 10.5961 - acc: 0.25 - ETA: 1s - loss: 10.5910 - acc: 0.25 - ETA: 1s - loss: 10.6087 - acc: 0.25 - ETA: 1s - loss: 10.6096 - acc: 0.25 - ETA: 1s - loss: 10.6151 - acc: 0.25 - ETA: 1s - loss: 10.5981 - acc: 0.25 - ETA: 1s - loss: 10.6064 - acc: 0.25 - ETA: 1s - loss: 10.6004 - acc: 0.25 - ETA: 1s - loss: 10.5906 - acc: 0.26 - ETA: 1s - loss: 10.5663 - acc: 0.26 - ETA: 1s - loss: 10.5945 - acc: 0.26 - ETA: 1s - loss: 10.5645 - acc: 0.26 - ETA: 1s - loss: 10.5454 - acc: 0.26 - ETA: 0s - loss: 10.5714 - acc: 0.26 - ETA: 0s - loss: 10.5461 - acc: 0.26 - ETA: 0s - loss: 10.5805 - acc: 0.26 - ETA: 0s - loss: 10.5785 - acc: 0.26 - ETA: 0s - loss: 10.5948 - acc: 0.26 - ETA: 0s - loss: 10.5980 - acc: 0.26 - ETA: 0s - loss: 10.5864 - acc: 0.26 - ETA: 0s - loss: 10.5858 - acc: 0.26 - ETA: 0s - loss: 10.5925 - acc: 0.26 - ETA: 0s - loss: 10.5843 - acc: 0.26 - ETA: 0s - loss: 10.5796 - acc: 0.26 - ETA: 0s - loss: 10.5870 - acc: 0.26 - ETA: 0s - loss: 10.5770 - acc: 0.26 - ETA: 0s - loss: 10.5722 - acc: 0.26 - ETA: 0s - loss: 10.5764 - acc: 0.26 - ETA: 0s - loss: 10.5664 - acc: 0.26 - ETA: 0s - loss: 10.5663 - acc: 0.26 - ETA: 0s - loss: 10.5683 - acc: 0.2668Epoch 00001: val_loss improved from 11.11253 to 10.46135, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.5853 - acc: 0.2659 - val_loss: 10.4614 - val_acc: 0.2719\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 7.4339 - acc: 0.500 - ETA: 3s - loss: 10.1003 - acc: 0.30 - ETA: 3s - loss: 10.3584 - acc: 0.29 - ETA: 3s - loss: 10.3164 - acc: 0.30 - ETA: 3s - loss: 10.0695 - acc: 0.31 - ETA: 2s - loss: 9.8533 - acc: 0.3310 - ETA: 2s - loss: 10.0237 - acc: 0.32 - ETA: 2s - loss: 9.9877 - acc: 0.3275 - ETA: 2s - loss: 10.2077 - acc: 0.31 - ETA: 2s - loss: 10.2725 - acc: 0.30 - ETA: 2s - loss: 10.2497 - acc: 0.30 - ETA: 2s - loss: 10.2540 - acc: 0.31 - ETA: 2s - loss: 10.2391 - acc: 0.30 - ETA: 2s - loss: 10.1901 - acc: 0.31 - ETA: 2s - loss: 10.2623 - acc: 0.30 - ETA: 2s - loss: 10.2116 - acc: 0.30 - ETA: 2s - loss: 10.1887 - acc: 0.30 - ETA: 2s - loss: 10.2257 - acc: 0.30 - ETA: 2s - loss: 10.2694 - acc: 0.30 - ETA: 2s - loss: 10.3150 - acc: 0.30 - ETA: 2s - loss: 10.2810 - acc: 0.30 - ETA: 2s - loss: 10.2686 - acc: 0.30 - ETA: 1s - loss: 10.2566 - acc: 0.30 - ETA: 1s - loss: 10.2616 - acc: 0.30 - ETA: 1s - loss: 10.2317 - acc: 0.30 - ETA: 1s - loss: 10.2080 - acc: 0.30 - ETA: 1s - loss: 10.2002 - acc: 0.30 - ETA: 1s - loss: 10.2143 - acc: 0.30 - ETA: 1s - loss: 10.1955 - acc: 0.31 - ETA: 1s - loss: 10.2646 - acc: 0.30 - ETA: 1s - loss: 10.2340 - acc: 0.30 - ETA: 1s - loss: 10.2064 - acc: 0.30 - ETA: 1s - loss: 10.2092 - acc: 0.30 - ETA: 1s - loss: 10.2165 - acc: 0.30 - ETA: 1s - loss: 10.1957 - acc: 0.30 - ETA: 1s - loss: 10.2211 - acc: 0.30 - ETA: 1s - loss: 10.2265 - acc: 0.30 - ETA: 1s - loss: 10.2359 - acc: 0.30 - ETA: 1s - loss: 10.2437 - acc: 0.30 - ETA: 1s - loss: 10.2427 - acc: 0.30 - ETA: 1s - loss: 10.2093 - acc: 0.30 - ETA: 0s - loss: 10.2015 - acc: 0.30 - ETA: 0s - loss: 10.1981 - acc: 0.30 - ETA: 0s - loss: 10.2064 - acc: 0.30 - ETA: 0s - loss: 10.1781 - acc: 0.31 - ETA: 0s - loss: 10.1833 - acc: 0.31 - ETA: 0s - loss: 10.1583 - acc: 0.31 - ETA: 0s - loss: 10.1453 - acc: 0.31 - ETA: 0s - loss: 10.1367 - acc: 0.31 - ETA: 0s - loss: 10.1236 - acc: 0.31 - ETA: 0s - loss: 10.1115 - acc: 0.31 - ETA: 0s - loss: 10.0908 - acc: 0.31 - ETA: 0s - loss: 10.0864 - acc: 0.31 - ETA: 0s - loss: 10.0690 - acc: 0.31 - ETA: 0s - loss: 10.0507 - acc: 0.31 - ETA: 0s - loss: 10.0745 - acc: 0.31 - ETA: 0s - loss: 10.0545 - acc: 0.31 - ETA: 0s - loss: 10.0474 - acc: 0.31 - ETA: 0s - loss: 10.0445 - acc: 0.3193Epoch 00002: val_loss improved from 10.46135 to 10.15213, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.0449 - acc: 0.3195 - val_loss: 10.1521 - val_acc: 0.2982\n",
      "Epoch 4/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 9.4201 - acc: 0.350 - ETA: 3s - loss: 9.9833 - acc: 0.342 - ETA: 3s - loss: 9.8775 - acc: 0.350 - ETA: 3s - loss: 9.9531 - acc: 0.347 - ETA: 2s - loss: 10.1836 - acc: 0.33 - ETA: 2s - loss: 10.2295 - acc: 0.32 - ETA: 2s - loss: 10.0666 - acc: 0.33 - ETA: 2s - loss: 9.9587 - acc: 0.3452 - ETA: 2s - loss: 9.9378 - acc: 0.349 - ETA: 2s - loss: 9.9019 - acc: 0.351 - ETA: 2s - loss: 9.8754 - acc: 0.354 - ETA: 2s - loss: 9.8965 - acc: 0.353 - ETA: 2s - loss: 9.9284 - acc: 0.351 - ETA: 2s - loss: 9.8811 - acc: 0.355 - ETA: 2s - loss: 9.8588 - acc: 0.354 - ETA: 2s - loss: 9.8275 - acc: 0.356 - ETA: 2s - loss: 9.8444 - acc: 0.355 - ETA: 2s - loss: 9.8609 - acc: 0.353 - ETA: 2s - loss: 9.9442 - acc: 0.349 - ETA: 2s - loss: 9.9249 - acc: 0.348 - ETA: 2s - loss: 9.8858 - acc: 0.350 - ETA: 2s - loss: 9.9084 - acc: 0.350 - ETA: 1s - loss: 9.9050 - acc: 0.350 - ETA: 1s - loss: 9.9038 - acc: 0.350 - ETA: 1s - loss: 9.9097 - acc: 0.350 - ETA: 1s - loss: 9.8958 - acc: 0.350 - ETA: 1s - loss: 9.8709 - acc: 0.351 - ETA: 1s - loss: 9.8900 - acc: 0.350 - ETA: 1s - loss: 9.8146 - acc: 0.353 - ETA: 1s - loss: 9.8788 - acc: 0.350 - ETA: 1s - loss: 9.8776 - acc: 0.350 - ETA: 1s - loss: 9.8683 - acc: 0.350 - ETA: 1s - loss: 9.8467 - acc: 0.350 - ETA: 1s - loss: 9.8670 - acc: 0.349 - ETA: 1s - loss: 9.8342 - acc: 0.350 - ETA: 1s - loss: 9.8166 - acc: 0.352 - ETA: 1s - loss: 9.8481 - acc: 0.350 - ETA: 1s - loss: 9.8366 - acc: 0.351 - ETA: 1s - loss: 9.8386 - acc: 0.350 - ETA: 0s - loss: 9.8278 - acc: 0.350 - ETA: 0s - loss: 9.8149 - acc: 0.350 - ETA: 0s - loss: 9.8356 - acc: 0.349 - ETA: 0s - loss: 9.8301 - acc: 0.349 - ETA: 0s - loss: 9.8516 - acc: 0.348 - ETA: 0s - loss: 9.8357 - acc: 0.349 - ETA: 0s - loss: 9.8286 - acc: 0.349 - ETA: 0s - loss: 9.8331 - acc: 0.348 - ETA: 0s - loss: 9.8421 - acc: 0.348 - ETA: 0s - loss: 9.8216 - acc: 0.349 - ETA: 0s - loss: 9.8341 - acc: 0.348 - ETA: 0s - loss: 9.8364 - acc: 0.348 - ETA: 0s - loss: 9.8536 - acc: 0.348 - ETA: 0s - loss: 9.8434 - acc: 0.348 - ETA: 0s - loss: 9.8243 - acc: 0.350 - ETA: 0s - loss: 9.8257 - acc: 0.350 - ETA: 0s - loss: 9.7978 - acc: 0.352 - ETA: 0s - loss: 9.7970 - acc: 0.3526Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.7986 - acc: 0.3525 - val_loss: 10.2109 - val_acc: 0.3006\n",
      "Epoch 5/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 10.0964 - acc: 0.35 - ETA: 2s - loss: 9.7901 - acc: 0.3429 - ETA: 2s - loss: 9.0983 - acc: 0.390 - ETA: 2s - loss: 9.3160 - acc: 0.385 - ETA: 2s - loss: 9.5151 - acc: 0.375 - ETA: 2s - loss: 9.6513 - acc: 0.365 - ETA: 2s - loss: 9.8696 - acc: 0.350 - ETA: 2s - loss: 9.7344 - acc: 0.362 - ETA: 2s - loss: 9.6992 - acc: 0.364 - ETA: 2s - loss: 9.7502 - acc: 0.359 - ETA: 2s - loss: 9.5449 - acc: 0.369 - ETA: 2s - loss: 9.5873 - acc: 0.367 - ETA: 2s - loss: 9.6262 - acc: 0.366 - ETA: 2s - loss: 9.6086 - acc: 0.367 - ETA: 2s - loss: 9.6052 - acc: 0.367 - ETA: 2s - loss: 9.6464 - acc: 0.367 - ETA: 2s - loss: 9.6654 - acc: 0.364 - ETA: 2s - loss: 9.6689 - acc: 0.362 - ETA: 2s - loss: 9.7553 - acc: 0.357 - ETA: 2s - loss: 9.7713 - acc: 0.355 - ETA: 1s - loss: 9.7408 - acc: 0.357 - ETA: 1s - loss: 9.7081 - acc: 0.359 - ETA: 1s - loss: 9.6829 - acc: 0.362 - ETA: 1s - loss: 9.6813 - acc: 0.362 - ETA: 1s - loss: 9.6516 - acc: 0.364 - ETA: 1s - loss: 9.6786 - acc: 0.363 - ETA: 1s - loss: 9.6988 - acc: 0.361 - ETA: 1s - loss: 9.7109 - acc: 0.361 - ETA: 1s - loss: 9.7388 - acc: 0.359 - ETA: 1s - loss: 9.7145 - acc: 0.361 - ETA: 1s - loss: 9.7338 - acc: 0.360 - ETA: 1s - loss: 9.7375 - acc: 0.360 - ETA: 1s - loss: 9.7575 - acc: 0.359 - ETA: 1s - loss: 9.7418 - acc: 0.359 - ETA: 1s - loss: 9.6780 - acc: 0.363 - ETA: 1s - loss: 9.6664 - acc: 0.364 - ETA: 1s - loss: 9.6342 - acc: 0.366 - ETA: 1s - loss: 9.6446 - acc: 0.365 - ETA: 1s - loss: 9.6547 - acc: 0.365 - ETA: 0s - loss: 9.6380 - acc: 0.367 - ETA: 0s - loss: 9.6425 - acc: 0.366 - ETA: 0s - loss: 9.6270 - acc: 0.367 - ETA: 0s - loss: 9.6322 - acc: 0.367 - ETA: 0s - loss: 9.6212 - acc: 0.368 - ETA: 0s - loss: 9.6294 - acc: 0.368 - ETA: 0s - loss: 9.6207 - acc: 0.368 - ETA: 0s - loss: 9.6202 - acc: 0.368 - ETA: 0s - loss: 9.6082 - acc: 0.368 - ETA: 0s - loss: 9.6181 - acc: 0.368 - ETA: 0s - loss: 9.6158 - acc: 0.368 - ETA: 0s - loss: 9.6247 - acc: 0.367 - ETA: 0s - loss: 9.6364 - acc: 0.367 - ETA: 0s - loss: 9.6446 - acc: 0.367 - ETA: 0s - loss: 9.6405 - acc: 0.367 - ETA: 0s - loss: 9.6360 - acc: 0.367 - ETA: 0s - loss: 9.6328 - acc: 0.3675Epoch 00004: val_loss improved from 10.15213 to 9.78425, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.6032 - acc: 0.3696 - val_loss: 9.7842 - val_acc: 0.3305\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 9.7190 - acc: 0.350 - ETA: 3s - loss: 9.5674 - acc: 0.378 - ETA: 3s - loss: 9.3854 - acc: 0.392 - ETA: 3s - loss: 9.3856 - acc: 0.386 - ETA: 2s - loss: 9.5001 - acc: 0.376 - ETA: 2s - loss: 9.1795 - acc: 0.400 - ETA: 2s - loss: 9.0342 - acc: 0.410 - ETA: 2s - loss: 9.0474 - acc: 0.409 - ETA: 2s - loss: 9.1077 - acc: 0.407 - ETA: 2s - loss: 9.2391 - acc: 0.400 - ETA: 2s - loss: 9.3340 - acc: 0.394 - ETA: 2s - loss: 9.3178 - acc: 0.397 - ETA: 2s - loss: 9.3274 - acc: 0.397 - ETA: 2s - loss: 9.3537 - acc: 0.396 - ETA: 2s - loss: 9.3910 - acc: 0.393 - ETA: 2s - loss: 9.4322 - acc: 0.390 - ETA: 2s - loss: 9.4086 - acc: 0.391 - ETA: 2s - loss: 9.4030 - acc: 0.391 - ETA: 2s - loss: 9.4061 - acc: 0.391 - ETA: 2s - loss: 9.3948 - acc: 0.392 - ETA: 1s - loss: 9.3274 - acc: 0.396 - ETA: 1s - loss: 9.3722 - acc: 0.394 - ETA: 1s - loss: 9.4426 - acc: 0.389 - ETA: 1s - loss: 9.4348 - acc: 0.391 - ETA: 1s - loss: 9.4287 - acc: 0.391 - ETA: 1s - loss: 9.4433 - acc: 0.390 - ETA: 1s - loss: 9.3621 - acc: 0.394 - ETA: 1s - loss: 9.4079 - acc: 0.392 - ETA: 1s - loss: 9.4449 - acc: 0.388 - ETA: 1s - loss: 9.4220 - acc: 0.389 - ETA: 1s - loss: 9.4181 - acc: 0.390 - ETA: 1s - loss: 9.4063 - acc: 0.390 - ETA: 1s - loss: 9.3886 - acc: 0.390 - ETA: 1s - loss: 9.3521 - acc: 0.392 - ETA: 1s - loss: 9.3751 - acc: 0.391 - ETA: 1s - loss: 9.3528 - acc: 0.393 - ETA: 1s - loss: 9.3442 - acc: 0.394 - ETA: 1s - loss: 9.3723 - acc: 0.392 - ETA: 1s - loss: 9.3463 - acc: 0.393 - ETA: 0s - loss: 9.3468 - acc: 0.393 - ETA: 0s - loss: 9.3704 - acc: 0.391 - ETA: 0s - loss: 9.3700 - acc: 0.392 - ETA: 0s - loss: 9.3625 - acc: 0.392 - ETA: 0s - loss: 9.3437 - acc: 0.393 - ETA: 0s - loss: 9.3673 - acc: 0.392 - ETA: 0s - loss: 9.3759 - acc: 0.391 - ETA: 0s - loss: 9.3709 - acc: 0.391 - ETA: 0s - loss: 9.3773 - acc: 0.391 - ETA: 0s - loss: 9.3850 - acc: 0.390 - ETA: 0s - loss: 9.4051 - acc: 0.389 - ETA: 0s - loss: 9.3946 - acc: 0.390 - ETA: 0s - loss: 9.3988 - acc: 0.390 - ETA: 0s - loss: 9.4033 - acc: 0.389 - ETA: 0s - loss: 9.4206 - acc: 0.389 - ETA: 0s - loss: 9.4135 - acc: 0.389 - ETA: 0s - loss: 9.3938 - acc: 0.391 - ETA: 0s - loss: 9.4055 - acc: 0.389 - ETA: 0s - loss: 9.4051 - acc: 0.389 - ETA: 0s - loss: 9.3992 - acc: 0.390 - ETA: 0s - loss: 9.4052 - acc: 0.3899Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.4108 - acc: 0.3897 - val_loss: 9.8678 - val_acc: 0.3257\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 3s - loss: 9.6851 - acc: 0.400 - ETA: 3s - loss: 9.3158 - acc: 0.400 - ETA: 3s - loss: 9.3991 - acc: 0.404 - ETA: 3s - loss: 9.0688 - acc: 0.417 - ETA: 3s - loss: 9.1658 - acc: 0.408 - ETA: 3s - loss: 9.0199 - acc: 0.417 - ETA: 2s - loss: 9.2085 - acc: 0.407 - ETA: 2s - loss: 9.3984 - acc: 0.397 - ETA: 2s - loss: 9.3978 - acc: 0.397 - ETA: 2s - loss: 9.3542 - acc: 0.402 - ETA: 2s - loss: 9.3731 - acc: 0.403 - ETA: 2s - loss: 9.2884 - acc: 0.409 - ETA: 2s - loss: 9.3394 - acc: 0.406 - ETA: 2s - loss: 9.2638 - acc: 0.411 - ETA: 2s - loss: 9.2399 - acc: 0.410 - ETA: 2s - loss: 9.1794 - acc: 0.414 - ETA: 2s - loss: 9.0981 - acc: 0.418 - ETA: 2s - loss: 9.0666 - acc: 0.421 - ETA: 2s - loss: 9.0676 - acc: 0.420 - ETA: 2s - loss: 9.0235 - acc: 0.423 - ETA: 2s - loss: 9.0309 - acc: 0.421 - ETA: 1s - loss: 9.0814 - acc: 0.419 - ETA: 1s - loss: 9.0784 - acc: 0.420 - ETA: 1s - loss: 9.0578 - acc: 0.421 - ETA: 1s - loss: 9.1091 - acc: 0.418 - ETA: 1s - loss: 9.0663 - acc: 0.420 - ETA: 1s - loss: 9.0716 - acc: 0.420 - ETA: 1s - loss: 9.1081 - acc: 0.417 - ETA: 1s - loss: 9.1155 - acc: 0.417 - ETA: 1s - loss: 9.1460 - acc: 0.415 - ETA: 1s - loss: 9.1421 - acc: 0.414 - ETA: 1s - loss: 9.1755 - acc: 0.412 - ETA: 1s - loss: 9.1588 - acc: 0.413 - ETA: 1s - loss: 9.2004 - acc: 0.411 - ETA: 1s - loss: 9.2080 - acc: 0.410 - ETA: 1s - loss: 9.2235 - acc: 0.408 - ETA: 1s - loss: 9.2164 - acc: 0.409 - ETA: 1s - loss: 9.2176 - acc: 0.409 - ETA: 1s - loss: 9.2133 - acc: 0.408 - ETA: 0s - loss: 9.2130 - acc: 0.408 - ETA: 0s - loss: 9.2329 - acc: 0.407 - ETA: 0s - loss: 9.2309 - acc: 0.407 - ETA: 0s - loss: 9.2300 - acc: 0.406 - ETA: 0s - loss: 9.2232 - acc: 0.406 - ETA: 0s - loss: 9.2271 - acc: 0.406 - ETA: 0s - loss: 9.2450 - acc: 0.404 - ETA: 0s - loss: 9.2329 - acc: 0.405 - ETA: 0s - loss: 9.2485 - acc: 0.403 - ETA: 0s - loss: 9.2651 - acc: 0.401 - ETA: 0s - loss: 9.2714 - acc: 0.401 - ETA: 0s - loss: 9.2657 - acc: 0.402 - ETA: 0s - loss: 9.2818 - acc: 0.401 - ETA: 0s - loss: 9.2772 - acc: 0.401 - ETA: 0s - loss: 9.2709 - acc: 0.402 - ETA: 0s - loss: 9.2793 - acc: 0.401 - ETA: 0s - loss: 9.2923 - acc: 0.400 - ETA: 0s - loss: 9.2857 - acc: 0.4003Epoch 00006: val_loss improved from 9.78425 to 9.60807, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.2809 - acc: 0.4006 - val_loss: 9.6081 - val_acc: 0.3377\n",
      "Epoch 8/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 8.0595 - acc: 0.500 - ETA: 4s - loss: 8.3474 - acc: 0.464 - ETA: 3s - loss: 9.0024 - acc: 0.420 - ETA: 3s - loss: 8.7876 - acc: 0.438 - ETA: 3s - loss: 8.8635 - acc: 0.428 - ETA: 3s - loss: 8.7268 - acc: 0.441 - ETA: 3s - loss: 8.9142 - acc: 0.432 - ETA: 2s - loss: 8.9695 - acc: 0.426 - ETA: 2s - loss: 9.1253 - acc: 0.414 - ETA: 2s - loss: 9.1965 - acc: 0.410 - ETA: 2s - loss: 9.0455 - acc: 0.420 - ETA: 2s - loss: 9.0974 - acc: 0.414 - ETA: 2s - loss: 9.0334 - acc: 0.416 - ETA: 2s - loss: 8.9627 - acc: 0.418 - ETA: 2s - loss: 8.9747 - acc: 0.418 - ETA: 2s - loss: 8.9917 - acc: 0.418 - ETA: 2s - loss: 9.0118 - acc: 0.417 - ETA: 2s - loss: 8.9911 - acc: 0.417 - ETA: 2s - loss: 8.9821 - acc: 0.417 - ETA: 2s - loss: 8.9854 - acc: 0.417 - ETA: 2s - loss: 8.9915 - acc: 0.416 - ETA: 1s - loss: 8.9144 - acc: 0.421 - ETA: 1s - loss: 8.8979 - acc: 0.422 - ETA: 1s - loss: 8.9244 - acc: 0.420 - ETA: 1s - loss: 8.9531 - acc: 0.418 - ETA: 1s - loss: 8.9686 - acc: 0.417 - ETA: 1s - loss: 8.9820 - acc: 0.416 - ETA: 1s - loss: 8.9692 - acc: 0.417 - ETA: 1s - loss: 8.9488 - acc: 0.419 - ETA: 1s - loss: 8.9861 - acc: 0.416 - ETA: 1s - loss: 9.0088 - acc: 0.414 - ETA: 1s - loss: 8.9819 - acc: 0.416 - ETA: 1s - loss: 9.0100 - acc: 0.415 - ETA: 1s - loss: 9.0194 - acc: 0.414 - ETA: 1s - loss: 9.0075 - acc: 0.415 - ETA: 1s - loss: 9.0156 - acc: 0.415 - ETA: 1s - loss: 9.0033 - acc: 0.416 - ETA: 1s - loss: 9.0503 - acc: 0.413 - ETA: 0s - loss: 9.0395 - acc: 0.413 - ETA: 0s - loss: 9.0396 - acc: 0.413 - ETA: 0s - loss: 9.0283 - acc: 0.414 - ETA: 0s - loss: 9.0279 - acc: 0.414 - ETA: 0s - loss: 9.0657 - acc: 0.412 - ETA: 0s - loss: 9.0650 - acc: 0.412 - ETA: 0s - loss: 9.0445 - acc: 0.413 - ETA: 0s - loss: 9.0310 - acc: 0.413 - ETA: 0s - loss: 9.0326 - acc: 0.413 - ETA: 0s - loss: 9.0232 - acc: 0.414 - ETA: 0s - loss: 9.0288 - acc: 0.413 - ETA: 0s - loss: 9.0371 - acc: 0.413 - ETA: 0s - loss: 9.0389 - acc: 0.413 - ETA: 0s - loss: 9.0093 - acc: 0.414 - ETA: 0s - loss: 9.0350 - acc: 0.412 - ETA: 0s - loss: 9.0151 - acc: 0.413 - ETA: 0s - loss: 9.0137 - acc: 0.413 - ETA: 0s - loss: 9.0248 - acc: 0.4131Epoch 00007: val_loss improved from 9.60807 to 9.44918, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.0195 - acc: 0.4129 - val_loss: 9.4492 - val_acc: 0.3461\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 11.2867 - acc: 0.30 - ETA: 3s - loss: 8.9022 - acc: 0.4250 - ETA: 3s - loss: 9.1872 - acc: 0.418 - ETA: 3s - loss: 9.3987 - acc: 0.402 - ETA: 3s - loss: 9.2877 - acc: 0.408 - ETA: 2s - loss: 9.3713 - acc: 0.401 - ETA: 2s - loss: 9.5053 - acc: 0.392 - ETA: 2s - loss: 9.5424 - acc: 0.387 - ETA: 2s - loss: 9.3242 - acc: 0.401 - ETA: 2s - loss: 9.2338 - acc: 0.401 - ETA: 2s - loss: 9.2322 - acc: 0.403 - ETA: 2s - loss: 9.2285 - acc: 0.402 - ETA: 2s - loss: 9.1754 - acc: 0.404 - ETA: 2s - loss: 9.0888 - acc: 0.410 - ETA: 2s - loss: 9.0612 - acc: 0.411 - ETA: 2s - loss: 8.9782 - acc: 0.417 - ETA: 2s - loss: 8.9901 - acc: 0.417 - ETA: 2s - loss: 8.9743 - acc: 0.417 - ETA: 2s - loss: 8.9420 - acc: 0.418 - ETA: 2s - loss: 8.9130 - acc: 0.421 - ETA: 2s - loss: 8.9005 - acc: 0.422 - ETA: 1s - loss: 8.9060 - acc: 0.422 - ETA: 1s - loss: 8.8330 - acc: 0.427 - ETA: 1s - loss: 8.8531 - acc: 0.426 - ETA: 1s - loss: 8.8994 - acc: 0.424 - ETA: 1s - loss: 8.9105 - acc: 0.423 - ETA: 1s - loss: 8.9045 - acc: 0.423 - ETA: 1s - loss: 8.8983 - acc: 0.424 - ETA: 1s - loss: 8.8865 - acc: 0.425 - ETA: 1s - loss: 8.8805 - acc: 0.426 - ETA: 1s - loss: 8.8651 - acc: 0.427 - ETA: 1s - loss: 8.8869 - acc: 0.427 - ETA: 1s - loss: 8.8885 - acc: 0.426 - ETA: 1s - loss: 8.8805 - acc: 0.427 - ETA: 1s - loss: 8.8853 - acc: 0.426 - ETA: 1s - loss: 8.8934 - acc: 0.426 - ETA: 1s - loss: 8.9120 - acc: 0.425 - ETA: 1s - loss: 8.9151 - acc: 0.425 - ETA: 0s - loss: 8.8960 - acc: 0.426 - ETA: 0s - loss: 8.8861 - acc: 0.427 - ETA: 0s - loss: 8.8884 - acc: 0.427 - ETA: 0s - loss: 8.8815 - acc: 0.427 - ETA: 0s - loss: 8.8650 - acc: 0.428 - ETA: 0s - loss: 8.8933 - acc: 0.427 - ETA: 0s - loss: 8.9150 - acc: 0.426 - ETA: 0s - loss: 8.9199 - acc: 0.425 - ETA: 0s - loss: 8.9444 - acc: 0.424 - ETA: 0s - loss: 8.9368 - acc: 0.424 - ETA: 0s - loss: 8.9369 - acc: 0.424 - ETA: 0s - loss: 8.9105 - acc: 0.425 - ETA: 0s - loss: 8.9148 - acc: 0.425 - ETA: 0s - loss: 8.9090 - acc: 0.425 - ETA: 0s - loss: 8.9085 - acc: 0.425 - ETA: 0s - loss: 8.9028 - acc: 0.425 - ETA: 0s - loss: 8.8825 - acc: 0.426 - ETA: 0s - loss: 8.8880 - acc: 0.4265Epoch 00008: val_loss improved from 9.44918 to 9.31059, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.8905 - acc: 0.4265 - val_loss: 9.3106 - val_acc: 0.3617\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 4.8484 - acc: 0.700 - ETA: 2s - loss: 8.7624 - acc: 0.457 - ETA: 2s - loss: 8.4601 - acc: 0.469 - ETA: 2s - loss: 8.8227 - acc: 0.442 - ETA: 2s - loss: 8.9260 - acc: 0.436 - ETA: 2s - loss: 8.9320 - acc: 0.435 - ETA: 2s - loss: 9.0617 - acc: 0.427 - ETA: 2s - loss: 9.0723 - acc: 0.425 - ETA: 2s - loss: 9.0050 - acc: 0.428 - ETA: 2s - loss: 8.8737 - acc: 0.437 - ETA: 2s - loss: 8.8613 - acc: 0.438 - ETA: 2s - loss: 8.8755 - acc: 0.437 - ETA: 2s - loss: 8.7408 - acc: 0.444 - ETA: 2s - loss: 8.7106 - acc: 0.445 - ETA: 2s - loss: 8.7272 - acc: 0.442 - ETA: 2s - loss: 8.7311 - acc: 0.442 - ETA: 2s - loss: 8.6805 - acc: 0.443 - ETA: 2s - loss: 8.7083 - acc: 0.441 - ETA: 2s - loss: 8.7551 - acc: 0.438 - ETA: 2s - loss: 8.7277 - acc: 0.440 - ETA: 2s - loss: 8.7484 - acc: 0.439 - ETA: 1s - loss: 8.7098 - acc: 0.441 - ETA: 1s - loss: 8.7590 - acc: 0.438 - ETA: 1s - loss: 8.8376 - acc: 0.433 - ETA: 1s - loss: 8.8594 - acc: 0.432 - ETA: 1s - loss: 8.8525 - acc: 0.432 - ETA: 1s - loss: 8.8568 - acc: 0.431 - ETA: 1s - loss: 8.8340 - acc: 0.433 - ETA: 1s - loss: 8.7980 - acc: 0.434 - ETA: 1s - loss: 8.8199 - acc: 0.433 - ETA: 1s - loss: 8.8380 - acc: 0.433 - ETA: 1s - loss: 8.7840 - acc: 0.435 - ETA: 1s - loss: 8.8139 - acc: 0.433 - ETA: 1s - loss: 8.8240 - acc: 0.432 - ETA: 1s - loss: 8.7892 - acc: 0.435 - ETA: 1s - loss: 8.7765 - acc: 0.436 - ETA: 1s - loss: 8.7757 - acc: 0.436 - ETA: 1s - loss: 8.7792 - acc: 0.436 - ETA: 1s - loss: 8.7732 - acc: 0.436 - ETA: 0s - loss: 8.7452 - acc: 0.438 - ETA: 0s - loss: 8.7285 - acc: 0.439 - ETA: 0s - loss: 8.7213 - acc: 0.440 - ETA: 0s - loss: 8.7208 - acc: 0.440 - ETA: 0s - loss: 8.7446 - acc: 0.439 - ETA: 0s - loss: 8.7298 - acc: 0.440 - ETA: 0s - loss: 8.7432 - acc: 0.439 - ETA: 0s - loss: 8.7392 - acc: 0.439 - ETA: 0s - loss: 8.7630 - acc: 0.437 - ETA: 0s - loss: 8.7614 - acc: 0.437 - ETA: 0s - loss: 8.7887 - acc: 0.435 - ETA: 0s - loss: 8.7756 - acc: 0.436 - ETA: 0s - loss: 8.8024 - acc: 0.435 - ETA: 0s - loss: 8.7855 - acc: 0.436 - ETA: 0s - loss: 8.7766 - acc: 0.437 - ETA: 0s - loss: 8.7635 - acc: 0.438 - ETA: 0s - loss: 8.7541 - acc: 0.438 - ETA: 0s - loss: 8.7320 - acc: 0.4396Epoch 00009: val_loss improved from 9.31059 to 9.27845, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.7358 - acc: 0.4394 - val_loss: 9.2784 - val_acc: 0.3641\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 7.3103 - acc: 0.550 - ETA: 3s - loss: 9.3024 - acc: 0.414 - ETA: 2s - loss: 9.6160 - acc: 0.396 - ETA: 2s - loss: 9.5296 - acc: 0.400 - ETA: 2s - loss: 9.3938 - acc: 0.408 - ETA: 2s - loss: 9.0344 - acc: 0.428 - ETA: 2s - loss: 8.8124 - acc: 0.443 - ETA: 2s - loss: 8.7511 - acc: 0.445 - ETA: 2s - loss: 8.7836 - acc: 0.443 - ETA: 2s - loss: 8.6499 - acc: 0.452 - ETA: 2s - loss: 8.6609 - acc: 0.451 - ETA: 2s - loss: 8.7277 - acc: 0.446 - ETA: 2s - loss: 8.7158 - acc: 0.447 - ETA: 2s - loss: 8.6780 - acc: 0.449 - ETA: 2s - loss: 8.6602 - acc: 0.450 - ETA: 2s - loss: 8.7164 - acc: 0.446 - ETA: 2s - loss: 8.6499 - acc: 0.451 - ETA: 2s - loss: 8.6136 - acc: 0.453 - ETA: 2s - loss: 8.6207 - acc: 0.453 - ETA: 2s - loss: 8.6160 - acc: 0.453 - ETA: 2s - loss: 8.5985 - acc: 0.454 - ETA: 1s - loss: 8.6408 - acc: 0.451 - ETA: 1s - loss: 8.6531 - acc: 0.450 - ETA: 1s - loss: 8.6530 - acc: 0.451 - ETA: 1s - loss: 8.5960 - acc: 0.454 - ETA: 1s - loss: 8.5915 - acc: 0.455 - ETA: 1s - loss: 8.5652 - acc: 0.457 - ETA: 1s - loss: 8.5731 - acc: 0.456 - ETA: 1s - loss: 8.5388 - acc: 0.458 - ETA: 1s - loss: 8.5816 - acc: 0.455 - ETA: 1s - loss: 8.5672 - acc: 0.456 - ETA: 1s - loss: 8.5750 - acc: 0.456 - ETA: 1s - loss: 8.5777 - acc: 0.455 - ETA: 1s - loss: 8.5586 - acc: 0.456 - ETA: 1s - loss: 8.5630 - acc: 0.455 - ETA: 1s - loss: 8.5925 - acc: 0.453 - ETA: 1s - loss: 8.5985 - acc: 0.452 - ETA: 1s - loss: 8.5919 - acc: 0.453 - ETA: 1s - loss: 8.5823 - acc: 0.453 - ETA: 0s - loss: 8.5694 - acc: 0.454 - ETA: 0s - loss: 8.6058 - acc: 0.452 - ETA: 0s - loss: 8.5998 - acc: 0.452 - ETA: 0s - loss: 8.5921 - acc: 0.453 - ETA: 0s - loss: 8.6273 - acc: 0.451 - ETA: 0s - loss: 8.6141 - acc: 0.452 - ETA: 0s - loss: 8.6426 - acc: 0.449 - ETA: 0s - loss: 8.6329 - acc: 0.450 - ETA: 0s - loss: 8.6286 - acc: 0.451 - ETA: 0s - loss: 8.6435 - acc: 0.450 - ETA: 0s - loss: 8.6361 - acc: 0.450 - ETA: 0s - loss: 8.6520 - acc: 0.449 - ETA: 0s - loss: 8.6415 - acc: 0.449 - ETA: 0s - loss: 8.6287 - acc: 0.450 - ETA: 0s - loss: 8.6393 - acc: 0.449 - ETA: 0s - loss: 8.6345 - acc: 0.449 - ETA: 0s - loss: 8.6318 - acc: 0.450 - ETA: 0s - loss: 8.6189 - acc: 0.4509Epoch 00010: val_loss improved from 9.27845 to 9.12524, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.6281 - acc: 0.4503 - val_loss: 9.1252 - val_acc: 0.3653\n",
      "Epoch 12/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 8.8650 - acc: 0.450 - ETA: 3s - loss: 8.7063 - acc: 0.443 - ETA: 3s - loss: 9.1784 - acc: 0.421 - ETA: 2s - loss: 8.9815 - acc: 0.432 - ETA: 2s - loss: 9.2692 - acc: 0.413 - ETA: 2s - loss: 9.0252 - acc: 0.429 - ETA: 2s - loss: 8.9028 - acc: 0.436 - ETA: 2s - loss: 8.9632 - acc: 0.432 - ETA: 2s - loss: 8.8904 - acc: 0.435 - ETA: 2s - loss: 8.9182 - acc: 0.434 - ETA: 2s - loss: 8.8975 - acc: 0.436 - ETA: 2s - loss: 8.7886 - acc: 0.442 - ETA: 2s - loss: 8.8026 - acc: 0.441 - ETA: 2s - loss: 8.7571 - acc: 0.443 - ETA: 2s - loss: 8.7297 - acc: 0.445 - ETA: 2s - loss: 8.8023 - acc: 0.441 - ETA: 2s - loss: 8.7646 - acc: 0.444 - ETA: 2s - loss: 8.7323 - acc: 0.447 - ETA: 2s - loss: 8.7221 - acc: 0.447 - ETA: 2s - loss: 8.6626 - acc: 0.450 - ETA: 1s - loss: 8.6574 - acc: 0.451 - ETA: 1s - loss: 8.5851 - acc: 0.455 - ETA: 1s - loss: 8.6041 - acc: 0.455 - ETA: 1s - loss: 8.5378 - acc: 0.458 - ETA: 1s - loss: 8.5310 - acc: 0.459 - ETA: 1s - loss: 8.5440 - acc: 0.458 - ETA: 1s - loss: 8.6085 - acc: 0.454 - ETA: 1s - loss: 8.6042 - acc: 0.454 - ETA: 1s - loss: 8.6394 - acc: 0.452 - ETA: 1s - loss: 8.6438 - acc: 0.452 - ETA: 1s - loss: 8.6208 - acc: 0.453 - ETA: 1s - loss: 8.6341 - acc: 0.452 - ETA: 1s - loss: 8.6569 - acc: 0.450 - ETA: 1s - loss: 8.6175 - acc: 0.452 - ETA: 1s - loss: 8.6189 - acc: 0.452 - ETA: 1s - loss: 8.6204 - acc: 0.452 - ETA: 1s - loss: 8.5995 - acc: 0.453 - ETA: 1s - loss: 8.6037 - acc: 0.453 - ETA: 0s - loss: 8.6231 - acc: 0.452 - ETA: 0s - loss: 8.6325 - acc: 0.452 - ETA: 0s - loss: 8.6341 - acc: 0.451 - ETA: 0s - loss: 8.6295 - acc: 0.452 - ETA: 0s - loss: 8.6036 - acc: 0.453 - ETA: 0s - loss: 8.6007 - acc: 0.454 - ETA: 0s - loss: 8.6007 - acc: 0.454 - ETA: 0s - loss: 8.6347 - acc: 0.452 - ETA: 0s - loss: 8.6314 - acc: 0.452 - ETA: 0s - loss: 8.6308 - acc: 0.452 - ETA: 0s - loss: 8.6175 - acc: 0.453 - ETA: 0s - loss: 8.6121 - acc: 0.453 - ETA: 0s - loss: 8.6133 - acc: 0.453 - ETA: 0s - loss: 8.5927 - acc: 0.454 - ETA: 0s - loss: 8.5822 - acc: 0.455 - ETA: 0s - loss: 8.5827 - acc: 0.455 - ETA: 0s - loss: 8.5753 - acc: 0.455 - ETA: 0s - loss: 8.5955 - acc: 0.4541Epoch 00011: val_loss improved from 9.12524 to 9.06220, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.5914 - acc: 0.4543 - val_loss: 9.0622 - val_acc: 0.3796\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 3s - loss: 8.0831 - acc: 0.500 - ETA: 2s - loss: 8.5044 - acc: 0.471 - ETA: 2s - loss: 8.3659 - acc: 0.476 - ETA: 2s - loss: 8.3975 - acc: 0.476 - ETA: 2s - loss: 8.4897 - acc: 0.464 - ETA: 2s - loss: 8.4935 - acc: 0.464 - ETA: 2s - loss: 8.6572 - acc: 0.455 - ETA: 2s - loss: 8.5560 - acc: 0.461 - ETA: 2s - loss: 8.5405 - acc: 0.462 - ETA: 2s - loss: 8.4431 - acc: 0.468 - ETA: 2s - loss: 8.6122 - acc: 0.456 - ETA: 2s - loss: 8.4894 - acc: 0.465 - ETA: 2s - loss: 8.5660 - acc: 0.461 - ETA: 2s - loss: 8.5588 - acc: 0.462 - ETA: 2s - loss: 8.5738 - acc: 0.461 - ETA: 2s - loss: 8.5535 - acc: 0.463 - ETA: 2s - loss: 8.5475 - acc: 0.463 - ETA: 2s - loss: 8.5835 - acc: 0.461 - ETA: 2s - loss: 8.5942 - acc: 0.460 - ETA: 2s - loss: 8.5648 - acc: 0.461 - ETA: 2s - loss: 8.5270 - acc: 0.463 - ETA: 2s - loss: 8.5099 - acc: 0.464 - ETA: 2s - loss: 8.5453 - acc: 0.462 - ETA: 1s - loss: 8.6019 - acc: 0.458 - ETA: 1s - loss: 8.6102 - acc: 0.457 - ETA: 1s - loss: 8.5842 - acc: 0.458 - ETA: 1s - loss: 8.5799 - acc: 0.458 - ETA: 1s - loss: 8.5778 - acc: 0.459 - ETA: 1s - loss: 8.5886 - acc: 0.458 - ETA: 1s - loss: 8.6148 - acc: 0.456 - ETA: 1s - loss: 8.5969 - acc: 0.457 - ETA: 1s - loss: 8.5798 - acc: 0.458 - ETA: 1s - loss: 8.5618 - acc: 0.459 - ETA: 1s - loss: 8.5712 - acc: 0.458 - ETA: 1s - loss: 8.5680 - acc: 0.459 - ETA: 1s - loss: 8.5342 - acc: 0.461 - ETA: 1s - loss: 8.5481 - acc: 0.460 - ETA: 1s - loss: 8.5280 - acc: 0.461 - ETA: 1s - loss: 8.5126 - acc: 0.462 - ETA: 1s - loss: 8.5158 - acc: 0.462 - ETA: 0s - loss: 8.4985 - acc: 0.463 - ETA: 0s - loss: 8.5077 - acc: 0.463 - ETA: 0s - loss: 8.4799 - acc: 0.464 - ETA: 0s - loss: 8.4802 - acc: 0.464 - ETA: 0s - loss: 8.4925 - acc: 0.464 - ETA: 0s - loss: 8.4767 - acc: 0.465 - ETA: 0s - loss: 8.4590 - acc: 0.466 - ETA: 0s - loss: 8.4514 - acc: 0.466 - ETA: 0s - loss: 8.4461 - acc: 0.466 - ETA: 0s - loss: 8.4326 - acc: 0.467 - ETA: 0s - loss: 8.4260 - acc: 0.468 - ETA: 0s - loss: 8.4295 - acc: 0.468 - ETA: 0s - loss: 8.4313 - acc: 0.467 - ETA: 0s - loss: 8.4449 - acc: 0.467 - ETA: 0s - loss: 8.4399 - acc: 0.467 - ETA: 0s - loss: 8.4295 - acc: 0.468 - ETA: 0s - loss: 8.4310 - acc: 0.4681Epoch 00012: val_loss improved from 9.06220 to 8.91309, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.4176 - acc: 0.4689 - val_loss: 8.9131 - val_acc: 0.3880\n",
      "Epoch 14/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 7.2540 - acc: 0.550 - ETA: 3s - loss: 7.6084 - acc: 0.528 - ETA: 3s - loss: 8.1280 - acc: 0.496 - ETA: 2s - loss: 8.0979 - acc: 0.497 - ETA: 2s - loss: 8.1533 - acc: 0.494 - ETA: 2s - loss: 8.4547 - acc: 0.474 - ETA: 2s - loss: 8.5343 - acc: 0.469 - ETA: 2s - loss: 8.2408 - acc: 0.487 - ETA: 2s - loss: 8.0652 - acc: 0.497 - ETA: 2s - loss: 8.0963 - acc: 0.496 - ETA: 2s - loss: 8.0528 - acc: 0.498 - ETA: 2s - loss: 8.0015 - acc: 0.501 - ETA: 2s - loss: 8.0423 - acc: 0.499 - ETA: 2s - loss: 8.0571 - acc: 0.497 - ETA: 2s - loss: 8.0802 - acc: 0.495 - ETA: 2s - loss: 8.0040 - acc: 0.500 - ETA: 2s - loss: 8.0969 - acc: 0.494 - ETA: 2s - loss: 8.1550 - acc: 0.490 - ETA: 2s - loss: 8.2043 - acc: 0.487 - ETA: 2s - loss: 8.2267 - acc: 0.485 - ETA: 2s - loss: 8.2063 - acc: 0.486 - ETA: 2s - loss: 8.2335 - acc: 0.484 - ETA: 1s - loss: 8.2378 - acc: 0.484 - ETA: 1s - loss: 8.2144 - acc: 0.485 - ETA: 1s - loss: 8.1914 - acc: 0.486 - ETA: 1s - loss: 8.2469 - acc: 0.483 - ETA: 1s - loss: 8.2076 - acc: 0.485 - ETA: 1s - loss: 8.2076 - acc: 0.485 - ETA: 1s - loss: 8.1974 - acc: 0.486 - ETA: 1s - loss: 8.2406 - acc: 0.483 - ETA: 1s - loss: 8.2766 - acc: 0.481 - ETA: 1s - loss: 8.2828 - acc: 0.481 - ETA: 1s - loss: 8.3443 - acc: 0.477 - ETA: 1s - loss: 8.3594 - acc: 0.476 - ETA: 1s - loss: 8.3733 - acc: 0.475 - ETA: 1s - loss: 8.3546 - acc: 0.476 - ETA: 1s - loss: 8.3959 - acc: 0.473 - ETA: 1s - loss: 8.3682 - acc: 0.475 - ETA: 1s - loss: 8.3689 - acc: 0.475 - ETA: 1s - loss: 8.3624 - acc: 0.475 - ETA: 0s - loss: 8.3711 - acc: 0.475 - ETA: 0s - loss: 8.3618 - acc: 0.475 - ETA: 0s - loss: 8.3625 - acc: 0.475 - ETA: 0s - loss: 8.3425 - acc: 0.476 - ETA: 0s - loss: 8.3518 - acc: 0.476 - ETA: 0s - loss: 8.3652 - acc: 0.475 - ETA: 0s - loss: 8.3752 - acc: 0.474 - ETA: 0s - loss: 8.3657 - acc: 0.474 - ETA: 0s - loss: 8.3874 - acc: 0.473 - ETA: 0s - loss: 8.3862 - acc: 0.473 - ETA: 0s - loss: 8.3978 - acc: 0.472 - ETA: 0s - loss: 8.3857 - acc: 0.473 - ETA: 0s - loss: 8.4082 - acc: 0.472 - ETA: 0s - loss: 8.4058 - acc: 0.472 - ETA: 0s - loss: 8.4345 - acc: 0.470 - ETA: 0s - loss: 8.4204 - acc: 0.471 - ETA: 0s - loss: 8.4076 - acc: 0.4719Epoch 00013: val_loss improved from 8.91309 to 8.91038, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.3883 - acc: 0.4731 - val_loss: 8.9104 - val_acc: 0.3940\n",
      "Epoch 15/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 11.2828 - acc: 0.30 - ETA: 2s - loss: 9.2358 - acc: 0.4071 - ETA: 3s - loss: 8.9462 - acc: 0.430 - ETA: 3s - loss: 8.9047 - acc: 0.430 - ETA: 3s - loss: 8.8615 - acc: 0.437 - ETA: 2s - loss: 8.6732 - acc: 0.448 - ETA: 2s - loss: 8.4991 - acc: 0.461 - ETA: 2s - loss: 8.5723 - acc: 0.458 - ETA: 2s - loss: 8.6137 - acc: 0.456 - ETA: 2s - loss: 8.5803 - acc: 0.459 - ETA: 2s - loss: 8.5706 - acc: 0.460 - ETA: 2s - loss: 8.5685 - acc: 0.460 - ETA: 2s - loss: 8.6354 - acc: 0.456 - ETA: 2s - loss: 8.6635 - acc: 0.454 - ETA: 2s - loss: 8.6033 - acc: 0.454 - ETA: 2s - loss: 8.5665 - acc: 0.455 - ETA: 2s - loss: 8.5581 - acc: 0.455 - ETA: 2s - loss: 8.6316 - acc: 0.451 - ETA: 2s - loss: 8.6073 - acc: 0.452 - ETA: 2s - loss: 8.5895 - acc: 0.453 - ETA: 2s - loss: 8.5846 - acc: 0.452 - ETA: 2s - loss: 8.5754 - acc: 0.454 - ETA: 1s - loss: 8.5474 - acc: 0.454 - ETA: 1s - loss: 8.5399 - acc: 0.455 - ETA: 1s - loss: 8.4979 - acc: 0.457 - ETA: 1s - loss: 8.4894 - acc: 0.458 - ETA: 1s - loss: 8.4959 - acc: 0.457 - ETA: 1s - loss: 8.4659 - acc: 0.459 - ETA: 1s - loss: 8.4709 - acc: 0.459 - ETA: 1s - loss: 8.4660 - acc: 0.458 - ETA: 1s - loss: 8.4866 - acc: 0.457 - ETA: 1s - loss: 8.5073 - acc: 0.456 - ETA: 1s - loss: 8.4964 - acc: 0.456 - ETA: 1s - loss: 8.4835 - acc: 0.457 - ETA: 1s - loss: 8.4566 - acc: 0.459 - ETA: 1s - loss: 8.4722 - acc: 0.458 - ETA: 1s - loss: 8.4502 - acc: 0.460 - ETA: 1s - loss: 8.3866 - acc: 0.463 - ETA: 1s - loss: 8.3866 - acc: 0.463 - ETA: 0s - loss: 8.4076 - acc: 0.462 - ETA: 0s - loss: 8.3559 - acc: 0.466 - ETA: 0s - loss: 8.3902 - acc: 0.464 - ETA: 0s - loss: 8.3614 - acc: 0.466 - ETA: 0s - loss: 8.3550 - acc: 0.466 - ETA: 0s - loss: 8.3374 - acc: 0.467 - ETA: 0s - loss: 8.3285 - acc: 0.468 - ETA: 0s - loss: 8.3509 - acc: 0.467 - ETA: 0s - loss: 8.3389 - acc: 0.468 - ETA: 0s - loss: 8.2991 - acc: 0.471 - ETA: 0s - loss: 8.2955 - acc: 0.470 - ETA: 0s - loss: 8.2978 - acc: 0.470 - ETA: 0s - loss: 8.3101 - acc: 0.470 - ETA: 0s - loss: 8.3059 - acc: 0.470 - ETA: 0s - loss: 8.3015 - acc: 0.470 - ETA: 0s - loss: 8.3144 - acc: 0.470 - ETA: 0s - loss: 8.3147 - acc: 0.470 - ETA: 0s - loss: 8.2989 - acc: 0.4715Epoch 00014: val_loss improved from 8.91038 to 8.88386, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.3168 - acc: 0.4705 - val_loss: 8.8839 - val_acc: 0.3904\n",
      "Epoch 16/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 7.2532 - acc: 0.550 - ETA: 3s - loss: 8.4112 - acc: 0.478 - ETA: 3s - loss: 8.5643 - acc: 0.465 - ETA: 2s - loss: 8.6617 - acc: 0.460 - ETA: 2s - loss: 8.6600 - acc: 0.460 - ETA: 2s - loss: 8.3889 - acc: 0.474 - ETA: 2s - loss: 8.3377 - acc: 0.474 - ETA: 2s - loss: 8.3761 - acc: 0.472 - ETA: 2s - loss: 8.2882 - acc: 0.478 - ETA: 2s - loss: 8.2564 - acc: 0.480 - ETA: 2s - loss: 8.2427 - acc: 0.482 - ETA: 2s - loss: 8.1996 - acc: 0.484 - ETA: 2s - loss: 8.3205 - acc: 0.477 - ETA: 2s - loss: 8.2934 - acc: 0.478 - ETA: 2s - loss: 8.2854 - acc: 0.478 - ETA: 2s - loss: 8.2530 - acc: 0.479 - ETA: 2s - loss: 8.2765 - acc: 0.478 - ETA: 2s - loss: 8.2530 - acc: 0.479 - ETA: 2s - loss: 8.2821 - acc: 0.478 - ETA: 2s - loss: 8.2712 - acc: 0.479 - ETA: 2s - loss: 8.2400 - acc: 0.479 - ETA: 1s - loss: 8.2136 - acc: 0.481 - ETA: 1s - loss: 8.2232 - acc: 0.480 - ETA: 1s - loss: 8.2883 - acc: 0.476 - ETA: 1s - loss: 8.2477 - acc: 0.478 - ETA: 1s - loss: 8.2250 - acc: 0.479 - ETA: 1s - loss: 8.2196 - acc: 0.479 - ETA: 1s - loss: 8.2019 - acc: 0.480 - ETA: 1s - loss: 8.2451 - acc: 0.478 - ETA: 1s - loss: 8.2637 - acc: 0.476 - ETA: 1s - loss: 8.2539 - acc: 0.477 - ETA: 1s - loss: 8.2352 - acc: 0.478 - ETA: 1s - loss: 8.2593 - acc: 0.477 - ETA: 1s - loss: 8.2774 - acc: 0.475 - ETA: 1s - loss: 8.2497 - acc: 0.477 - ETA: 1s - loss: 8.2446 - acc: 0.477 - ETA: 1s - loss: 8.2251 - acc: 0.479 - ETA: 1s - loss: 8.2061 - acc: 0.480 - ETA: 1s - loss: 8.1868 - acc: 0.481 - ETA: 0s - loss: 8.1961 - acc: 0.480 - ETA: 0s - loss: 8.2116 - acc: 0.480 - ETA: 0s - loss: 8.2227 - acc: 0.479 - ETA: 0s - loss: 8.2061 - acc: 0.480 - ETA: 0s - loss: 8.2008 - acc: 0.480 - ETA: 0s - loss: 8.2116 - acc: 0.480 - ETA: 0s - loss: 8.2476 - acc: 0.477 - ETA: 0s - loss: 8.2481 - acc: 0.477 - ETA: 0s - loss: 8.2381 - acc: 0.478 - ETA: 0s - loss: 8.2232 - acc: 0.479 - ETA: 0s - loss: 8.2205 - acc: 0.479 - ETA: 0s - loss: 8.2265 - acc: 0.479 - ETA: 0s - loss: 8.2321 - acc: 0.478 - ETA: 0s - loss: 8.2313 - acc: 0.478 - ETA: 0s - loss: 8.2157 - acc: 0.479 - ETA: 0s - loss: 8.2069 - acc: 0.479 - ETA: 0s - loss: 8.1995 - acc: 0.4804Epoch 00015: val_loss improved from 8.88386 to 8.84395, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.2192 - acc: 0.4793 - val_loss: 8.8440 - val_acc: 0.3928\n",
      "Epoch 17/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 4.8389 - acc: 0.700 - ETA: 2s - loss: 7.0635 - acc: 0.550 - ETA: 2s - loss: 7.4159 - acc: 0.526 - ETA: 2s - loss: 7.8826 - acc: 0.502 - ETA: 2s - loss: 7.7443 - acc: 0.511 - ETA: 2s - loss: 8.1789 - acc: 0.485 - ETA: 2s - loss: 8.1251 - acc: 0.487 - ETA: 2s - loss: 8.1182 - acc: 0.487 - ETA: 2s - loss: 8.1797 - acc: 0.485 - ETA: 2s - loss: 8.3067 - acc: 0.476 - ETA: 2s - loss: 8.3637 - acc: 0.473 - ETA: 2s - loss: 8.4469 - acc: 0.468 - ETA: 2s - loss: 8.4029 - acc: 0.471 - ETA: 2s - loss: 8.4264 - acc: 0.470 - ETA: 2s - loss: 8.4096 - acc: 0.471 - ETA: 2s - loss: 8.4227 - acc: 0.470 - ETA: 2s - loss: 8.2872 - acc: 0.478 - ETA: 2s - loss: 8.2385 - acc: 0.482 - ETA: 2s - loss: 8.2219 - acc: 0.483 - ETA: 2s - loss: 8.2143 - acc: 0.483 - ETA: 2s - loss: 8.2262 - acc: 0.482 - ETA: 1s - loss: 8.2770 - acc: 0.479 - ETA: 1s - loss: 8.2954 - acc: 0.478 - ETA: 1s - loss: 8.2812 - acc: 0.478 - ETA: 1s - loss: 8.2323 - acc: 0.482 - ETA: 1s - loss: 8.2151 - acc: 0.483 - ETA: 1s - loss: 8.1410 - acc: 0.488 - ETA: 1s - loss: 8.2087 - acc: 0.483 - ETA: 1s - loss: 8.2190 - acc: 0.482 - ETA: 1s - loss: 8.2229 - acc: 0.482 - ETA: 1s - loss: 8.2549 - acc: 0.480 - ETA: 1s - loss: 8.2182 - acc: 0.483 - ETA: 1s - loss: 8.2217 - acc: 0.483 - ETA: 1s - loss: 8.1745 - acc: 0.486 - ETA: 1s - loss: 8.1248 - acc: 0.489 - ETA: 1s - loss: 8.1230 - acc: 0.489 - ETA: 1s - loss: 8.1188 - acc: 0.490 - ETA: 1s - loss: 8.1582 - acc: 0.487 - ETA: 1s - loss: 8.1449 - acc: 0.488 - ETA: 0s - loss: 8.1638 - acc: 0.487 - ETA: 0s - loss: 8.1452 - acc: 0.488 - ETA: 0s - loss: 8.1639 - acc: 0.487 - ETA: 0s - loss: 8.1711 - acc: 0.487 - ETA: 0s - loss: 8.1688 - acc: 0.487 - ETA: 0s - loss: 8.1786 - acc: 0.486 - ETA: 0s - loss: 8.1730 - acc: 0.487 - ETA: 0s - loss: 8.1562 - acc: 0.488 - ETA: 0s - loss: 8.1806 - acc: 0.486 - ETA: 0s - loss: 8.1927 - acc: 0.486 - ETA: 0s - loss: 8.1912 - acc: 0.486 - ETA: 0s - loss: 8.1923 - acc: 0.485 - ETA: 0s - loss: 8.2020 - acc: 0.485 - ETA: 0s - loss: 8.1926 - acc: 0.485 - ETA: 0s - loss: 8.2089 - acc: 0.484 - ETA: 0s - loss: 8.2066 - acc: 0.484 - ETA: 0s - loss: 8.2084 - acc: 0.484 - ETA: 0s - loss: 8.1947 - acc: 0.4852Epoch 00016: val_loss improved from 8.84395 to 8.80676, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1932 - acc: 0.4852 - val_loss: 8.8068 - val_acc: 0.4060\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 9.6709 - acc: 0.400 - ETA: 3s - loss: 8.4057 - acc: 0.478 - ETA: 3s - loss: 7.6306 - acc: 0.523 - ETA: 2s - loss: 7.7804 - acc: 0.510 - ETA: 2s - loss: 8.2345 - acc: 0.484 - ETA: 2s - loss: 8.2846 - acc: 0.480 - ETA: 2s - loss: 8.2308 - acc: 0.481 - ETA: 2s - loss: 8.1356 - acc: 0.487 - ETA: 2s - loss: 8.0946 - acc: 0.489 - ETA: 2s - loss: 8.1077 - acc: 0.489 - ETA: 2s - loss: 8.0766 - acc: 0.491 - ETA: 2s - loss: 8.1032 - acc: 0.490 - ETA: 2s - loss: 8.1997 - acc: 0.484 - ETA: 2s - loss: 8.1585 - acc: 0.487 - ETA: 2s - loss: 8.1137 - acc: 0.490 - ETA: 2s - loss: 8.1105 - acc: 0.491 - ETA: 2s - loss: 8.0575 - acc: 0.494 - ETA: 2s - loss: 8.0899 - acc: 0.492 - ETA: 2s - loss: 8.1259 - acc: 0.490 - ETA: 2s - loss: 8.0518 - acc: 0.495 - ETA: 1s - loss: 8.0455 - acc: 0.496 - ETA: 1s - loss: 8.0527 - acc: 0.496 - ETA: 1s - loss: 8.0536 - acc: 0.495 - ETA: 1s - loss: 8.1028 - acc: 0.492 - ETA: 1s - loss: 8.0963 - acc: 0.492 - ETA: 1s - loss: 8.0898 - acc: 0.493 - ETA: 1s - loss: 8.1197 - acc: 0.491 - ETA: 1s - loss: 8.1275 - acc: 0.491 - ETA: 1s - loss: 8.1299 - acc: 0.491 - ETA: 1s - loss: 8.1231 - acc: 0.491 - ETA: 1s - loss: 8.1331 - acc: 0.490 - ETA: 1s - loss: 8.0962 - acc: 0.493 - ETA: 1s - loss: 8.1034 - acc: 0.492 - ETA: 1s - loss: 8.0899 - acc: 0.493 - ETA: 1s - loss: 8.1324 - acc: 0.490 - ETA: 1s - loss: 8.1451 - acc: 0.490 - ETA: 1s - loss: 8.1427 - acc: 0.490 - ETA: 1s - loss: 8.1080 - acc: 0.492 - ETA: 0s - loss: 8.1111 - acc: 0.492 - ETA: 0s - loss: 8.1046 - acc: 0.492 - ETA: 0s - loss: 8.1003 - acc: 0.492 - ETA: 0s - loss: 8.0964 - acc: 0.493 - ETA: 0s - loss: 8.0785 - acc: 0.494 - ETA: 0s - loss: 8.0936 - acc: 0.493 - ETA: 0s - loss: 8.0918 - acc: 0.493 - ETA: 0s - loss: 8.1166 - acc: 0.491 - ETA: 0s - loss: 8.1009 - acc: 0.493 - ETA: 0s - loss: 8.1116 - acc: 0.492 - ETA: 0s - loss: 8.1276 - acc: 0.491 - ETA: 0s - loss: 8.1205 - acc: 0.491 - ETA: 0s - loss: 8.1381 - acc: 0.490 - ETA: 0s - loss: 8.1314 - acc: 0.491 - ETA: 0s - loss: 8.1280 - acc: 0.491 - ETA: 0s - loss: 8.1453 - acc: 0.490 - ETA: 0s - loss: 8.1654 - acc: 0.489 - ETA: 0s - loss: 8.1648 - acc: 0.4886Epoch 00017: val_loss improved from 8.80676 to 8.72537, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1690 - acc: 0.4883 - val_loss: 8.7254 - val_acc: 0.4012\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 3s - loss: 8.0591 - acc: 0.500 - ETA: 2s - loss: 8.0405 - acc: 0.485 - ETA: 2s - loss: 8.0203 - acc: 0.488 - ETA: 2s - loss: 7.9127 - acc: 0.497 - ETA: 2s - loss: 8.1464 - acc: 0.482 - ETA: 2s - loss: 8.2434 - acc: 0.477 - ETA: 2s - loss: 8.3520 - acc: 0.471 - ETA: 2s - loss: 8.2381 - acc: 0.479 - ETA: 2s - loss: 8.1359 - acc: 0.486 - ETA: 2s - loss: 8.0835 - acc: 0.489 - ETA: 2s - loss: 8.1383 - acc: 0.486 - ETA: 2s - loss: 8.0797 - acc: 0.491 - ETA: 2s - loss: 8.0452 - acc: 0.493 - ETA: 2s - loss: 8.0054 - acc: 0.496 - ETA: 2s - loss: 8.0198 - acc: 0.494 - ETA: 2s - loss: 7.9602 - acc: 0.498 - ETA: 2s - loss: 8.0434 - acc: 0.493 - ETA: 2s - loss: 8.0024 - acc: 0.496 - ETA: 2s - loss: 8.0129 - acc: 0.496 - ETA: 2s - loss: 8.0223 - acc: 0.496 - ETA: 1s - loss: 8.0634 - acc: 0.493 - ETA: 1s - loss: 8.0254 - acc: 0.496 - ETA: 1s - loss: 8.0095 - acc: 0.497 - ETA: 1s - loss: 8.0003 - acc: 0.497 - ETA: 1s - loss: 7.9898 - acc: 0.497 - ETA: 1s - loss: 7.9677 - acc: 0.498 - ETA: 1s - loss: 7.9691 - acc: 0.498 - ETA: 1s - loss: 7.9191 - acc: 0.501 - ETA: 1s - loss: 7.9101 - acc: 0.502 - ETA: 1s - loss: 7.9063 - acc: 0.502 - ETA: 1s - loss: 7.9206 - acc: 0.501 - ETA: 1s - loss: 7.9337 - acc: 0.501 - ETA: 1s - loss: 7.9418 - acc: 0.500 - ETA: 1s - loss: 7.9737 - acc: 0.498 - ETA: 1s - loss: 7.9836 - acc: 0.497 - ETA: 1s - loss: 7.9778 - acc: 0.498 - ETA: 1s - loss: 7.9996 - acc: 0.496 - ETA: 1s - loss: 8.0325 - acc: 0.494 - ETA: 0s - loss: 8.0299 - acc: 0.495 - ETA: 0s - loss: 8.0396 - acc: 0.494 - ETA: 0s - loss: 8.0599 - acc: 0.493 - ETA: 0s - loss: 8.0331 - acc: 0.494 - ETA: 0s - loss: 8.0383 - acc: 0.493 - ETA: 0s - loss: 8.0443 - acc: 0.492 - ETA: 0s - loss: 8.0206 - acc: 0.494 - ETA: 0s - loss: 8.0245 - acc: 0.494 - ETA: 0s - loss: 8.0052 - acc: 0.494 - ETA: 0s - loss: 7.9888 - acc: 0.495 - ETA: 0s - loss: 7.9613 - acc: 0.496 - ETA: 0s - loss: 7.9975 - acc: 0.494 - ETA: 0s - loss: 7.9914 - acc: 0.494 - ETA: 0s - loss: 7.9914 - acc: 0.493 - ETA: 0s - loss: 8.0077 - acc: 0.492 - ETA: 0s - loss: 8.0066 - acc: 0.491 - ETA: 0s - loss: 7.9946 - acc: 0.492 - ETA: 0s - loss: 7.9778 - acc: 0.4932Epoch 00018: val_loss improved from 8.72537 to 8.62710, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9876 - acc: 0.4925 - val_loss: 8.6271 - val_acc: 0.4048\n",
      "Epoch 20/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 8.1963 - acc: 0.400 - ETA: 3s - loss: 7.3669 - acc: 0.514 - ETA: 2s - loss: 7.6386 - acc: 0.507 - ETA: 2s - loss: 7.4448 - acc: 0.513 - ETA: 2s - loss: 7.4555 - acc: 0.516 - ETA: 2s - loss: 7.5122 - acc: 0.512 - ETA: 2s - loss: 7.8223 - acc: 0.495 - ETA: 2s - loss: 7.7533 - acc: 0.501 - ETA: 2s - loss: 7.7400 - acc: 0.500 - ETA: 2s - loss: 7.6580 - acc: 0.507 - ETA: 2s - loss: 7.6697 - acc: 0.504 - ETA: 2s - loss: 7.6453 - acc: 0.505 - ETA: 2s - loss: 7.5107 - acc: 0.513 - ETA: 2s - loss: 7.4609 - acc: 0.516 - ETA: 2s - loss: 7.5670 - acc: 0.509 - ETA: 2s - loss: 7.5896 - acc: 0.507 - ETA: 2s - loss: 7.5284 - acc: 0.512 - ETA: 2s - loss: 7.5523 - acc: 0.512 - ETA: 2s - loss: 7.4881 - acc: 0.515 - ETA: 2s - loss: 7.4840 - acc: 0.515 - ETA: 1s - loss: 7.4919 - acc: 0.513 - ETA: 1s - loss: 7.4899 - acc: 0.512 - ETA: 1s - loss: 7.5430 - acc: 0.510 - ETA: 1s - loss: 7.5551 - acc: 0.509 - ETA: 1s - loss: 7.6217 - acc: 0.505 - ETA: 1s - loss: 7.6189 - acc: 0.505 - ETA: 1s - loss: 7.6501 - acc: 0.503 - ETA: 1s - loss: 7.6459 - acc: 0.504 - ETA: 1s - loss: 7.6925 - acc: 0.501 - ETA: 1s - loss: 7.6798 - acc: 0.502 - ETA: 1s - loss: 7.7098 - acc: 0.500 - ETA: 1s - loss: 7.6934 - acc: 0.501 - ETA: 1s - loss: 7.7045 - acc: 0.500 - ETA: 1s - loss: 7.6985 - acc: 0.500 - ETA: 1s - loss: 7.6800 - acc: 0.501 - ETA: 1s - loss: 7.6829 - acc: 0.501 - ETA: 1s - loss: 7.6655 - acc: 0.501 - ETA: 1s - loss: 7.6825 - acc: 0.500 - ETA: 0s - loss: 7.6991 - acc: 0.499 - ETA: 0s - loss: 7.7238 - acc: 0.497 - ETA: 0s - loss: 7.7153 - acc: 0.498 - ETA: 0s - loss: 7.7239 - acc: 0.497 - ETA: 0s - loss: 7.7092 - acc: 0.497 - ETA: 0s - loss: 7.6744 - acc: 0.499 - ETA: 0s - loss: 7.6865 - acc: 0.499 - ETA: 0s - loss: 7.6791 - acc: 0.499 - ETA: 0s - loss: 7.6612 - acc: 0.501 - ETA: 0s - loss: 7.6486 - acc: 0.501 - ETA: 0s - loss: 7.6567 - acc: 0.501 - ETA: 0s - loss: 7.6263 - acc: 0.503 - ETA: 0s - loss: 7.6218 - acc: 0.503 - ETA: 0s - loss: 7.6319 - acc: 0.503 - ETA: 0s - loss: 7.6535 - acc: 0.502 - ETA: 0s - loss: 7.6399 - acc: 0.503 - ETA: 0s - loss: 7.6356 - acc: 0.503 - ETA: 0s - loss: 7.6165 - acc: 0.5047Epoch 00019: val_loss improved from 8.62710 to 8.30365, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6303 - acc: 0.5037 - val_loss: 8.3037 - val_acc: 0.4168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c17b5e6f60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 41.3876%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "\n",
    "bottleneck_features2 = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_DogResnet50 = bottleneck_features2['train']\n",
    "valid_DogResnet50 = bottleneck_features2['valid']\n",
    "test_DogResnet50 = bottleneck_features2['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "DogResnet50_model = Sequential()\n",
    "DogResnet50_model.add(GlobalAveragePooling2D(input_shape=train_DogResnet50.shape[1:]))\n",
    "DogResnet50_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "DogResnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "\n",
    "DogResnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 207s - loss: 5.5710 - acc: 0.0000e+0 - ETA: 15s - loss: 5.5275 - acc: 0.0536    - ETA: 8s - loss: 4.8776 - acc: 0.1130 - ETA: 5s - loss: 4.4219 - acc: 0.163 - ETA: 4s - loss: 3.9877 - acc: 0.210 - ETA: 3s - loss: 3.6674 - acc: 0.251 - ETA: 2s - loss: 3.3731 - acc: 0.295 - ETA: 2s - loss: 3.1541 - acc: 0.329 - ETA: 2s - loss: 2.9572 - acc: 0.362 - ETA: 1s - loss: 2.7838 - acc: 0.392 - ETA: 1s - loss: 2.6298 - acc: 0.421 - ETA: 1s - loss: 2.5073 - acc: 0.441 - ETA: 1s - loss: 2.3962 - acc: 0.458 - ETA: 1s - loss: 2.2948 - acc: 0.476 - ETA: 0s - loss: 2.2051 - acc: 0.493 - ETA: 0s - loss: 2.1173 - acc: 0.508 - ETA: 0s - loss: 2.0509 - acc: 0.519 - ETA: 0s - loss: 1.9895 - acc: 0.529 - ETA: 0s - loss: 1.9284 - acc: 0.540 - ETA: 0s - loss: 1.8758 - acc: 0.550 - ETA: 0s - loss: 1.8268 - acc: 0.558 - ETA: 0s - loss: 1.7720 - acc: 0.568 - ETA: 0s - loss: 1.7243 - acc: 0.577 - ETA: 0s - loss: 1.6863 - acc: 0.584 - ETA: 0s - loss: 1.6512 - acc: 0.5899Epoch 00000: val_loss improved from inf to 0.83634, saving model to saved_models/weights.best.DogResnet50.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.6468 - acc: 0.5904 - val_loss: 0.8363 - val_acc: 0.7425\n",
      "Epoch 2/20\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 0.3476 - acc: 0.800 - ETA: 1s - loss: 0.3787 - acc: 0.861 - ETA: 1s - loss: 0.4092 - acc: 0.868 - ETA: 1s - loss: 0.3998 - acc: 0.870 - ETA: 1s - loss: 0.4036 - acc: 0.875 - ETA: 1s - loss: 0.4274 - acc: 0.870 - ETA: 0s - loss: 0.4397 - acc: 0.865 - ETA: 0s - loss: 0.4401 - acc: 0.864 - ETA: 0s - loss: 0.4448 - acc: 0.862 - ETA: 0s - loss: 0.4499 - acc: 0.860 - ETA: 0s - loss: 0.4612 - acc: 0.857 - ETA: 0s - loss: 0.4540 - acc: 0.860 - ETA: 0s - loss: 0.4590 - acc: 0.858 - ETA: 0s - loss: 0.4571 - acc: 0.859 - ETA: 0s - loss: 0.4610 - acc: 0.857 - ETA: 0s - loss: 0.4600 - acc: 0.857 - ETA: 0s - loss: 0.4643 - acc: 0.853 - ETA: 0s - loss: 0.4580 - acc: 0.854 - ETA: 0s - loss: 0.4569 - acc: 0.855 - ETA: 0s - loss: 0.4517 - acc: 0.857 - ETA: 0s - loss: 0.4544 - acc: 0.856 - ETA: 0s - loss: 0.4521 - acc: 0.856 - ETA: 0s - loss: 0.4548 - acc: 0.855 - ETA: 0s - loss: 0.4507 - acc: 0.856 - ETA: 0s - loss: 0.4487 - acc: 0.8575Epoch 00001: val_loss improved from 0.83634 to 0.71391, saving model to saved_models/weights.best.DogResnet50.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.4488 - acc: 0.8575 - val_loss: 0.7139 - val_acc: 0.7737\n",
      "Epoch 3/20\n",
      "6460/6680 [============================>.] - ETA: 1s - loss: 0.1521 - acc: 0.900 - ETA: 1s - loss: 0.2453 - acc: 0.929 - ETA: 1s - loss: 0.2350 - acc: 0.932 - ETA: 1s - loss: 0.2588 - acc: 0.921 - ETA: 1s - loss: 0.2578 - acc: 0.919 - ETA: 1s - loss: 0.2683 - acc: 0.922 - ETA: 0s - loss: 0.2588 - acc: 0.921 - ETA: 0s - loss: 0.2524 - acc: 0.921 - ETA: 0s - loss: 0.2581 - acc: 0.920 - ETA: 0s - loss: 0.2555 - acc: 0.921 - ETA: 0s - loss: 0.2568 - acc: 0.920 - ETA: 0s - loss: 0.2590 - acc: 0.918 - ETA: 0s - loss: 0.2555 - acc: 0.919 - ETA: 0s - loss: 0.2578 - acc: 0.918 - ETA: 0s - loss: 0.2561 - acc: 0.919 - ETA: 0s - loss: 0.2552 - acc: 0.919 - ETA: 0s - loss: 0.2567 - acc: 0.918 - ETA: 0s - loss: 0.2597 - acc: 0.917 - ETA: 0s - loss: 0.2562 - acc: 0.918 - ETA: 0s - loss: 0.2573 - acc: 0.918 - ETA: 0s - loss: 0.2550 - acc: 0.918 - ETA: 0s - loss: 0.2582 - acc: 0.917 - ETA: 0s - loss: 0.2562 - acc: 0.918 - ETA: 0s - loss: 0.2588 - acc: 0.917 - ETA: 0s - loss: 0.2634 - acc: 0.9159Epoch 00002: val_loss improved from 0.71391 to 0.70456, saving model to saved_models/weights.best.DogResnet50.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.2648 - acc: 0.9159 - val_loss: 0.7046 - val_acc: 0.7868\n",
      "Epoch 4/20\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 0.2759 - acc: 0.850 - ETA: 1s - loss: 0.1461 - acc: 0.953 - ETA: 1s - loss: 0.1420 - acc: 0.955 - ETA: 1s - loss: 0.1413 - acc: 0.956 - ETA: 1s - loss: 0.1364 - acc: 0.958 - ETA: 1s - loss: 0.1368 - acc: 0.957 - ETA: 1s - loss: 0.1382 - acc: 0.954 - ETA: 0s - loss: 0.1513 - acc: 0.949 - ETA: 0s - loss: 0.1564 - acc: 0.947 - ETA: 0s - loss: 0.1545 - acc: 0.947 - ETA: 0s - loss: 0.1566 - acc: 0.948 - ETA: 0s - loss: 0.1646 - acc: 0.946 - ETA: 0s - loss: 0.1633 - acc: 0.946 - ETA: 0s - loss: 0.1607 - acc: 0.946 - ETA: 0s - loss: 0.1620 - acc: 0.945 - ETA: 0s - loss: 0.1614 - acc: 0.946 - ETA: 0s - loss: 0.1615 - acc: 0.946 - ETA: 0s - loss: 0.1676 - acc: 0.945 - ETA: 0s - loss: 0.1697 - acc: 0.945 - ETA: 0s - loss: 0.1733 - acc: 0.944 - ETA: 0s - loss: 0.1719 - acc: 0.945 - ETA: 0s - loss: 0.1732 - acc: 0.945 - ETA: 0s - loss: 0.1716 - acc: 0.945 - ETA: 0s - loss: 0.1781 - acc: 0.943 - ETA: 0s - loss: 0.1791 - acc: 0.943 - ETA: 0s - loss: 0.1795 - acc: 0.942 - ETA: 0s - loss: 0.1780 - acc: 0.9432Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.1780 - acc: 0.9433 - val_loss: 0.7105 - val_acc: 0.8024\n",
      "Epoch 5/20\n",
      "6520/6680 [============================>.] - ETA: 1s - loss: 0.0655 - acc: 1.000 - ETA: 1s - loss: 0.0800 - acc: 0.980 - ETA: 1s - loss: 0.0757 - acc: 0.982 - ETA: 1s - loss: 0.0795 - acc: 0.979 - ETA: 1s - loss: 0.0954 - acc: 0.973 - ETA: 0s - loss: 0.0976 - acc: 0.973 - ETA: 0s - loss: 0.1135 - acc: 0.969 - ETA: 0s - loss: 0.1074 - acc: 0.971 - ETA: 0s - loss: 0.1087 - acc: 0.969 - ETA: 0s - loss: 0.1074 - acc: 0.969 - ETA: 0s - loss: 0.1039 - acc: 0.971 - ETA: 0s - loss: 0.1051 - acc: 0.970 - ETA: 0s - loss: 0.1053 - acc: 0.970 - ETA: 0s - loss: 0.1080 - acc: 0.969 - ETA: 0s - loss: 0.1108 - acc: 0.968 - ETA: 0s - loss: 0.1123 - acc: 0.967 - ETA: 0s - loss: 0.1137 - acc: 0.966 - ETA: 0s - loss: 0.1141 - acc: 0.965 - ETA: 0s - loss: 0.1153 - acc: 0.965 - ETA: 0s - loss: 0.1161 - acc: 0.965 - ETA: 0s - loss: 0.1202 - acc: 0.964 - ETA: 0s - loss: 0.1196 - acc: 0.964 - ETA: 0s - loss: 0.1250 - acc: 0.962 - ETA: 0s - loss: 0.1256 - acc: 0.962 - ETA: 0s - loss: 0.1253 - acc: 0.962 - ETA: 0s - loss: 0.1253 - acc: 0.961 - ETA: 0s - loss: 0.1255 - acc: 0.9613Epoch 00004: val_loss improved from 0.70456 to 0.65954, saving model to saved_models/weights.best.DogResnet50.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 0.1248 - acc: 0.9615 - val_loss: 0.6595 - val_acc: 0.8084\n",
      "Epoch 6/20\n",
      "6500/6680 [============================>.] - ETA: 1s - loss: 0.0086 - acc: 1.000 - ETA: 1s - loss: 0.0786 - acc: 0.983 - ETA: 1s - loss: 0.0744 - acc: 0.976 - ETA: 1s - loss: 0.0670 - acc: 0.978 - ETA: 1s - loss: 0.0642 - acc: 0.980 - ETA: 1s - loss: 0.0617 - acc: 0.982 - ETA: 0s - loss: 0.0605 - acc: 0.982 - ETA: 0s - loss: 0.0645 - acc: 0.978 - ETA: 0s - loss: 0.0646 - acc: 0.979 - ETA: 0s - loss: 0.0638 - acc: 0.980 - ETA: 0s - loss: 0.0655 - acc: 0.979 - ETA: 0s - loss: 0.0711 - acc: 0.977 - ETA: 0s - loss: 0.0749 - acc: 0.975 - ETA: 0s - loss: 0.0776 - acc: 0.975 - ETA: 0s - loss: 0.0820 - acc: 0.973 - ETA: 0s - loss: 0.0815 - acc: 0.974 - ETA: 0s - loss: 0.0808 - acc: 0.974 - ETA: 0s - loss: 0.0803 - acc: 0.974 - ETA: 0s - loss: 0.0808 - acc: 0.974 - ETA: 0s - loss: 0.0828 - acc: 0.973 - ETA: 0s - loss: 0.0841 - acc: 0.972 - ETA: 0s - loss: 0.0840 - acc: 0.972 - ETA: 0s - loss: 0.0837 - acc: 0.973 - ETA: 0s - loss: 0.0836 - acc: 0.973 - ETA: 0s - loss: 0.0834 - acc: 0.973 - ETA: 0s - loss: 0.0846 - acc: 0.973 - ETA: 0s - loss: 0.0856 - acc: 0.973 - ETA: 0s - loss: 0.0872 - acc: 0.972 - ETA: 0s - loss: 0.0883 - acc: 0.972 - ETA: 0s - loss: 0.0880 - acc: 0.971 - ETA: 0s - loss: 0.0887 - acc: 0.971 - ETA: 0s - loss: 0.0885 - acc: 0.971 - ETA: 0s - loss: 0.0888 - acc: 0.9715Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0895 - acc: 0.9705 - val_loss: 0.6974 - val_acc: 0.8120\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6420/6680 [===========================>..] - ETA: 1s - loss: 0.0191 - acc: 1.000 - ETA: 2s - loss: 0.0263 - acc: 1.000 - ETA: 2s - loss: 0.0467 - acc: 0.987 - ETA: 1s - loss: 0.0403 - acc: 0.990 - ETA: 1s - loss: 0.0408 - acc: 0.988 - ETA: 1s - loss: 0.0387 - acc: 0.989 - ETA: 2s - loss: 0.0369 - acc: 0.990 - ETA: 2s - loss: 0.0372 - acc: 0.991 - ETA: 1s - loss: 0.0372 - acc: 0.991 - ETA: 1s - loss: 0.0419 - acc: 0.990 - ETA: 1s - loss: 0.0457 - acc: 0.988 - ETA: 1s - loss: 0.0515 - acc: 0.986 - ETA: 1s - loss: 0.0510 - acc: 0.986 - ETA: 1s - loss: 0.0505 - acc: 0.986 - ETA: 1s - loss: 0.0522 - acc: 0.985 - ETA: 0s - loss: 0.0515 - acc: 0.986 - ETA: 0s - loss: 0.0544 - acc: 0.984 - ETA: 0s - loss: 0.0558 - acc: 0.983 - ETA: 0s - loss: 0.0552 - acc: 0.983 - ETA: 0s - loss: 0.0558 - acc: 0.982 - ETA: 0s - loss: 0.0573 - acc: 0.982 - ETA: 0s - loss: 0.0579 - acc: 0.982 - ETA: 0s - loss: 0.0610 - acc: 0.980 - ETA: 0s - loss: 0.0627 - acc: 0.980 - ETA: 0s - loss: 0.0629 - acc: 0.980 - ETA: 0s - loss: 0.0640 - acc: 0.980 - ETA: 0s - loss: 0.0654 - acc: 0.979 - ETA: 0s - loss: 0.0650 - acc: 0.9799Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0652 - acc: 0.9796 - val_loss: 0.6816 - val_acc: 0.8132\n",
      "Epoch 8/20\n",
      "6460/6680 [============================>.] - ETA: 1s - loss: 0.0226 - acc: 1.000 - ETA: 1s - loss: 0.0250 - acc: 0.996 - ETA: 1s - loss: 0.0231 - acc: 0.998 - ETA: 1s - loss: 0.0606 - acc: 0.988 - ETA: 1s - loss: 0.0496 - acc: 0.989 - ETA: 1s - loss: 0.0518 - acc: 0.986 - ETA: 0s - loss: 0.0488 - acc: 0.987 - ETA: 0s - loss: 0.0496 - acc: 0.987 - ETA: 0s - loss: 0.0475 - acc: 0.987 - ETA: 0s - loss: 0.0448 - acc: 0.988 - ETA: 0s - loss: 0.0470 - acc: 0.987 - ETA: 0s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0474 - acc: 0.986 - ETA: 0s - loss: 0.0476 - acc: 0.987 - ETA: 0s - loss: 0.0473 - acc: 0.986 - ETA: 0s - loss: 0.0479 - acc: 0.986 - ETA: 0s - loss: 0.0490 - acc: 0.986 - ETA: 0s - loss: 0.0479 - acc: 0.986 - ETA: 0s - loss: 0.0489 - acc: 0.986 - ETA: 0s - loss: 0.0497 - acc: 0.985 - ETA: 0s - loss: 0.0491 - acc: 0.986 - ETA: 0s - loss: 0.0490 - acc: 0.985 - ETA: 0s - loss: 0.0491 - acc: 0.985 - ETA: 0s - loss: 0.0482 - acc: 0.985 - ETA: 0s - loss: 0.0503 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0507 - acc: 0.9853 - val_loss: 0.7092 - val_acc: 0.8228\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 1s - loss: 0.0142 - acc: 1.000 - ETA: 1s - loss: 0.0165 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0273 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.994 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0273 - acc: 0.992 - ETA: 0s - loss: 0.0286 - acc: 0.992 - ETA: 0s - loss: 0.0293 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0315 - acc: 0.991 - ETA: 0s - loss: 0.0312 - acc: 0.991 - ETA: 0s - loss: 0.0380 - acc: 0.989 - ETA: 0s - loss: 0.0365 - acc: 0.990 - ETA: 0s - loss: 0.0372 - acc: 0.989 - ETA: 0s - loss: 0.0361 - acc: 0.990 - ETA: 0s - loss: 0.0366 - acc: 0.990 - ETA: 0s - loss: 0.0364 - acc: 0.990 - ETA: 0s - loss: 0.0380 - acc: 0.989 - ETA: 0s - loss: 0.0380 - acc: 0.989 - ETA: 0s - loss: 0.0385 - acc: 0.989 - ETA: 0s - loss: 0.0378 - acc: 0.9892Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0377 - acc: 0.9894 - val_loss: 0.7187 - val_acc: 0.8096\n",
      "Epoch 10/20\n",
      "6600/6680 [============================>.] - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0204 - acc: 0.992 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0200 - acc: 0.992 - ETA: 1s - loss: 0.0195 - acc: 0.992 - ETA: 1s - loss: 0.0206 - acc: 0.991 - ETA: 0s - loss: 0.0194 - acc: 0.992 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0231 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0228 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.993 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0260 - acc: 0.993 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0277 - acc: 0.9927Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0276 - acc: 0.9928 - val_loss: 0.7786 - val_acc: 0.8204\n",
      "Epoch 11/20\n",
      "6440/6680 [===========================>..] - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0078 - acc: 1.000 - ETA: 1s - loss: 0.0087 - acc: 0.998 - ETA: 1s - loss: 0.0087 - acc: 0.998 - ETA: 1s - loss: 0.0095 - acc: 0.999 - ETA: 1s - loss: 0.0088 - acc: 0.999 - ETA: 1s - loss: 0.0178 - acc: 0.998 - ETA: 0s - loss: 0.0210 - acc: 0.997 - ETA: 0s - loss: 0.0219 - acc: 0.996 - ETA: 0s - loss: 0.0221 - acc: 0.996 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0239 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.995 - ETA: 0s - loss: 0.0244 - acc: 0.994 - ETA: 0s - loss: 0.0248 - acc: 0.994 - ETA: 0s - loss: 0.0254 - acc: 0.994 - ETA: 0s - loss: 0.0250 - acc: 0.994 - ETA: 0s - loss: 0.0243 - acc: 0.994 - ETA: 0s - loss: 0.0238 - acc: 0.994 - ETA: 0s - loss: 0.0230 - acc: 0.994 - ETA: 0s - loss: 0.0232 - acc: 0.994 - ETA: 0s - loss: 0.0227 - acc: 0.994 - ETA: 0s - loss: 0.0230 - acc: 0.9944Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0227 - acc: 0.9945 - val_loss: 0.7665 - val_acc: 0.8180\n",
      "Epoch 12/20\n",
      "6620/6680 [============================>.] - ETA: 1s - loss: 0.1972 - acc: 0.950 - ETA: 1s - loss: 0.0182 - acc: 0.996 - ETA: 1s - loss: 0.0168 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0129 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0125 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.9947Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0176 - acc: 0.9948 - val_loss: 0.7881 - val_acc: 0.8263\n",
      "Epoch 13/20\n",
      "6620/6680 [============================>.] - ETA: 1s - loss: 0.0017 - acc: 1.000 - ETA: 1s - loss: 0.0031 - acc: 1.000 - ETA: 1s - loss: 0.0029 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0076 - acc: 0.996 - ETA: 1s - loss: 0.0078 - acc: 0.996 - ETA: 1s - loss: 0.0080 - acc: 0.996 - ETA: 1s - loss: 0.0089 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.996 - ETA: 0s - loss: 0.0094 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0092 - acc: 0.996 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0132 - acc: 0.997 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.9965Epoch 00012: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 1s - loss: 0.0140 - acc: 0.9966 - val_loss: 0.7983 - val_acc: 0.8311\n",
      "Epoch 14/20\n",
      "6440/6680 [===========================>..] - ETA: 1s - loss: 0.0060 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.000 - ETA: 1s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0117 - acc: 0.9967Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0121 - acc: 0.9967 - val_loss: 0.8194 - val_acc: 0.8347\n",
      "Epoch 15/20\n",
      "6560/6680 [============================>.] - ETA: 1s - loss: 0.0010 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0053 - acc: 0.999 - ETA: 0s - loss: 0.0055 - acc: 0.999 - ETA: 0s - loss: 0.0060 - acc: 0.999 - ETA: 0s - loss: 0.0058 - acc: 0.999 - ETA: 0s - loss: 0.0060 - acc: 0.999 - ETA: 0s - loss: 0.0059 - acc: 0.999 - ETA: 0s - loss: 0.0055 - acc: 0.999 - ETA: 0s - loss: 0.0053 - acc: 0.999 - ETA: 0s - loss: 0.0049 - acc: 0.999 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0090 - acc: 0.998 - ETA: 0s - loss: 0.0087 - acc: 0.998 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.9977Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0095 - acc: 0.9978 - val_loss: 0.8299 - val_acc: 0.8263\n",
      "Epoch 16/20\n",
      "6420/6680 [===========================>..] - ETA: 1s - loss: 2.9759e-04 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.0000    - ETA: 1s - loss: 0.0031 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.997 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.9974Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0090 - acc: 0.9975 - val_loss: 0.8700 - val_acc: 0.8287\n",
      "Epoch 17/20\n",
      "6460/6680 [============================>.] - ETA: 1s - loss: 4.5368e-04 - acc: 1.000 - ETA: 1s - loss: 0.0016 - acc: 1.0000    - ETA: 1s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0129 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.998 - ETA: 1s - loss: 0.0086 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.9978Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0073 - acc: 0.9978 - val_loss: 0.8764 - val_acc: 0.8287\n",
      "Epoch 18/20\n",
      "6460/6680 [============================>.] - ETA: 1s - loss: 0.0016 - acc: 1.000 - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.999 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.9981Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0078 - acc: 0.9982 - val_loss: 0.9038 - val_acc: 0.8180\n",
      "Epoch 19/20\n",
      "6440/6680 [===========================>..] - ETA: 1s - loss: 0.0040 - acc: 1.000 - ETA: 1s - loss: 4.9901e-04 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.0000    - ETA: 1s - loss: 0.0118 - acc: 0.997 - ETA: 1s - loss: 0.0097 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.9984Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0053 - acc: 0.9984 - val_loss: 0.8890 - val_acc: 0.8216\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6520/6680 [============================>.] - ETA: 1s - loss: 1.2266e-04 - acc: 1.000 - ETA: 1s - loss: 4.0099e-04 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.0000    - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 1s - loss: 0.0017 - acc: 1.000 - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0045 - acc: 0.999 - ETA: 0s - loss: 0.0054 - acc: 0.9989Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 0.0053 - acc: 0.9990 - val_loss: 0.9285 - val_acc: 0.8299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2948b941400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.DogResnet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "DogResnet50_model.fit(train_DogResnet50, train_targets, \n",
    "          validation_data=(valid_DogResnet50, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "\n",
    "DogResnet50_model.load_weights('saved_models/weights.best.DogResnet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.4593%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "\n",
    "\n",
    "DogResnet50_predictions = [np.argmax(DogResnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(DogResnet50_predictions)==np.argmax(test_targets, axis=1))/len(DogResnet50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "from extract_bottleneck_features import *\n",
    "\n",
    "def DogResnet50_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature2 = extract_Resnet50(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = DogResnet50_model.predict(bottleneck_feature2)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def humanordog(img_path):\n",
    "    if dog_detector(img_path) == True:\n",
    "        print('Dog detected of type %s.' % DogResnet50_predict_breed(img_path))\n",
    "    elif face_detector(img_path) == True:\n",
    "        print('Hello human!  You look like a dog of type %s.' % DogResnet50_predict_breed(img_path))\n",
    "    else:\n",
    "        print('No human or dog detected, try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No human or dog detected, try again!\n",
      "Hello human!  You look like a dog of type Papillon.\n",
      "Hello human!  You look like a dog of type Dachshund.\n",
      "Hello human!  You look like a dog of type Airedale_terrier.\n",
      "Hello human!  You look like a dog of type Nova_scotia_duck_tolling_retriever.\n",
      "Hello human!  You look like a dog of type French_bulldog.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7211a0a7247e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhuman_files_short\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mhumanordog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdog_files_short\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mhumanordog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-9197dda4859c>\u001b[0m in \u001b[0;36mhumanordog\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dog detected of type %s.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mDogResnet50_predict_breed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hello human!  You look like a dog of type %s.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mDogResnet50_predict_breed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No human or dog detected, try again!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-4b0cbc148c6e>\u001b[0m in \u001b[0;36mDogResnet50_predict_breed\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mDogResnet50_predict_breed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# extract bottleneck features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mbottleneck_feature2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_Resnet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# obtain predicted vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpredicted_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDogResnet50_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbottleneck_feature2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Documents\\grive\\school\\udacity\\machinelearning - nanodegree\\dog-project-full\\extract_bottleneck_features.py\u001b[0m in \u001b[0;36mextract_Resnet50\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_Resnet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_Xception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\keras\\applications\\resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentity_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentity_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentity_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2048\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\keras\\applications\\resnet50.py\u001b[0m in \u001b[0;36midentity_block\u001b[1;34m(input_tensor, kernel_size, filters, stage, block)\u001b[0m\n\u001b[0;32m     66\u001b[0m     x = Conv2D(filters2, kernel_size,\n\u001b[0;32m     67\u001b[0m                padding='same', name=conv_name_base + '2b')(x)\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbn_name_base\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'2b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    138\u001b[0m         normed, mean, variance = K.normalize_batch_in_training(\n\u001b[0;32m    139\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_axes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             epsilon=self.epsilon)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mnormalize_batch_in_training\u001b[1;34m(x, gamma, beta, reduction_axes, epsilon)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         normed = tf.nn.batch_normalization(x, broadcast_mean, broadcast_var,\n\u001b[0;32m   1478\u001b[0m                                            \u001b[0mbroadcast_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast_gamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1479\u001b[1;33m                                            epsilon)\n\u001b[0m\u001b[0;32m   1480\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnormed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m       \u001b[0minv\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m     return x * inv + (offset - mean * inv\n\u001b[0m\u001b[0;32m    755\u001b[0m                       if offset is not None else -mean * inv)\n\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    792\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1013\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1015\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1016\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_mul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1623\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m   \"\"\"\n\u001b[1;32m-> 1625\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1717\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1718\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mistr\\Anaconda2\\envs\\dog-project\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    669\u001b[0m       output = pywrap_tensorflow.RunCppShapeInference(\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No shape inference function exists for op\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "\n",
    "for i in human_files_short:\n",
    "    humanordog(i)\n",
    "for i in dog_files_short:\n",
    "    humanordog(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
